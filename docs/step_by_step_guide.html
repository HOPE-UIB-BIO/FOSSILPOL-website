<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>A step-by-step guide to data processing</title>

<script src="site_libs/header-attrs-2.11/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>








<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.tab('show');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">FOSSILPOL project</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="about.html">General Information</a>
</li>
<li>
  <a href="step_by_step_guide.html">A step-by-step guide</a>
</li>
<li>
  <a href="contributing.html">Contribute</a>
</li>
<li>
  <a href="road_map.html">Project roadmap</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">A step-by-step guide to data processing</h1>

</div>


<p>The Workflow is structured in a modular manner where all steps are organised sequentially and guided by one main configuration file (<em>Config file</em>) where all criteria and setup configurations are pre-defined by the user (see next section, Supplementary Figure 1). <br></br></p>
<div id="table-of-contents" class="section level1">
<h1>Table of Contents</h1>
<ol style="list-style-type: decimal">
<li><a href="#data-input">Data input</a></li>
<li><a href="#data-stor">Data storage</a></li>
<li><a href="#data-proc">Data processing</a>
<ul>
<li><a href="#01">I. Data sourcing: <code>01_Neotoma_source</code></a>
<ul>
<li><a href="#0101"><code>01_Download_neotoma.R</code></a></li>
<li><a href="#0102"><code>02_Extract_samples.R</code></a></li>
<li><a href="#0103"><code>03_Filter_dep_env.R</code></a></li>
<li><a href="#0104"><code>04_Extract_chron_control_tables.R</code></a></li>
<li><a href="#0105"><code>05_Extract_raw_pollen_data.R</code></a></li>
</ul></li>
<li><a href="#02">II. Data sourcing: <code>02_Private_source</code></a>
<ul>
<li><a href="#0201"><code>01_Import_private_data.R</code></a></li>
</ul></li>
<li><a href="#03">III. Initial data processing: <code>03_Merging_and_geography</code></a>
<ul>
<li><a href="#0301"><code>01_Merge_datasets.R</code></a>
<ul>
<li><a href="#0301-dupl">Detection of duplicates</a></li>
<li><a href="#0301-add">Additional data preparation</a></li>
</ul></li>
</ul></li>
<li><a href="#04">IV. Chronologies: <code>04_Chronologies</code></a>
<ul>
<li><a href="#0401"><code>01_Prepare_chron_control_tables.R</code></a>
<ul>
<li><a href="#0401-curves">Calibration curves</a></li>
<li><a href="#0401-points">Types of chronology control points</a></li>
<li><a href="#0401-prep">Chronology table preparation</a></li>
<li><a href="#0401-pmc">Percentage carbon</a></li>
</ul></li>
<li><a href="#0402"><code>02_Run_age_depth_models.R</code></a>
<ul>
<li><a href="#0402-core">Multi-core computation</a></li>
<li><a href="#0402-it">Iterations</a></li>
</ul></li>
<li><a href="#0403"><code>03_Predict_ages.R</code></a></li>
<li><a href="#0404"><code>04_Save_AD_figures.R</code></a></li>
<li><a href="#0405"><code>05_Merge_chron_output.R</code></a></li>
</ul></li>
<li><a href="#05">V. Harmonisation: <code>05_Harmonisation</code></a>
<ul>
<li><a href="#0501"><code>01_Harmonisation.R</code></a></li>
</ul></li>
<li><a href="#06">VI. Data filtering: <code>06_Main_filtering</code></a>
<ul>
<li><a href="#0601"><code>01_Level_filtering.R</code></a>
<ul>
<li><a href="#0601-polsum">Pollen count sum</a></li>
<li><a href="#0601-age">Age criteria</a></li>
<li><a href="#0601-extrap">Level age extrapolation</a></li>
<li><a href="#0601-interest">Interest period</a></li>
<li><a href="#0601-levels">Number of levels</a></li>
</ul></li>
</ul></li>
<li><a href="#07">VII. Outputs: <code>07_Outputs</code></a>
<ul>
<li><a href="#0701"><code>01_Pollen_diagrams.R</code></a></li>
<li><a href="#0702"><code>02_Save_assembly.R</code></a></li>
<li><a href="#0703"><code>03_Save_references.R</code></a></li>
</ul></li>
</ul></li>
<li><a href="#ref">References</a></li>
</ol>
<p><br></p>
</div>
<div id="data-input" class="section level1">
<h1>Data input<a name = "data-input"></a></h1>
<p>The Workflow is set up in a way that data from Neotoma are the primary data. However, additional data sources (<code>private</code>) can be used in parallel by using our predefined format (see section <strong>Data sourcing</strong>). The user thus has the flexibility to either source data from Neotoma or another data source if a consistent formatting file is used as provided in this Workflow.</p>
<p>Three additional data inputs are required for the initial set-up of the Workflow:</p>
<ol style="list-style-type: decimal">
<li><p>Configuration file (<code>00_Config_file.R</code>) – this contains all the user-selected settings which will be applied throughout the Workflow. These range from technical settings (e.g. location of the data storage) to specific requirements (e.g. filtering criteria) for sequences to be included.</p></li>
<li><p>Geographical shapefiles – several shapefiles, which define the geographical information and harmonisation region for each sequence, must be provided by the user. Firstly, the Workflow is conceptualised for a global project, so the general structure of data processing is done “per continent”, but the user can use any other regionalisation of interest. The Workflow comes with a default shapefile roughly delimiting continents, but it can be adjusted or replaced to fit project needs. Secondly, we provide a <code>harmonisation region shapefile</code> which is a copy of the continental regions by default, but we strongly recommend adjusting the harmonisation regions for each specific project and working with corresponding taxonomic harmonisation tables. Finally, any other geographical information can also be added for each sequence as long as shapefiles are provided.</p></li>
<li><p>Harmonisation tables – for each project, one harmonisation table must be provided per harmonisation region (delimited by the corresponding harmonisation region shapefile, see above). A harmonisation table must have two columns: i) <code>original taxa</code> with taxonomic names originally present in Neotoma (and other data sourced in the project), and ii) <code>harmonised taxa</code> with the final taxonomic names. The Workflow will detect if a harmonisation table has been provided by the user, or otherwise create a new table with all detected “raw” taxa names for each harmonisation region. The latter can consequently serve as a template for harmonisation in the <code>harmonised taxa</code> column.</p></li>
</ol>
<p><br></p>
</div>
<div id="data-storage" class="section level1">
<h1>Data storage<a name = "data-stor"></a></h1>
<p>The Workflow will produce several files including temporary output files, “stop-check” tables, and final output (figures, data assembly, etc.) (Supplementary Figure 1):</p>
<ol style="list-style-type: decimal">
<li><p>Temporary output files: the Workflow is set up in a way that temporary (in-progress) data files are saved at various stages of the Workflow. Each file will contain the date of creation for easier organisation. When run multiple times, the Workflow will automatically detect if there are any changes in a selected file and only overwrite it if an updated file can be produced. This also means that the user does not have to re-run the whole Workflow but can re-run only specific parts. As the total size of the files can become substantial, the user can specify if all files should be stored within the project folder (default) or in another directory (specified by using the <code>data_storage_path</code> in the Config file). With such specification, and after running the <code>00_Config_file.R</code> script, there will be an additional folder structure created (Code block 3).</p></li>
<li><p>“Stop-checks” CSV tables: while running the workflow, there will be several times that a user will be asked to check and, where necessary, adjust the produced CSV tables to subsequently continue with the Workflow (i.e. re-run script). This is done to oblige the user to check the produced intermediate results before continuing. For example, at a certain point, the Workflow will produce a list of all ecological groups detected within the dataset compilation obtained from Neotoma. A user then has to edit the mentioned CSV table and specify which ecological groups should be kept (include = TRUE) and which should be filtered out (include = FALSE). There are several stop-checks throughout the workflow (Supplementary Figure 1)</p></li>
<li><p>Workflow final output:</p>
<ul>
<li>A ready-to-use, taxonomically harmonised and standardised compilation of fossil pollen data, ready for the analytical stage</li>
<li>Age-depth model of each sequence</li>
<li>Pollen diagram of each sequence</li>
<li>Metadata table relaying the main data contributor, contact information, and corresponding publications for citation purposes of the used datasets.</li>
</ul></li>
</ol>
<p><br></p>
<p><strong>Code block 3</strong></p>
<pre><code>data_storage_path
│
└───Data
│   │
│   └───Input
│   │   │
│   │   └───Chronology_setting
│   │   │   │
│   │   │   └───Bchron_crash
│   │   │   │
│   │   │   └───Chron_control_point_types
│   │   │   │
│   │   │   └───Percentage_radiocarbon
│   │   │
│   │   └───Depositional_environment
│   │   │   │
│   │   │   └───Neotoma
│   │   │   │
│   │   │   └───Other
│   │   │
│   │   └───Eco_group
│   │   │
│   │   └───Harmonisation_tables
│   │   │
│   │   └───Neotoma_download
│   │   │
│   │   └───Potential_duplicates
│   │   │
│   │   └───Private
│   │   │
│   │   └───Regional_age_limits
│   │   
│   └───Personal_database_storage
│   │
│   └───Processed
│       │
│       └───Chronology
│       │   │
│       │   └───Chron_tables_prepared
│       │   │
│       │   └───Model_full
│       │   │
│       │   └───Predicted_ages
│       │   │
│       │   └───Temporary_output
│       │
│       └───Data_filtered
│       │
│       └───Data_harmonised
│       │
│       └───Data_merged
│       │
│       └───Data_with_chronologies
│       │
│       └───Neotoma_processed
│       │   │
│       │   └───Neotoma_chron_control
│       │   │
│       │   └───Neotoma_dep_env
│       │   │
│       │   └───Neotoma_meta
│       │
│       └───Private
│ 
└───Outputs
    │
    └───Data
    │
    └───Figures
    │   │
    │   └───Chronology
    │   │
    │   └───Pollen_diagrams
    │   
    └───Tables
        │
        └───Meta_and_references</code></pre>
<p><br></p>
</div>
<div id="data-processing" class="section level1">
<h1>Data processing<a name = "data-proc"></a></h1>
<p>Here we focus on the scripts within the <code>R/01_Data_processing</code> folder representing all steps needed for data processing from obtaining data to the final dataset compilation, organised in the following sections:</p>
<ol style="list-style-type: upper-roman">
<li><p><a href="#01">Data sourcing: <code>01_Neotoma_source</code></a>- retrieve and process data from Neotoma</p></li>
<li><p><a href="#02">Data sourcing: <code>02_Private_source</code></a> - process data from private datasets (optional)</p></li>
<li><p><a href="#03">Initial data processing: <code>03_Merging_and_geography</code></a> - merge data sources, filter out duplicates, and assign values based on geographical location</p></li>
<li><p><a href="#04">Chronologies: <code>04_Chronologies</code></a> - prepare chronology control tables, calculate age-depth models, and predict ages for levels</p></li>
<li><p><a href="#05">Harmonisation: <code>05_Harmonisation</code></a> - prepare all harmonisation tables and harmonise pollen taxa</p></li>
<li><p><a href="#06">Data filtering: <code>06_Main_filtering</code></a> - filter out levels and sequences based on user-defined criteria</p></li>
<li><p><a href="#07">Outputs: <code>07_Outputs</code></a> - save the final output including dataset compilation, pollen diagrams, and metadata information</p></li>
</ol>
<p><br></p>
<div id="i.-data-sourcing-01_neotoma_source" class="section level2">
<h2>I. Data sourcing: 01_Neotoma_source <a name = "01"></a></h2>
<div id="scripts" class="section level3">
<h3><strong>Scripts</strong></h3>
<ul>
<li><p><code>Run_01_01.R</code>- run all scripts within this folder</p></li>
<li><p><a href="#0101"><code>01_Download_neotoma.R</code></a>- download the pollen data from the Neotoma database</p></li>
<li><p><a href="#0102"><code>02_Extract_samples.R</code></a> - create a tibble from Neotoma downloaded lists</p></li>
<li><p><a href="#0103"><code>03_Filter_dep_env.R</code></a> - get depositional environment data and filter out sequences based on the user preferences</p></li>
<li><p><a href="#0104"><code>04_Extract_chron_control_tables.R</code></a> - get chronologies, including the preferred table with chronology control points</p></li>
<li><p><a href="#0105"><code>05_Extract_raw_pollen_data.R</code></a> - extract the raw pollen counts from Neotoma and filter by user-selected ecological groups.</p></li>
</ul>
<p><br></p>
</div>
<div id="description-of-individual-scripts" class="section level3">
<h3><strong>Description of individual scripts</strong></h3>
<p><br></p>
<div id="download_neotoma.r" class="section level4">
<h4>01_Download_neotoma.R <a name = "0101"></a></h4>
<p>All pollen sequences are downloaded from Neotoma based on the geographical criteria (spatial extent) and the selected data type, in this case: <em><code>pollen</code></em>.</p>
<p><br></p>
</div>
<div id="extract_samples.r" class="section level4">
<h4>02_Extract_samples.R <a name = "0102"></a></h4>
<p>Each sequence is processed using a unique dataset ID (<code>dataset_id</code>) with metadata information extracted. Metadata includes information about the name of the sequence, geographical information, and the authors and publication DOI connected to the dataset. The authors and their link to the dataset are saved into a separate Author-Dataset database created specifically for each project. It allows easy extraction of authors and DOI for the final dataset compilation produced by the Workflow.</p>
<p><br></p>
</div>
<div id="filter_dep_env.r" class="section level4">
<h4>03_Filter_dep_env.R <a name = "0103"></a></h4>
<p>Depositional information about each sequence gives information about the environments where a core was extracted. Based on the research question, there could be a preference for certain environments (e.g. terrestrial vs marine). Currently in Neotoma, the data about depositional environments are organised in a hierarchical structure (e.g. “Pond” is nested in “Natural Lake”, which is nested in “Lacustrine”), in which the maximum number of nested layers is five. At the lowest hierarchical level, there are currently over 50 different categories of depositional environments (for fossil pollen sequences). Based on the selected sequences, the Workflow will produce a full list of all depositional environments (and their hierarchical position) presented in the data selection. The user is then requested to define the environments of choice (this is a “stop-check” point, Supplementary Figure 1). Note that excluding depositional environments with the higher hierarchical position will not automatically exclude all depositional environments nested in it.</p>
<p><br></p>
</div>
<div id="extract_chron_control_tables.r" class="section level4">
<h4>04_Extract_chron_control_tables.R <a name = "0104"></a></h4>
<p>Each chronology has a chronology table that contains information about chronology control points used to construct an age-depth model. Some sequences can have multiple chronology tables as some sequences have been used for several projects or recalibrated (updated) by data stewards. These tables are numbered according to the order in which they were created and uploaded. Each chronology comes with the age-unit of the age-depth model output (e.g. “Radiocarbon years BP”, “Calibrated radiocarbon years BP”) and the temporal range of the sequence (youngest and oldest age). The chronologies in “Radiocarbon years BP” are often older chronologies as it is now common practice to recalibrate radiocarbon-dated material and produce chronologies expressed in “Calibrated radiocarbon years BP”. Note: The chronologies in “Calibrated radiocarbon years BP” still come with chronology table(s) containing the uncalibrated radiocarbon ages and need to be calibrated by the user if a new age-depth model is desired. The Workflow automatically selects one table per sequence based on the order defined by <code>chron_order</code> in the Config file. Note: if more tables have the same age-unit type (e.g. Calibrated radiocarbon years BP), the Workflow will opt for the more recent table). The user can specify their preference for certain age unit types in the Config file. In addition, only sequences which have at least a certain number of control points (defined by <code>min_n_of_control_points</code> in the Config file) will be subsequently used.</p>
<p><br></p>
</div>
<div id="extract_raw_pollen_data.r" class="section level4">
<h4>05_Extract_raw_pollen_data.R <a name = "0105"></a></h4>
<p>Each level of each sequence comes with additional information: a) unique sample ID (sample_id), b) information about depth (and estimated age later), and c) pollen counts for each taxon present in that level. The information about levels is split into two different tables (first with depth and ages, and second with pollen counts) linked together by sample ID.</p>
<p>The Workflow will only keep sequences with a minimal number of levels as defined in the Config file (<code>min_n_sites_initial</code>). The minimum number of levels is by default selected as three but the user can change this setting.</p>
<p>In the case of data derived from Neotoma, each pollen taxon has information about the ecological group (e.g. palms, mangroves, etc). Based on the selected sequences, the Workflow will produce a full list of all ecological groups after which the user is requested to define which ecological groups to include (a “stop-check”, Supplementary Figure 1, see explanation of abbreviation in Supplementary Table 1)</p>
<p><br></p>
<p><strong>Supplementary Table 1.</strong> Ecological groups assigned to pollen taxa as defined in Neotoma.</p>
<table>
<thead>
<tr class="header">
<th>ABBREVIATION</th>
<th>ECOLOGICAL GROUP</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>ACRI</td>
<td>Acritarchs</td>
</tr>
<tr class="even">
<td>ANAC</td>
<td>Anachronic</td>
</tr>
<tr class="odd">
<td>ALGA</td>
<td>Algae (e.g. Botryococcus)</td>
</tr>
<tr class="even">
<td>AQB</td>
<td>Aquatics (e.g. Sphagnum)</td>
</tr>
<tr class="odd">
<td>AQVP</td>
<td>Aquatic vascular plants (e.g. Isoëtes)</td>
</tr>
<tr class="even">
<td>BIOM</td>
<td>Biometric measurements</td>
</tr>
<tr class="odd">
<td>EMBR</td>
<td>Embryophyta</td>
</tr>
<tr class="even">
<td>FUNG</td>
<td>Fungi</td>
</tr>
<tr class="odd">
<td>LABO</td>
<td>Lab analyses</td>
</tr>
<tr class="even">
<td>MAN</td>
<td>Mangroves</td>
</tr>
<tr class="odd">
<td>PALM</td>
<td>Palms</td>
</tr>
<tr class="even">
<td>PLNT</td>
<td>Plant</td>
</tr>
<tr class="odd">
<td>SEED</td>
<td>Unidentified, but definitely pollen - Spermatophyte rank or clade</td>
</tr>
<tr class="even">
<td>SUCC</td>
<td>Succulents</td>
</tr>
<tr class="odd">
<td>TRSH</td>
<td>Trees and shrubs</td>
</tr>
<tr class="even">
<td>UNID</td>
<td>Unknown and Indeterminable</td>
</tr>
<tr class="odd">
<td>UPBR</td>
<td>Upland bryophytes</td>
</tr>
<tr class="even">
<td>UPHE</td>
<td>Upland herbs</td>
</tr>
<tr class="odd">
<td>VACR</td>
<td>Terrestrial vascular cryptogams</td>
</tr>
<tr class="even">
<td>VASC</td>
<td>Vascular plants</td>
</tr>
</tbody>
</table>
<p><br></p>
</div>
</div>
</div>
<div id="ii.-data-sourcing-02_private_source" class="section level2">
<h2>II. Data sourcing: <code>02_Private_source</code><a name = "02"></a></h2>
<p>Our FOSSILPOL Workflow allows the use of private data in combination with the Neotoma data, as long as it is prepared following our provided <a href="https://figshare.com/articles/dataset/FOSSILPOL-private_data-template/19794112">template</a>. Any data can be used as long as it contains the following required information: a) meta information, b) depositional information, c) chronology information, d) level information, and e) pollen counts information. Including private data is fully optional and can be skipped as indicated by <code>private_data</code> = TRUE/FALSE in the Config file.</p>
<div id="scripts-1" class="section level3">
<h3><strong>Scripts</strong></h3>
<ul>
<li><code>Run_01_02.R</code> - run all scripts within this folder</li>
<li><a href="#0201"><code>01_Import_private_data.R</code></a> - source private data and filter the sequences in a similar way as Neotoma.</li>
</ul>
<p><br></p>
</div>
<div id="description-of-individual-scripts-1" class="section level3">
<h3><strong>Description of individual scripts</strong></h3>
<p><br></p>
<div id="import_private_data.r" class="section level4">
<h4>01_Import_private_data.R <a name = "0201"></a></h4>
<p>The sourcing of private data follows a simple order of actions:</p>
<ol style="list-style-type: decimal">
<li><p>Data files need to be prepared by the user following the template.</p></li>
<li><p>Data are extracted and formatted to be compatible with Neotoma data using the <code>import_datasets_from_folder</code> function. The user can specify which folder contains the prepared data files with the argument <code>dir</code> (default = <code>Data/Input/Private/</code>). In addition, the user can specify the <code>suffix</code> argument to keep track of the source of the data.</p></li>
<li><p>Authors of data are extracted and added to the Author-Dataset database (see above).</p></li>
<li><p>Data are treated in a similar way as data from Neotoma, in terms of filtering by geographical location, depositional environments, and number of levels.</p></li>
</ol>
<p><br></p>
</div>
</div>
</div>
<div id="iii.-initial-data-processing-03_merging_and_geography" class="section level2">
<h2>III. Initial data processing: 03_Merging_and_geography <a name = "03"></a></h2>
<div id="scripts-2" class="section level3">
<h3><strong>Scripts</strong></h3>
<ul>
<li><code>Run_01_03.R</code> - run all scripts within this folder</li>
<li><a href="#0301"><code>01_Merge_datasets.R</code></a> - merge data from all sources, filter out duplicates, and assign values based on geographical location</li>
</ul>
<p><br></p>
</div>
<div id="description-of-individual-scripts-2" class="section level3">
<h3><strong>Description of individual scripts</strong></h3>
<p><br></p>
<div id="merge_datasets.r" class="section level4">
<h4>01_Merge_datasets.R <a name = "0301"></a></h4>
<p>After initial data processing, sequences from Neotoma and Private are merged together.</p>
<p><br></p>
<div id="detection-of-duplicates" class="section level5">
<h5><em>Detection of duplicates</em> <a name = "0301-dupl"></a></h5>
<p>There is a possibility that some datasets from the user’s private source are already in Neotoma. To avoid duplication within the final dataset compilation, the Workflow will compare datasets from both sources and identify potential duplicates. This step is optional but recommended to follow. To do so, the user needs to specify that <code>detect_duplicates</code> == TRUE in the Config file (this is set as default). The Workflow will start a simple subroutine using the function <code>proc_filter_out_duplicates</code>. Because comparing all sequences between groups to each other is relatively computationally demanding, the function will split the data into several groups using their geographical location (ca. 100 sequences per group). The user can define the number of groups using the <code>n_subgroups</code> argument. Next, each sequence from one source is compared to all sequences from the other source as long as they are within 1 degree radius (the assumption here is that duplicated sequences will be in a similar location). The user can define the maximum distance using the <code>max_degree_distance</code> argument. Finally, the Workflow will output a list of potential duplicated sequences (a “stop-check”, Supplementary Figure 1). For each sequence pair, the user must specify which sequences should be deleted by writing <code>1</code> or <code>2</code> in the <code>delete</code> column (leaving <code>0</code> will leave both sequences in).</p>
<p><br></p>
</div>
<div id="additional-data-preparation" class="section level5">
<h5><em>Additional data preparation</em> <a name = "0301-add"></a></h5>
<p>Several more steps take place to create the fully merged dataset compilation before proceeding to the chronology step:</p>
<ol style="list-style-type: decimal">
<li><p>All taxon names are transformed into a more computer-friendly format for easier manipulation. The <code>proc_clean_count_names</code> function will first transform special characters to text (e.g. <code>+</code> to <code>_plus_</code>) and then use the janitor package (LINK) to transform into <code>snake_case</code> style. In addition, the user can specify additional specific changes in names (e.g. based on presence of special characters) by adjusting the <code>user_name_patterns</code> argument (see example in the script).</p></li>
<li><p>Individual levels are sorted by their depth for each sequence. This includes subroutines, for example, only keeping levels present in all data tables, filtering out levels without pollen records, and taxa which are not present in any level.</p></li>
<li><p>Spatial information for each sequence is assigned based on the provided geographical shapefiles. Specifically:</p>
<ul>
<li><p>Region information – the shapefile in <code>Data/Input/Spatial/Regions_shapefile</code> will assign the regional names for each sequence (see Data input section above). The user can (and is recommended to) change the spatial delimitation of the data by altering the shapefile.</p></li>
<li><p>Political delimitation (countries) - obtained from GADM database (www.gadm.org), version 2.8, November 2015.</p></li>
<li><p>Harmonisation region – the shapefile in <code>Data/Input/Spatial/Harmonisation_regions_shapefile</code> will assign the harmonisation region (to be able to link the corresponding harmonisation table to use; See Data input section above). The default shapefile in the Workflow is a copy of the Region information shapefile but should be adjusted by the user to correspond to the area covered by the different harmonisation tables.</p></li>
<li><p>Calibration curves (normal and post-bomb) - depending on the geographical position of the sequence, a different calibration curve needs to be assigned, as different curves are used for the northern and southern hemispheres, and for terrestrial and marine environments. See more details about calibration curves below.</p></li>
<li><p>The user can add any additional spatial delimitation (e.g. ecozones). This will require adding the specific shapefile (or TIF file) in <code>/Data/Input/Spatial/NAME_OF_FOLDER</code> and adjusting the R code manually (<code>optional_info_to_assign</code>) so that the shapefile is sourced, and its information assigned to each sequence (see the example in the script).</p></li>
</ul></li>
<li><p>The Workflow will create a new table with age limitations for each region presented in the data, which needs to be edited by the user (a “stop-check”). For example, <code>Regional_age_limits</code> table will have the following values:</p>
<ul>
<li><code>young_age</code> = youngest age the sequence must have</li>
<li><code>old_age</code> = oldest age the sequence must have</li>
<li><code>end_of_interest_period</code> = levels beyond this age will be omitted</li>
</ul></li>
</ol>
<p><br></p>
</div>
</div>
</div>
</div>
<div id="iv.-chronologies-04_chronologies" class="section level2">
<h2>IV. Chronologies: 04_Chronologies <a name = "04"></a></h2>
<p>In order to estimate the age of individual levels based on their depth (a “chronology”), an “age-depth model” needs to be constructed. With a successful age-depth model, the ages of each individual level are estimated, and the full age of the sequence is now known.</p>
<p>Age-depth modelling can be very computationally heavy and can take a substantial amount of time. In addition, the saved results can result in very large files (&gt;10 GB). Therefore, the Workflow automatically processes several files (rds format):</p>
<ul>
<li>CHRON TABLES:
<ul>
<li><code>/Data/Processed/Bchron_tibble_prepared/bchron_tibble_prepared.rds</code> contains all the chronology control tables prepared to be re-calibrated (mostly small file)</li>
</ul></li>
<li>AGE-DEPTH MODELS:
<ul>
<li><code>/Data/Processed/Bchron/Full_outputs/Full_outputs/bchron_format_file.rds</code> contains the actual age-depth models (often very large file)</li>
</ul></li>
<li>PREDICTED AGES:
<ul>
<li><code>/Data/Processed/Bchron/Predicted_ages/bchron_ages.rds</code> contains the predicted ages using the age-depth models (mostly small file)</li>
</ul></li>
</ul>
<p>It may be not preferential to re-run all age-depth models every time the user wants to re-run the Workflow. Therefore, the Workflow offers the option to save the successful age-depth models (AGE-DEPTH MODELS &amp; PREDICTED AGES; see above) and keep them between individual runs. This can be done in several ways:</p>
<ul>
<li><p><code>recalib_AD_models</code> in the Config file can be set to FALSE. This will result in completely omitting the scripts aimed at calculating age-depth models, and therefore this step will be skipped. Note that this will only work if the age-depth models have already been successfully created at least once (AGE-DEPTH MODELS &amp; PREDICTED AGES; see above).</p></li>
<li><p><code>calc_AD_models_denovo</code> in the Config file can be set to FALSE (this is the default). This will result in the Workflow using any old file with successful age-depth models (AGE-DEPTH MODELS) and adding any additional sequences detected in the data to re-calibrate.</p></li>
<li><p><code>predict_ages_denovo</code> in the Config file can be set to FALSE (this is the default). This will result in the Workflow using the previous successful age-depth models but re-assigning the ages to each level of all related sequences (even for sequences where ages were successfully predicted). This is done, for instance, when the number of levels increased in a sequence between the Workflow run and ages still need to be assigned.</p></li>
</ul>
<p><strong>IMPORTANT NOTE</strong>: If you select both <code>calc_AD_models_denovo</code> and <code>predict_ages_denovo</code> as FALSE for your first run of age-depth modelling, you will be asked by the Workflow to temporarily switch both to TRUE. This will happen in the console, you do not have to change anything in the Config file.</p>
<div id="scripts-3" class="section level3">
<h3><strong>Scripts</strong></h3>
<ul>
<li><code>Run_01_04.R</code> - run all scripts within this folder</li>
<li><a href="#0401"><code>01_Prepare_chron_control_tables.R</code></a> - prepare the chronology tables for age-depth modelling</li>
<li><a href="#0402"><code>02_Run_age_depth_models.R</code></a>- create age-depth models with BChron</li>
<li><a href="#0403"><code>03_Predict_ages.R</code></a> - estimate the ages of individual levels</li>
<li><a href="#0404"><code>04_Save_AD_figures.R</code></a> - save visual output of the age-depth models</li>
<li><a href="#0405"><code>05_Merge_chron_output.R</code></a> - link age-depth models to corresponding datasets</li>
</ul>
<p><br></p>
</div>
<div id="description-of-individual-scripts-3" class="section level3">
<h3><strong>Description of individual scripts</strong></h3>
<p><br></p>
<div id="prepare_chron_control_tables.r" class="section level4">
<h4>01_Prepare_chron_control_tables.R <a name = "0401"></a></h4>
<p>Age-depth models are constructed using <code>chronology control points</code> (usually radiocarbon dates) with known depth, estimated age, and associated age uncertainties. Each sequence can have and should ideally have several such points saved in the <code>chronology control table</code>. Each chronology control point has the following properties:</p>
<ol style="list-style-type: lower-roman">
<li><p>Depth</p></li>
<li><p>Estimated age</p></li>
<li><p>Error of the estimated age</p></li>
<li><p>Type of the chronology control point</p></li>
<li><p>The calibration curve used (which is needed to convert the raw radiocarbon ages to calendar ages (Note: radiocarbon “ages” are not true ages)).</p></li>
</ol>
<p>This script will take several steps to prepare all sequences for age-depth modelling, specifically:</p>
<ol style="list-style-type: decimal">
<li><p>Create and attach the necessary calibration curves</p></li>
<li><p>Select the preferred chronology control point types</p></li>
<li><p>Prepare chronology tables</p></li>
<li><p>Fix issues with percentage carbon (if necessary)</p></li>
</ol>
<p><br></p>
<div id="calibration-curves" class="section level5">
<h5><em>Calibration curves</em> <a name = "0401-curve"></a></h5>
<p>Calibration curves are assigned to each control point based on several criteria. If a control point has a type flagged to be calibrated, a calibration curve is assigned based on the geographical position of the sequence. Note: only chronology control points in uncalibrated radiocarbon ages need recalibration. IntCal20 is assigned to terrestrial northern hemisphere material, SHCal20 for southern hemisphere dates, and a mixed calibration curve for the between zone (constructed using the rcarbon package with a proportion of curve contribution 1:1. See <a href="https://chrono.qub.ac.uk/blaauw/clam.html">link</a> for more info on mixed calibration curve use).</p>
<p>All calibration curves have an age limitation, i.e. each curve can be used only for certain ages. Radiocarbon calibration curves currently do not cover ages older than 55 thousand years. For the younger ages, during the last century, there are issues with the Earth’s atmospheric radiocarbon and a different set of calibration curves needs to be used. This is caused by the deployment of nuclear weapons in the ’50s and ’60s, which caused a spike in atmospheric radiocarbon and resulted in radiocarbon measurements from material in the following decades to return often highly negative ages. Therefore, if the control point has a radiocarbon age younger than 200 yr BP, a “post-bomb calibration curve” is used instead. As with the normal calibration curves, the geographical location of the sequence is taken into account to assign the corresponding post-bomb calibration curve.</p>
<p>Modern radiocarbon dates are calibrated by using one of the post-bomb calibration curves (<em>nh_zone_1</em>, <em>nh_zone_2</em>, <em>nh_zone_3</em>, <em>sh_zone_1_2</em>, <em>sh_zone_3</em>), following Hua et al., 2013 and <a href="http://calib.org/CALIBomb/">link</a>. The Workflow will automatically assign the corresponding curve based on the geographical location of the sequence (see function <code>IntCal::copyCalibrationCurve</code>). If modern radiocarbon dates are detected, the Workflow will then display the detected sequences to the user and apply the conversion according to the post-bomb curves from the <em>IntCal</em> package (Blaauw 2021).</p>
<p><br></p>
</div>
<div id="types-of-chronology-control-points" class="section level5">
<h5><em>Types of chronology control points</em> <a name = "0401-points"></a></h5>
<p>Each control point in the control table has several properties: unique ID, depth, age, error, thickness, and chron control point type (e.g. radiocarbon, biostratigraphical, annual laminations). Each type of chronology control point has different age uncertainties. For instance, many older sequences relied on indirect dating techniques based on biostratigraphical layers, similar geological levels from other records (e.g. a volcanic event), and pollen-based levels (e.g. the appearance of a key taxon), among others and can have large age uncertainties into thousands of years. Neotoma has over 50 different chronology controls points that fall within the categories of geochronological (e.g. lead-210, radiocarbon, uranium-series), relative time scale (e.g. MIS5e, Heinrich Stadial 1, Late Wisconsin event), stratigraphic (e.g. biostratigraphic events such as the introduction of anthropogenic taxa), cultural (e.g. European Settlement Horizon), other absolute dating methods (e.g. annual laminations, collection date), and other dating methods (e.g. extrapolated or guesses). Only the chronology control points in uncalibrated radiocarbon ages require recalibration with the calibration curves as most, if not all, other control points will be in calendar ages and no recalibration should be implemented.</p>
<p>A user has the option to select which control point types should be accepted and which should be calibrated (a “stop-check”, Supplementary Figure 1). The Workflow will automatically produce a list of all detected control points from all selected sequences, which includes columns called <code>include</code> (the user should indicate if the chronology control point should be included) and <code>calibrate</code> (the user should indicate if the point should be recalibrated using the calibration curves).</p>
<p><br></p>
</div>
<div id="chronology-table-preparation" class="section level5">
<h5><em>Chronology table preparation</em> <a name = "0401-prep"></a></h5>
<p>The chronology control tables will need to undergo a number of user-defined adjustments:</p>
<ul>
<li><p>Filtering out selected unwanted control point types (defined by “stop-check” above)</p></li>
<li><p>Fixing instances of missing values. Defined in the Config file, the values <code>default_thickness</code> (default = 1) and <code>default_error</code> (default = 100) will replace missing values (NAs) for thickness and error, specifically.</p></li>
<li><p>Filtering out control points with an error that is considered too big. Any control point with an error bigger than <code>max_age_error</code> (defined in the Config file; default = 3000 yr) will be filtered out.</p></li>
<li><p>Removing control points that are duplicated in terms of age and/or depth.</p></li>
<li><p>In several cases, the chronology control point from the core-top has a type specified as <code>guess</code>. A user can specify that the type <code>guess</code> is only acceptable to a certain depth using the <code>guess_depth</code> variable in the Config file (default is 10 cm)</p></li>
</ul>
<p>In addition, the number and distribution of such control points can give a good indicator of the temporal uncertainty around levels’ ages (Giesecke et al. 2014 [VHA], Flantua et al. 2016 [CP]). For example, a sequence with few chronology control points within the focus time period, will have large uncertainties of predicted ages. Hence, the information of the quality of chronologies, i.e. taking into account the types and levels of chronology control points, can be a criterion used in the selection of sequences. The value <code>min_n_of_control_points</code> (defined in the Config file; default = 2) will serve as a criterion for the prepared chronology control tables, where sequences that do not fulfil such requirements will be filtered out. Note that there is a trade-off between accepting only the tables with a high number of control points per time interval (more robust age-depth model) and the number of sequences that will be able to fulfil high criteria.</p>
<p><br></p>
</div>
<div id="percentage-carbon" class="section level5">
<h5><em>Percentage carbon</em> <a name = "0401-pmc"></a></h5>
<p>There are three ways by which post-bomb radiocarbon dates are reported, namely by 1) modern radiocarbon dates (&lt;0 F14C yr); 2) percent modern carbon (pMC, normalised to 100%); and 3) fraction radiocarbon (F14C, normalised to 1; Reimer et al. 2004). Currently, there is no direct way to know from Neotoma whether the dates are in pMC or F14C. Even if the cases are likely to be few, such dates need to be checked to avoid incorrect calculations.</p>
<p>The strongest rule of thumb is that normal radiocarbon dates and errors should always be integer (no decimals) and are uploaded so by the data stewards to Neotoma. The second rule of thumb is that pMC control points are characterised by an age value from around 100 with a decimal place. Thus, the Workflow will automatically export suspicious sequences and the user must specify which control points should be back-transformed (a “stop-check”). For those selected by the user to be back-transformed (include == TRUE), the Workflow will first convert the pMC values to “normal” post-bomb radiocarbon ages (negative 14C ages) by using the <code>IntCal::pMC.age</code> function, after which normal post-bomb calibration curves are used to calibrate the values to final calendar ages (see above).</p>
<p>Although F14C has been recommended by experts for the reporting of post-bomb samples (Reimer et al. 2004 [Radiocarbon], in practice, the bulk of the reporting is done as modern radiocarbon dates followed by pMC to a much lesser extent. The Workflow currently does not deal with F14C as no such cases have been detected to date. As this is not consistent with the rest of the data, back-transformation could be done when working with private data, and the IntCal package (Blaauw 2021) can be used to do so.</p>
<p><br></p>
</div>
</div>
<div id="run_age_depth_models.r" class="section level4">
<h4>02_Run_age_depth_models.R <a name = "0402"></a></h4>
<p>Individual age-depth models are estimated using the Bchron package (Haslett &amp; Parnell, 2008)), which estimates the Bayesian probabilistic trajectory of the age-depth model curve (non-parametric chronology model to age-depth data according to the Compound Poisson-Gamma model). Therefore, it is suitable for various combinations of control point types, outliers, and age reversals.</p>
<p>If there are many ages at close or similar depths (e.g. annual laminations), initialisation problems may occur and the Bchron could fail to converge the age-depth model. In such a scenario, the thickness of duplicated chronology control points is automatically increased by 0.01 (see <code>artificialThickness</code> argument in <code>Bchron::Bchronology</code> function).</p>
<p><br></p>
<div id="multi-core-computation" class="section level5">
<h5><em>Multi-core computation</em> <a name = "0402-core"></a></h5>
<p>Because creating age-depth models for multiple sequences can be computationally demanding, the Workflow uses multi-core (parallel) computation. The Workflow automatically detects the number of cores for the machine on which the code is running (this can be adjusted by specifying the number in <code>number_of_cores</code> in the Config file). Several age-depth models are then created at the same time. This is done by splitting the sequences into batches, with each batch containing a certain number of sequences (based on the <code>number_of_cores</code>).</p>
<p>Note that there is a possibility that the age-depth model estimation will crash for unforeseen reasons. Therefore, the Workflow is structured in a way that if an estimation of the whole batch crashes (freezes), the Workflow will skip that batch and continue with other batches. The user can specify how long the machine should wait before skipping the batch with the <code>time_per_sequence</code> argument in the <code>chron_recalibrate_ad_models</code> function.</p>
<p>For each batch, the Workflow will try to estimate age-depth models three times. The user can specify the number of attempts by changing the <code>batch_attempts</code> in the <code>chron_recalibrate_ad_models</code> function. If the age-depth modelling is stopped in the process, the Workflow will automatically use the previously successful batches, which are saved automatically. This should help when the user needs to stop the age-depth modelling and resume it at a later time.</p>
<p>Next, the Workflow continues to another subroutine where age-depth models for sequences from “failed” batches are estimated one by one. Similar to batch estimation, the age-depth model estimation is tried three times for each “crashed” sequence until successful at least once. On some rare occasions, a sequence could cause R to freeze completely and prevents it from skipping to another sequence. In that case, the dataset ID of the dataset that caused the crash will be automatically written in the Crash file (found in <code>/Data/Input/Chronology_setting/Bchron_crash/</code>) and omitted from the future run of age-depth models. The user is recommended to do a detailed check of the specific dataset, e.g. the chronology control table for possible inconsistencies or other flaws that could be causing BChron to fail to produce an age-depth model.</p>
<p><br></p>
</div>
<div id="iterations" class="section level5">
<h5><em>Iterations</em> <a name = "0402-cit"></a></h5>
<p>The default setting is that the number of iterations is set to 50k, discarding the first 10k iterations (<em>burn-in</em>) and keeping every iteration beyond the burn-in with a step size of 40 (<em>thinning</em>). The user can change the number of iterations by altering the <code>iteration_multiplier</code> in the Config file. This will result in changing the total interactions but keeping the ratios of burn-ins and thinning. Thus 1000 ((50k - 10k) / 40) posterior values are drawn. The default number of iterations should produce a robust estimation but they can be increased by the user if preferred (by increasing the <code>iteration_multiplier</code>). Note that increasing the <code>iteration_multiplier</code> will automatically increase the time that the program will wait for estimation of an age-depth model before skipping it (see <code>time_per_sequence</code> above).</p>
<p>The user should keep in mind that creating age-depth models for hundreds of sequences using a high number of iterations is still computationally demanding and can take a significant amount of time in the order of tens of hours or days.</p>
<p><br></p>
</div>
</div>
<div id="predict_ages.r" class="section level4">
<h4>03_Predict_ages.R <a name = "0403"></a></h4>
<p>With a successful age-depth model, the ages of individual levels are estimated. The Workflow will estimate age and the error estimate for each level, which will encapsulate 95% of all age posterior values (<em>upper</em> and <em>lower</em> boundary). As <em>Bchron</em> uses a probabilistic model it is possible to obtain “possible” ages of each level. To do so, a number of ages (default = 1000, see above) are drawn from the model posterior representing all the “possible” ages, and a series of quantiles of various values (25, 50, 75, etc.) are then calculated. The 50th quantile (median) is used as the final age of each level, and the 2.5th and 97.5th quantiles as upper and lower boundaries respectively. This results in the age estimate of each level including its error estimates. The whole matrix “levels by posterior drawn (possible age)” is saved as an <em>age uncertainty matrix</em> (<code>age_ uncertainty</code> column). This whole process is completely automated and does not require any input from the user.</p>
<p><br></p>
</div>
<div id="save_ad_figures.r" class="section level4">
<h4>04_Save_AD_figures.R <a name = "0404"></a></h4>
<p>The visual representation of all age-depth models is saved as output for visual confirmation of successful model estimation. The files (as PDFs) are stored in the <code>/Outputs/Figures/Chronologies/</code> folder and split into subfolders defined by region. The properties of the figures (size of figures, font size, etc) can be altered in the Config file (<code>image_width</code>, <code>image_height</code>, <code>image_units</code>, <code>image_dpi</code>, <code>text_size</code>, <code>line_size</code>)</p>
<p><br></p>
</div>
<div id="merge_chron_output.r" class="section level4">
<h4>05_Merge_chron_output.R <a name = "0405"></a></h4>
<p>The successfully predicted ages are linked with all the sequences from various sources (sections I-III). The individual levels of each sequence are then ordered by the predicted ages and the same order of levels is then applied to the tables with the pollen counts.</p>
<p><br></p>
</div>
</div>
</div>
<div id="v.-harmonisation-05_harmonisation" class="section level2">
<h2>V. Harmonisation: 05_Harmonisation <a name = "05"></a></h2>
<p>The goal of taxonomic harmonisation is to standardise all taxa synonyms to the same morphological type and reduce the effect of taxonomic uncertainty (Rull 2012 [Jbio]; Deza-Araujo et al. 2021 [VHA]). For this purpose, a ‘harmonisation table’, will clump the morphotypes (type synonyms) into the higher taxonomic level that is most likely to be identified by most researchers.</p>
<div id="scripts-4" class="section level3">
<h3><strong>Scripts</strong></h3>
<ul>
<li><code>Run_01_05.R</code> - run all scripts within this folder</li>
<li><a href="#0501"><code>01_Harmonisation.R</code></a> - Prepare all harmonisation tables and harmonise the raw counts.</li>
</ul>
<p><br></p>
</div>
<div id="description-of-individual-scripts-4" class="section level3">
<h3><strong>Description of individual scripts</strong></h3>
<p><br></p>
<div id="harmonisation.r" class="section level4">
<h4>01_Harmonisation.R <a name = "0501"></a></h4>
<p>First, the Workflow will check the <code>harmonisation regions</code> present in the data, defined by the shapefile (see Data input and section III), and confirm that there is one harmonisation table per region (a “stop-check”, Supplementary Figure 1). If any table is missing (or the Workflow is run for the first time), the Workflow will automatically create a harmonisation table per harmonisation region, with all the raw taxa names from all the sequences from within that region.</p>
<p>Each harmonisation table is created so each taxon can have two columns:</p>
<ol style="list-style-type: lower-roman">
<li><p><code>taxon_name</code> which is the original name of the taxa formatted into a more computer-friendly format (<em>snake_case</em>)</p></li>
<li><p><code>level 1</code>, which should be used to merge various taxa into higher taxonomical units, specific for the project.</p></li>
</ol>
<p>The user can also define which taxa should be completely removed during the harmonisation process (marked as “delete”), in case of a taxonomic mistake or palaeoecological proxy not of interest, e.g. spores). The user can add additional columns (e.g. level 2) and then specify which levels should be included by altering the argument <code>harm_name</code> when using the <code>harmonise_all_regions</code> function.</p>
<p>With these tables, each dataset is harmonised so that all taxa that belong to the same harmonised taxa are summed together in each level (this process is applicable for both count and percentage data). This process includes automatic detection of successful pollen harmonisation by checking the total number of pollen grains before and after harmonisation (it can be turned off by changing the argument <code>pollen_grain_test</code> when using the <code>harmonise_all_regions</code> function).</p>
<p><br></p>
</div>
</div>
</div>
<div id="vi.-data-filtering-06_main_filtering" class="section level2">
<h2>VI. Data filtering: 06_Main_filtering <a name = "06"></a></h2>
<p>Before obtaining the final harmonised comprehensive dataset compilation, data need to be further trimmed down by filtering out unwanted levels and sequences. All of these filtering criteria can be adjusted by the user in the Config file:</p>
<ul>
<li><p><code>filter_by_pollen_sum</code> - if <code>TRUE</code>, the Workflow will use the quantity of counted pollen grains at each level as a factor in determining the quality of the level.</p></li>
<li><p><code>filter_by_age_limit</code> - if <code>TRUE</code>, the Workflow will filter out sequences which do not span the user-defined time period (defined by <code>young_age</code> and <code>old_age</code> in <code>Regional_age_limits</code> table; see section III).</p></li>
<li><p><code>filter_by_extrapolation</code> - if <code>TRUE</code>, the Workflow will filter out levels based on the number of years between their age and the last chronology control point used for age-depth modelling.</p></li>
<li><p><code>filter_by_interest_region</code> - if <code>TRUE</code>, the Workflow will filter out levels that are older than <code>end_of_interest_period</code> (defined in <code>Regional_age_limits</code> table; see section III) to subsequently reduce processing time during follow-up analyses.</p></li>
<li><p><code>filter_by_number_of_levels</code> - if <code>TRUE</code>, the Workflow will filter out sequences based on the number of levels.</p></li>
</ul>
<p>In addition, two more options can be turned on by the user:</p>
<ul>
<li><p><code>use_age_quantiles</code> - if <code>TRUE</code>, the Workflow will use the 95th age quantile (uncertainty of the level age) throughout the data filtration process, i.e. the age of the level will be assumed to be anywhere between the upper and lower boundary (see section IV). This will result in a more stable dataset compilation between the different results of age-depth modelling (as a probabilistic result can output slightly different results each time they are run). However, the final dataset compilation may require some additional filtering before any analyses, as the 95th age quantile can span very long time periods.</p></li>
<li><p><code>use_bookend_level</code> - if <code>TRUE</code>, the Workflow will leave one additional level in the older time period through the data filtration process. This will result in a “bookend” level, which can help to anchor information beyond the period of interest.</p></li>
</ul>
<div id="scripts-5" class="section level3">
<h3><strong>Scripts</strong></h3>
<ul>
<li><code>Run_01_06.R</code> - run all scripts within this folder</li>
<li><a href="#0601"><code>01_Level_filtering.R</code></a> - filter out levels and sequences based on the user-defined criteria predefined in the Config file</li>
</ul>
<p><br></p>
</div>
<div id="description-of-individual-scripts-5" class="section level3">
<h3><strong>Description of individual scripts</strong></h3>
<p><br></p>
<div id="level_filtering.r" class="section level4">
<h4>01_Level_filtering.R <a name = "0601"></a></h4>
<p><br></p>
<div id="pollen-count-sum" class="section level5">
<h5><em>Pollen count sum</em> <a name = "0401-polsum"></a></h5>
<p>The quantity of counted pollen grains at each level can be a factor in determining the quality of the level, and different reasons exist for defining a minimum and/or acceptable number of counted pollen grains per level. To obtain a reliable representation of the vegetation, researchers often aim to obtain counts higher than 300 (following Moore et al. 1991 [Blackwell, Oxford], but other recommendations may have been followed (&gt;150; #REFS, Djamali &amp; Cilleros, 2020). A low pollen count (&lt;100) can have different reasons: i) natural depositional phenomenon causing poor pollen preservation and ii) low pollen counts by the data contributor due to time constraints.</p>
<p>The user can select two different quantities of total pollen grains per level: a) minimum number (<code>min_n_grains</code>) and b) acceptable number (<code>target_n_grains</code>). All levels with total pollen grains below the minimum number will be filtered out. In addition, the whole sequence will only be accepted if X% (set by <code>percentage_samples</code>; default = 50) of all levels fulfils at least the acceptable number of pollen grains. This filtration criterion will only be used if <code>filter_by_pollen_sum</code> == <code>TRUE</code>.</p>
<p><br></p>
</div>
<div id="age-criteria" class="section level5">
<h5><em>Age criteria</em> <a name = "0401-age"></a></h5>
<p>As projects differ in their temporal focus, only a subset of sequences will be of interest to the particular project. Therefore, sequences that do not span a certain age period (from <code>young_age</code> to <code>old_age</code>; specified by the <code>Regional_age_limits</code> table) will be filtered out. This filtration criterion will only be used if <code>filter_by_age_limit</code> == <code>TRUE</code>.</p>
<p><br></p>
</div>
<div id="level-age-extrapolation" class="section level5">
<h5><em>Level age extrapolation</em> <a name = "0401-extrap"></a></h5>
<p>Levels between chronology control points have a certain degree of temporal uncertainty, which increases with the distance to the points (Giesecke et al. 2014 [VHA]). In addition, levels older than the oldest chronology control point have no other chronology control point to extrapolate towards and, therefore, have increasingly large uncertainty. In order to limit the use of levels based on large extrapolations, the user can select the maximum extrapolation age (<code>maximum_age_extrapolation</code>), i.e. levels older than the selected criterion from the last chronology control point will be filtered out. This filtration criterion will only be used if <code>filter_by_extrapolation</code> == <code>TRUE</code>.</p>
<p><br></p>
</div>
<div id="interest-period" class="section level5">
<h5><em>Interest period</em> <a name = "0401-interest"></a></h5>
<p>The individual levels of a sequence outside of the “interest period” (<code>end_of_interest_period</code> specified by the <code>Regional_age_limits</code> table) will be filtered out as they do not provide additional information. This filtration criterion will only be used if <code>filter_by_interest_region</code> == <code>TRUE</code>.</p>
<p><br></p>
</div>
<div id="number-of-levels" class="section level5">
<h5><em>Number of levels</em> <a name = "0401-levels"></a></h5>
<p>The total number of levels in a sequence might be an important criterion for further usage of the sequence in specific analysis. Sequences might have been sampled at low resolution (large space between levels such as &gt; 30 cm) leaving substantial unassessed gaps – and thus time periods – between levels. In addition, sequences with few levels will likely contribute poorly to studies focused on specific time periods (many non-value levels) and can cause unnecessary outlier values. Therefore, the user can select the minimum number of levels (<code>min_n_levels</code>) which sequences must have at the end of the filtration subroutine. This filtration criterion will only be used if <code>filter_by_number_of_levels</code> == <code>TRUE</code></p>
<p><br></p>
</div>
</div>
</div>
</div>
<div id="vii.-outputs-07_outputs" class="section level2">
<h2>VII. Outputs: 07_Outputs <a name = "07"></a></h2>
<div id="scripts-6" class="section level3">
<h3><strong>Scripts</strong></h3>
<ul>
<li><code>Run_01_07.R</code> - run all scripts within this folder</li>
<li><a href="#0701"><code>01_Pollen_diagrams.R</code></a> - save pollen diagrams for all sequences</li>
<li><a href="#0702"><code>02_Save_assembly.R</code></a> - save data assembly with selected variables (columns)</li>
<li><a href="#0703"><code>03_Save_references.R</code></a> - save reference and metatable</li>
</ul>
<p><br></p>
</div>
<div id="description-of-individual-scripts-6" class="section level3">
<h3><strong>Description of individual scripts</strong></h3>
<p><br></p>
<div id="pollen_diagrams.r" class="section level4">
<h4>01_Pollen_diagrams.R <a name = "0701"></a></h4>
<p>Pollen diagrams for all sequences will be created using the rioja R package (LINK). The harmonised data will be automatically transformed into relative proportion for plotting purposes. The pollen diagrams will be saved in the <code>/Outputs/Figures/Pollen_diagrams/</code> folder and split into sub-folders defined by Region.</p>
<p>The <code>plot_all_pollen_diagrams</code> function will automatically produce a PDF of the pollen diagram split into several A4 pages ready to be printed out. The y-axis is, by default, the age of the levels, but it can be altered to depth (by the <code>y_var</code> argument). The maximum number of taxa per page can be altered by the <code>max_taxa</code> argument (default = 20). In addition, the function will automatically omit very rare taxa. Specifically, the <code>min_n_occur</code> argument will determine the minimum number of occurrences per sequence each taxon has to have to be plotted.</p>
<p><br></p>
</div>
<div id="save_assembly.r" class="section level4">
<h4>02_Save_assembly.R <a name = "0702"></a></h4>
<p>A ready-to-use, taxonomically harmonised and standardised compilation of fossil pollen data (ready for the analytical stage) is produced and saved in the <code>/Outputs/Data/</code> folder. The file is saved as a ‘tibble’ (tidyverse) in <code>rds</code> format.</p>
<p>The user can select which columns (variables) should be present in the final data by selecting <code>select_final_variables</code> == TRUE in the Config file. In that case, the Workflow will interactively (in the R console) ask the user to specify if each variable should be included (Yes/No).</p>
<p><br></p>
</div>
<div id="save_references.r" class="section level4">
<h4>03_Save_references.R <a name = "0703"></a></h4>
<p>First, the Workflow will output a “metadata table” (<code>data_assembly_meta.csv</code>) that contains: - the list of all sequences in the final dataset assembly - information about geographical location - depositional environment - assigned continental region - assigned harmonisation region - final number of levels - number of chronology control points used for age-depth modelling - assigned calibration curve - age limits of each sequence - data source (Neotoma or private) - DOI (from Neotoma).</p>
<p>This list can be altered by changing the final variables (see previous section).</p>
<p>Second, the Workflow will output a “reference table” (<code>authors_meta.csv</code>) containing information about the datasets used, the main data contributor, and their contact information.</p>
<p>If an affiliation is provided with the private data, the “affiliation table” (<code>affiliation_table</code>) is also exported linking affiliations and their authors.</p>
<p>Finally, the Workflow will produce <code>reproducibility_bundle.zip</code>, which is a zip file of the Config file, all “stop-checks” CSV tables, and all shapefiles. The idea is that such zip files can be shared when publishing any results gathered in a project created using the FOSSILPOL Workflow to increase the reproducibility of work. All of these outputs can be found in the <code>/Outputs/Tables/Meta_and_references/</code> folder.</p>
<p><br></p>
</div>
</div>
</div>
</div>
<div id="references" class="section level1">
<h1>References <a name = "ref"></a></h1>
<p>WIP</p>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
