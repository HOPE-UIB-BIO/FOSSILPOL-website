<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>step_by_step_guide</title>

<script src="site_libs/header-attrs-2.22/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">FOSSILPOL project</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="about.html">General Information</a>
</li>
<li>
  <a href="step_by_step_guide.html">A step-by-step guide</a>
</li>
<li>
  <a href="get_in_touch.html">Get in touch!</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">




</div>


<div id="a-step-by-step-guide-to-data-processing-with-fossilpol"
class="section level1">
<h1>A step-by-step guide to data processing with FOSSILPOL
<img src="figures/Logo%20FOSSILPOL%20regular%20-%20600ppi.png" align="right" width="200" /></h1>
<ul>
<li><a href="#summary">Summary</a>
<ul>
<li><a href="#figure-1">Figure 1</a></li>
</ul></li>
<li><a href="#data-input">Data input</a>
<ul>
<li><a href="#figure-2">Figure 2</a></li>
</ul></li>
<li><a href="#data-storage">Data storage</a>
<ul>
<li><a href="#code-block-3">Code block 3</a></li>
</ul></li>
<li><a href="#data-processing">Data processing</a>
<ul>
<li><a href="#i-data-sourcing-01_neotoma_source"><strong>I. Data
sourcing: <code>01_Neotoma_source</code></strong></a>
<ul>
<li><a href="#data-sourcing-neotoma---scripts">Data sourcing (Neotoma) -
Scripts</a></li>
<li><a
href="#data-sourcing-neotoma---description-of-individual-scripts">Data
sourcing (Neotoma) - Description of individual scripts</a>
<ul>
<li><a
href="#id_01_download_neotomar"><em>01_Download_neotoma.R</em></a></li>
<li><a
href="#id_02_extract_samplesr"><em>02_Extract_samples.R</em></a></li>
<li><a
href="#id_03_filter_dep_envr"><em>03_Filter_dep_env.R</em></a></li>
<li><a
href="#id_04_extract_chron_control_tablesr"><em>04_Extract_chron_control_tables.R</em></a></li>
<li><a
href="#id_05_extract_raw_pollen_datar"><em>05_Extract_raw_pollen_data.R</em></a>
<ul>
<li><a
href="#table-1-ecological-groups-assigned-to-pollen-taxa-as-defined-in-neotoma">Table
1. Ecological groups assigned to pollen taxa as defined in
Neotoma</a></li>
</ul></li>
</ul></li>
</ul></li>
<li><a href="#ii-data-sourcing-02_other_source"><strong>II. Data
sourcing: <code>02_Other_source</code></strong></a>
<ul>
<li><a href="#data-sourcing-other---scripts">Data sourcing (other) -
Scripts</a></li>
<li><a
href="#data-sourcing-other---description-of-individual-scripts">Data
sourcing (other) - Description of individual scripts</a>
<ul>
<li><a
href="#id_01_import_other_datar"><em>01_Import_other_data.R</em></a></li>
</ul></li>
</ul></li>
<li><a
href="#iii-initial-data-processing-03_merging_and_geographic_delineation"><strong>III.
Initial data processing:
<code>03_Merging_and_geographic_delineation</code></strong></a>
<ul>
<li><a href="#initial-data-processing---scripts">Initial data processing
- Scripts</a></li>
<li><a
href="#initial-data-processing---description-of-individual-scripts">Initial
data processing - Description of individual scripts</a>
<ul>
<li><a href="#id_01_merge_datasetsr"><em>01_Merge_datasets.R</em></a>
<ul>
<li><a href="#detection-of-duplicates">Detection of duplicates</a></li>
<li><a href="#additional-data-preparation">Additional data
preparation</a></li>
</ul></li>
</ul></li>
</ul></li>
<li><a href="#iv-chronologies-04_chronologies"><strong>IV. Chronologies:
<code>04_Chronologies</code></strong></a>
<ul>
<li><a href="#chronologies---scripts">Chronologies - Scripts</a></li>
<li><a
href="#chronologies---description-of-individual-scripts">Chronologies -
Description of individual scripts</a>
<ul>
<li><a
href="#id_01_prepare_chron_control_tablesr"><em>01_Prepare_chron_control_tables.R</em></a>
<ul>
<li><a href="#calibration-curves">Calibration curves</a></li>
<li><a href="#types-of-chronology-control-points">Types of chronology
control points</a></li>
<li><a href="#chronology-table-preparation">Chronology table
preparation</a></li>
<li><a href="#percentage-carbon">Percentage carbon</a></li>
</ul></li>
<li><a
href="#id_02_run_age_depth_modelsr"><em>02_Run_age_depth_models.R</em></a>
<ul>
<li><a href="#multi-core-computation">Multi-core computation</a></li>
<li><a href="#iterations">Iterations</a></li>
</ul></li>
<li><a href="#id_03_predict_agesr"><em>03_Predict_ages.R</em></a></li>
<li><a
href="#id_04_save_ad_figuresr"><em>04_Save_AD_figures.R</em></a></li>
<li><a
href="#id_05_merge_chron_outputr"><em>05_Merge_chron_output.R</em></a></li>
</ul></li>
</ul></li>
<li><a href="#v-harmonisation-05_harmonisation"><strong>V.
Harmonisation: <code>05_Harmonisation</code></strong></a>
<ul>
<li><a href="#harmonisation---scripts">Harmonisation - Scripts</a></li>
<li><a
href="#harmonisation---description-of-individual-scripts">Harmonisation
- Description of individual scripts</a>
<ul>
<li><a href="#id_01_harmonisationr"><em>01_Harmonisation.R</em></a></li>
</ul></li>
</ul></li>
<li><a href="#vi-data-filtering-06_main_filtering"><strong>VI. Data
filtering: <code>06_Main_filtering</code></strong></a>
<ul>
<li><a href="#data-filtering---scripts">Data filtering -
Scripts</a></li>
<li><a href="#data-filtering---description-of-individual-scripts">Data
filtering - Description of individual scripts</a>
<ul>
<li><a href="#id_01_level_filteringr"><em>01_Level_filtering.R</em></a>
<ul>
<li><a href="#pollen-count-sum">Pollen count sum</a></li>
<li><a href="#age-criteria">Age criteria</a></li>
<li><a href="#level-age-extrapolation">Level age extrapolation</a></li>
<li><a href="#interest-period">Interest period</a></li>
<li><a href="#number-of-levels">Number of levels</a></li>
</ul></li>
</ul></li>
</ul></li>
<li><a href="#vii-outputs-07_outputs"><strong>VII. Outputs:
<code>07_Outputs</code></strong></a>
<ul>
<li><a href="#outputs---scripts">Outputs - Scripts</a></li>
<li><a href="#outputs---description-of-individual-scripts">Outputs -
Description of individual scripts</a>
<ul>
<li><a
href="#id_01_pollen_diagramsr"><em>01_Pollen_diagrams.R</em></a></li>
<li><a href="#id_02_save_assemblyr"><em>02_Save_assembly.R</em></a></li>
<li><a
href="#id_03_save_referencesr"><em>03_Save_references.R</em></a></li>
</ul></li>
</ul></li>
</ul></li>
<li><a href="#references">References</a></li>
</ul>
<div id="summary" class="section level2">
<h2>Summary</h2>
<p>The FOSSILPOL Workflow is structured in a modular manner, where all
steps are organised sequentially and guided by one main configuration
file (<em>Config file</em>) where all criteria and setup configurations
are pre-defined by the user.</p>
<div id="figure-1figure-1" class="section level3">
<h3>Figure 1<img src="figures/Workflow%20complete%20-%20600ppi.png"
alt="Figure 1" /></h3>
</div>
</div>
<div id="data-input" class="section level2">
<h2>Data input</h2>
<p>The FOSSILPOL workflow is set up in a way that data from <a
href="https://www.neotomadb.org/">Neotoma Paleoecological Database</a>
(“<em>Neotoma</em>” hereafter) are the primary data input. However,
other data sources can also be used in parallel by using our predefined
format (<a href="#figure-2figure-2">Fig. 2</a>). The user thus has the
flexibility to source data from either Neotoma or from another data
source as long as our predefined format file is used (see <a
href="#ii-data-sourcing-02_other_source">other data sourcing</a>).</p>
<p>Three additional data inputs are required for the initial set-up of
the Workflow:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Configuration file</strong>
(<code>00_Config_file.R</code>) - this contains all the user-selected
settings which will be applied throughout the Workflow. These range from
technical settings (e.g. location of the data storage) to specific
requirements (e.g. filtering criteria) for records to be included. An
overview of where the config critera are used through the Workflow is
summaried in (<a href="#figure-2figure-2">Fig. 2</a>).</p></li>
<li><p><strong>Geographical shapefiles</strong> - the workflow is
internally set-up in a way that data are processed by geographical
regions and shapefiles are used to assign relevant geographical
information to the records to process. First, the Workflow is
conceptualised for a global project, so the general structure of data
processing is done <em>per continent</em> (i.e. <code>region</code> =
“<em>continent</em>”), but the user can use any other delineation of
interest. The Workflow comes with a default shapefile roughly delimiting
continents, but it can be adjusted or replaced as per project needs.
Second, the taxonomic harmonisation of records is structured by
<em>harmonisation regions</em> provided by
<code>harmonisation region shapefile</code>. By default, this shapefile
is a copy of the continental shapefile, but as harmonisation tables are
region-specific (see next data input item) this shapefile needs to be
adjusted to represent the geographical delimitation of the harmonisation
regions used. Finally, if the user is interested in other biogeographic,
climatic, or ecological units of interest to be linked to each record
(e.g. ecozones, biome type, climate zones), then additional shapefiles
(or TIF files) can be added to the workflow (see <a
href="step_by_step_guide.html#0301-add">details here</a>).</p></li>
<li><p><strong>Harmonisation tables</strong> - in each project, one
harmonisation table must be provided per harmonisation region (delimited
by the corresponding harmonisation region shapefile, see above). A
harmonisation table always comes with two columns: i)
<code>original taxa</code>(original taxa) with taxonomic names
originally present in Neotoma and/or other data source in the project,
and ii) <code>level_1</code> (harmonised taxa) with the standardised
taxonomic names. The Workflow will detect if a harmonisation table has
been provided by the user, or otherwise create a new table with all
detected <em>raw</em> taxa names for each harmonisation region. The
latter can consequently serve as a template for harmonisation in the
<code>level_1</code> column (see <a
href="step_by_step_guide.html#05">details here</a>).</p></li>
</ol>
<div id="figure-2figure-2" class="section level3">
<h3>Figure 2<img src="figures/Workflow_Detailed.png"
alt="Figure 2" /></h3>
</div>
</div>
<div id="data-storage" class="section level2">
<h2>Data storage</h2>
<p>The Workflow will produce several files including <em>temporary
output files</em>, <em>stop-check</em> tables, and final
<em>outputs</em> (data assembly, figures, etc.):</p>
<ol style="list-style-type: decimal">
<li><p><strong>Temporary output files</strong>: the Workflow is set up
in a way that temporary (in-progress) data files are saved at various
stages of the Workflow. Each file will contain the date of creation for
easier organisation. When run multiple times, the Workflow will
automatically detect if there are any changes in a selected file and
only overwrite it, if an updated file is produced (this is powered by <a
href="https://github.com/HOPE-UIB-BIO/R-Utilpol-package"><code>{RUtilpol}</code>
package</a>). This also means that the user does not have to re-run the
whole Workflow but can re-run only specific parts. As the total size of
the files can become substantial, the user can specify if all files
should be stored within the project folder (default) or in another
directory (specified by using the <code>data_storage_path</code> in the
Config file). With such specification, and after running the
<code>00_Config_file.R</code> script, there will be an additional folder
structure created (see <a href="#code-block-3">Code block
3</a>).</p></li>
<li><p><strong><em>stop-checks</em> CSV tables</strong>: while running
the Workflow, there will be several times that a user will be asked to
check and, where necessary, adjust the produced CSV tables to
subsequently continue with the Workflow (i.e. re-run script). This is
done to oblige the user to check the produced intermediate results
before continuing. For example, at a certain point, the Workflow will
produce a list of all ecological groups detected within the dataset
compilation obtained from Neotoma. A user then has to edit the mentioned
CSV table and specify, which ecological groups should be kept
(<code>include</code> = <code>TRUE</code>) and which should be filtered
out (<code>include</code> = <code>FALSE</code>). Note that there are
several stop-checks throughout the Workflow (see overview in <a
href="#figure-2figure-2">Fig. 2</a>).</p></li>
<li><p><strong>Workflow output</strong> (<code>Outputs/</code>, see <a
href="#vii-outputs-07_outputs">Section VII</a> for more
information):</p>
<ul>
<li>a ready-to-use, taxonomically harmonised and temporally standardised
compilation of fossil pollen data, ready for the analytical stage (rds
format)</li>
<li>plots of modelled age-depth curves for each record (pdf format)</li>
<li>pollen diagram of each record (pdf format)</li>
<li>metadata table relaying the main data contributor, contact
information, and corresponding publications for citation purposes of the
used datasets (PDF).</li>
<li>reproducibility bundle, a zip file with contains all important
sections for a reproducibility of the whole project.<br />
</li>
<li>overview figures of the spatial and temporal distribution of the
dataset compilation, namely a map and a graph of the record lengths,
respectively (PDF).</li>
</ul></li>
</ol>
<div id="code-block-3" class="section level3">
<h3>Code block 3</h3>
<pre class="{r}"><code>data_storage_path
│
└───Data
│   │
│   └───Input
│   │   │
│   │   └───Chronology_setting
│   │   │   │
│   │   │   └───Bchron_crash
│   │   │   │
│   │   │   └───Chron_control_point_types
│   │   │   │
│   │   │   └───Percentage_radiocarbon
│   │   │
│   │   └───Depositional_environment
│   │   │   │
│   │   │   └───Neotoma
│   │   │   │
│   │   │   └───Other
│   │   │
│   │   └───Eco_group
│   │   │
│   │   └───Harmonisation_tables
│   │   │
│   │   └───Neotoma_download
│   │   │
│   │   └───Potential_duplicates
│   │   │
│   │   └───Other
│   │   │
│   │   └───Regional_age_limits
│   │   
│   └───Personal_database_storage
│   │
│   └───Processed
│       │
│       └───Chronology
│       │   │
│       │   └───Chron_tables_prepared
│       │   │
│       │   └───Models_full
│       │   │
│       │   └───Predicted_ages
│       │   │
│       │   └───Temporary_output
│       │
│       └───Data_filtered
│       │
│       └───Data_harmonised
│       │
│       └───Data_merged
│       │
│       └───Data_with_chronologies
│       │
│       └───Neotoma_processed
│       │   │
│       │   └───Neotoma_chron_control
│       │   │
│       │   └───Neotoma_dep_env
│       │   │
│       │   └───Neotoma_meta
│       │
│       └───Other
│ 
└───Outputs
    │
    └───Data
    │
    └───Figures
    │   │
    │   └───Chronology
    │   │
    │   └───Pollen_diagrams
    │   
    └───Tables
        │
        └───Meta_and_references</code></pre>
</div>
</div>
<div id="data-processing" class="section level2">
<h2>Data processing</h2>
<p>Here we focus on the scripts within the
<code>R/01_Data_processing</code> folder representing all steps needed
for data processing (from obtaining data to the final dataset
compilation), organised in the following Sections:</p>
<ol style="list-style-type: upper-roman">
<li><p><strong><a href="#i-data-sourcing-01_neotoma_source">Data
sourcing: <code>01_Neotoma_source</code></a></strong> - retrieve and
process data from Neotoma</p></li>
<li><p><strong><a href="#ii-data-sourcing-02_other_source">Data
sourcing: <code>02_Other_source</code></a></strong> - process data from
other data source (optional)</p></li>
<li><p><strong><a
href="#iii-initial-data-processing-03_merging_and_geographic_delineation">Initial
data processing:
<code>03_Merging_and_geographic_delineation</code></a></strong> - merge
data sources, filter out duplicates, and assign values based on
geographical location</p></li>
<li><p><strong><a href="#iv-chronologies-04_chronologies">Chronologies:
<code>04_Chronologies</code></a></strong> - prepare chronology control
tables, calculate age-depth models, and predict ages for levels</p></li>
<li><p><strong><a
href="#v-harmonisation-05_harmonisation">Harmonisation:
<code>05_Harmonisation</code></a></strong> - prepare all harmonisation
tables and harmonise pollen taxa (morphotypes)</p></li>
<li><p><strong><a href="#vi-data-filtering-06_main_filtering">Data
filtering: <code>06_Main_filtering</code></a></strong> - filter out
levels and records based on user-defined criteria</p></li>
<li><p><strong><a href="#vii-outputs-07_outputs">Outputs:
<code>07_Outputs</code></a></strong> - save the final output including
dataset compilation, pollen diagrams, metadata information, graphical
summary snd reproducibility bundle</p></li>
</ol>
<div id="i.-data-sourcing-01_neotoma_source" class="section level3">
<h3><strong>I. Data sourcing:
<code>01_Neotoma_source</code></strong></h3>
<div id="data-sourcing-neotoma---scripts" class="section level4">
<h4>Data sourcing (Neotoma) - Scripts</h4>
<ul>
<li><p><code>Run_01_01.R</code>- run all scripts within this
folder</p></li>
<li><p><a
href="#id_01_download_neotomar"><code>01_Download_neotoma.R</code></a>-
download the pollen data from the Neotoma database</p></li>
<li><p><a
href="#id_02_extract_samplesr"><code>02_Extract_samples.R</code></a> -
create a table from Neotoma´s downloaded lists and download author
information</p></li>
<li><p><a
href="#id_03_filter_dep_envr"><code>03_Filter_dep_env.R</code></a> - get
depositional environment data and filter out records based on the user
preferences</p></li>
<li><p><a
href="#id_04_extract_chron_control_tablesr"><code>04_Extract_chron_control_tables.R</code></a>
- get chronologies, including the preferred table with chronology
control points</p></li>
<li><p><a
href="#id_05_extract_raw_pollen_datar"><code>05_Extract_raw_pollen_data.R</code></a>
- extract the raw pollen counts from Neotoma and filter by user-selected
ecological groups.</p></li>
</ul>
</div>
<div id="data-sourcing-neotoma---description-of-individual-scripts"
class="section level4">
<h4>Data sourcing (Neotoma) - Description of individual scripts</h4>
<div id="download_neotoma.r" class="section level5">
<h5><em>01_Download_neotoma.R</em></h5>
<p>All pollen records are downloaded from Neotoma based on the
geographical criteria (spatial extent, <a href="#figure-2figure-2">Fig.
2</a> - config criteria <strong>1</strong>) and the selected data type,
in this case: <code>"pollen"</code>. Note that a more complex spatial
extent, like a polygon, can be used with the <code>loc</code> argument
in <code>RFossilpol::proc_neo_get_all_neotoma_datasets()</code>(see
usage of <code>loc</code> in <code>neotoma2</code> example <a
href="https://github.com/NeotomaDB/EPD_binder/blob/main/slides/slides2.pdf">here</a>).</p>
</div>
<div id="extract_samples.r" class="section level5">
<h5><em>02_Extract_samples.R</em></h5>
<p>Each record is processed using a unique dataset ID
(<code>dataset_id</code>) with metadata information extracted. Metadata
includes information about the name of the record, geographical
information, and the authors and publication DOI connected to the
dataset. The authors and their link to the dataset are saved into a
separate Author-Dataset database created specifically for each project.
It allows easy extraction of authors and DOI for the final dataset
compilation produced by the Workflow.</p>
</div>
<div id="filter_dep_env.r" class="section level5">
<h5><em>03_Filter_dep_env.R</em></h5>
<p>Depositional information from each record gives information about the
environments where a record was extracted. Based on the research
question, there could be a preference for certain environments
(e.g. terrestrial vs. marine). Currently in Neotoma, the data about
depositional environments are organised in a hierarchical structure
(e.g. “<em>Pond</em>” is nested in “<em>Natural Lake</em>”, which is
nested in “<em>Lacustrine</em>”), in which the maximum number of nested
layers is five. At the lowest hierarchical level, there are currently
over 50 different categories of depositional environments (for fossil
pollen records). Based on the selected records, the Workflow will
produce a list of all depositional environments (and their hierarchical
position) present in the user´s data selection. The user is then
requested to define the environments of choice (this is a <a
href="#data-storage"><strong><em>stop-check</em></strong></a> point, <a
href="#figure-2figure-2">Fig. 2</a>). Note that excluding depositional
environments with the higher hierarchical position will
<strong>not</strong> automatically exclude all depositional environments
nested in it.</p>
</div>
<div id="extract_chron_control_tables.r" class="section level5">
<h5><em>04_Extract_chron_control_tables.R</em></h5>
<p>Chronology data for each record is available in a table that contains
information about chronology control points used to construct an
age-depth model. Some records can have multiple chronology tables as
some records have been used for several projects or recalibrated
(updated) by data stewards. These tables are numbered according to the
order in which they were created and uploaded. Each chronology comes
with the age-unit of the age-depth model output (e.g. “<em>Radiocarbon
years BP</em>”, “<em>Calibrated radiocarbon years BP</em>”) and the
temporal range of the record (youngest and oldest age). The chronologies
in “<em>Radiocarbon years BP</em>” are often older chronologies as it is
now common practice to recalibrate radiocarbon-dated material and
produce chronologies expressed in “<em>Calibrated radiocarbon years
BP</em>”. Note: The chronologies in “<em>Calibrated radiocarbon years
BP</em>” still come with chronology table(s) containing the uncalibrated
radiocarbon ages and need to be calibrated by the user if a new
age-depth model is desired. The Workflow automatically selects one table
per record based on the order defined by <code>chron_order</code> in the
Config file (<a href="#figure-2figure-2">Fig. 2</a> - config criteria
<strong>2</strong>). Note: if more tables have the same age-unit type
(e.g. Calibrated radiocarbon years BP), the Workflow will opt for the
more recent table. The user can specify their preference for certain
age-unit types in the Config file. In addition, only records which have
at least a certain number of control points (defined by
<code>min_n_of_control_points</code> in the Config file, <a
href="#figure-2figure-2">Fig. 2</a> - config criteria
<strong>3</strong>) will be subsequently used.</p>
</div>
<div id="extract_raw_pollen_data.r" class="section level5">
<h5><em>05_Extract_raw_pollen_data.R</em></h5>
<p>Each level of each record comes with additional information: a)
unique sample ID (sample_id), b) information about depth (and estimated
age later), and c) pollen counts for each taxon present in that level.
The information about levels is split into two different tables (first
with depth and ages, and second with pollen counts) linked together by
sample ID (<code>sample_id</code>).</p>
<p>The Workflow will only keep records with a minimal number of levels
as defined in the Config file (<code>min_n_levels</code>, <a
href="#figure-2figure-2">Fig. 2</a> - config criteria
<strong>4</strong>). The minimum number of levels is by default selected
as three but the user can change this setting.</p>
<p>In the case of data sourced from Neotoma, each pollen taxon has
information about the ecological group (e.g. palms, mangroves, etc).
Based on the selected records, the Workflow will produce a full list of
all ecological groups after which the user is requested to define which
ecological groups to include (a <a
href="#data-storage"><strong><em>stop-check</em></strong></a> point, <a
href="#figure-2figure-2">Fig. 2</a>, see explanation of abbreviation in
<a
href="#table-1-ecological-groups-assigned-to-pollen-taxa-as-defined-in-neotoma">Table
1</a>)</p>
<div
id="table-1.-ecological-groups-assigned-to-pollen-taxa-as-defined-in-neotoma"
class="section level6">
<h6>Table 1. Ecological groups assigned to pollen taxa as defined in
Neotoma</h6>
<table>
<colgroup>
<col width="50%" />
<col width="50%" />
</colgroup>
<thead>
<tr class="header">
<th>ABBREVIATION</th>
<th>ECOLOGICAL GROUP</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>ACRI</td>
<td>Acritarchs</td>
</tr>
<tr class="even">
<td>ANAC</td>
<td>Anachronic</td>
</tr>
<tr class="odd">
<td>ALGA</td>
<td>Algae (e.g. Botryococcus)</td>
</tr>
<tr class="even">
<td>AQB</td>
<td>Aquatics (e.g. Sphagnum)</td>
</tr>
<tr class="odd">
<td>AQVP</td>
<td>Aquatic vascular plants (e.g. Isoetes)</td>
</tr>
<tr class="even">
<td>BIOM</td>
<td>Biometric measurements</td>
</tr>
<tr class="odd">
<td>EMBR</td>
<td>Embryophyta</td>
</tr>
<tr class="even">
<td>FUNG</td>
<td>Fungi</td>
</tr>
<tr class="odd">
<td>LABO</td>
<td>Lab analyses</td>
</tr>
<tr class="even">
<td>MAN</td>
<td>Mangroves</td>
</tr>
<tr class="odd">
<td>PALM</td>
<td>Palms</td>
</tr>
<tr class="even">
<td>PLNT</td>
<td>Plant</td>
</tr>
<tr class="odd">
<td>SEED</td>
<td>Unidentified, but definitely pollen - Spermatophyte rank or
clade</td>
</tr>
<tr class="even">
<td>SUCC</td>
<td>Succulents</td>
</tr>
<tr class="odd">
<td>TRSH</td>
<td>Trees and shrubs</td>
</tr>
<tr class="even">
<td>UNID</td>
<td>Unknown and Indeterminable</td>
</tr>
<tr class="odd">
<td>UPBR</td>
<td>Upland bryophytes</td>
</tr>
<tr class="even">
<td>UPHE</td>
<td>Upland herbs</td>
</tr>
<tr class="odd">
<td>VACR</td>
<td>Terrestrial vascular cryptogams</td>
</tr>
<tr class="even">
<td>VASC</td>
<td>Vascular plants</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<div id="ii.-data-sourcing-02_other_source" class="section level3">
<h3><strong>II. Data sourcing:
<code>02_Other_source</code></strong></h3>
<p>Our FOSSILPOL Workflow allows the use of other data source in
combination with the Neotoma data. Including other data sources is fully
optional and can be skipped as indicated by
<code>use_other_datasource</code> = <code>TRUE</code>/<code>FALSE</code>
in the Config file.</p>
<p>Any data can be used as long as it contains the following required
information: a) metadata, b) depositional environment, c) chronology, d)
level (age-depth), and e) pollen counts. In order to prepare data for
usage, the user needs to download <a
href="https://figshare.com/articles/dataset/FOSSILPOL-private_data-template/19794112">file
template</a> specially prepared for this. Each pollen record needs to be
stored as a separate file with <strong>unique</strong> name. We
recommend e.g., <code>private_data_(insert_site_name).xlsx</code>. The
<em>site name</em> in the filename is crucial as it will be compared
against all other pollen records in Neotoma to test for potential
duplicates in a later stage of the Workflow. All files must be stored in
<code>/Data/Input/Other/</code> (or specified by <code>dir_files</code>
argument, see below).</p>
<div id="data-sourcing-other---scripts" class="section level4">
<h4>Data sourcing (other) - Scripts</h4>
<ul>
<li><code>Run_01_02.R</code> - run all scripts within this folder</li>
<li><a
href="#id_01_import_other_datar"><code>01_Import_other_data.R</code></a>
- source other data sources and filter the records in a similar way as
Neotoma.</li>
</ul>
</div>
<div id="data-sourcing-other---description-of-individual-scripts"
class="section level4">
<h4>Data sourcing (other) - Description of individual scripts</h4>
<div id="import_other_data.r" class="section level5">
<h5><em>01_Import_other_data.R</em></h5>
<p>The sourcing of other data sources follows a simple order of
actions:</p>
<ol style="list-style-type: decimal">
<li><p>Data files need to be prepared by the user following the template
one record per file.</p></li>
<li><p>Data are extracted and formatted to be compatible with Neotoma
data using the <code>RFossilpol::import_datasets_from_folder()</code>
function, with the following arguments:</p>
<ul>
<li><code>dir_files</code> - user can specify which folder contains the
prepared data (default = <code>Data/Input/Other/</code></li>
<li><code>suffix</code> - argument to keep track of the source of the
data. Default is set to <code>"other"</code>, which means that datasets
can be easily identified as their name will be
<code>(dataset id)_other</code></li>
<li><code>source_of_data</code> - will flag the source of each dataset
in the compilation in meta-data overview (see <a
href="#vii-outputs-07_outputs">section VII</a>). Default is set to
<code>"personal_data"</code></li>
<li><code>data_publicity</code> - will flag the data publicity of each
dataset in the compilation in meta-data overview (see <a
href="#vii-outputs-07_outputs">section VII - Outputs</a>). Default is
set to <code>"restricted"</code></li>
<li><code>pollen_percentage</code> - pollen counts measured as
proportions (e.g., from scanning of pollen diagrams) can be flagged
here. Default set to <code>FALSE</code></li>
</ul></li>
<li><p>Names of data contributors are extracted and added to the <a
href="#id_02_extract_samplesr">Author-Dataset database</a> used in
Author-dataset attribution (see <a
href="#vii-outputs-07_outputs">section VII - Outputs</a>).</p></li>
<li><p>Data are treated in a similar way as data from Neotoma, in terms
of filtering by geographical location, number of levels (<a
href="#figure-2figure-2">Fig. 2</a> - config criteria
<strong>5</strong>,<strong>6</strong>), and depositional environments
(<a href="#data-storage"><strong><em>stop-check</em></strong></a> point,
<a href="#figure-2figure-2">Fig. 2</a>).</p></li>
</ol>
</div>
</div>
</div>
<div
id="iii.-initial-data-processing-03_merging_and_geographic_delineation"
class="section level3">
<h3><strong>III. Initial data processing:
<code>03_Merging_and_geographic_delineation</code></strong></h3>
<div id="initial-data-processing---scripts" class="section level4">
<h4>Initial data processing - Scripts</h4>
<ul>
<li><code>Run_01_03.R</code> - run all scripts within this folder</li>
<li><a
href="#id_01_merge_datasetsr"><code>01_Merge_datasets.R</code></a> -
merge data from all sources, filter out duplicates, and assign values
based on geographical location</li>
</ul>
</div>
<div id="initial-data-processing---description-of-individual-scripts"
class="section level4">
<h4>Initial data processing - Description of individual scripts</h4>
<div id="merge_datasets.r" class="section level5">
<h5><em>01_Merge_datasets.R</em></h5>
<p>After initial data processing, records from Neotoma and Other sources
are merged together.</p>
<div id="detection-of-duplicates" class="section level6">
<h6>Detection of duplicates</h6>
<p>There is a possibility that some datasets from the other data sources
are already in Neotoma. To avoid duplication within the final dataset
compilation, the Workflow will compare datasets from both sources and
identify potential duplicates. This step is optional but recommended to
follow. To do so, the user needs to specify that
<code>detect_duplicates</code> == <code>TRUE</code> in the Config file
(this is set as default, <a href="#figure-2figure-2">Fig. 2</a> - config
criteria <strong>7</strong>). The Workflow will start a simple
subroutine using the function
<code>RFossilpol::proc_filter_out_duplicates()</code>. Because comparing
all records within each data source to each other is relatively
computationally demanding, the function will split the data into several
groups using their geographical location (ca. 100 records per group).
The user can define the number of groups using the
<code>n_subgroups</code> argument. Next, each record from one source is
compared to all records from the other source as long as they are within
1 degree radius (the assumption here is that duplicated records will be
in a similar location). The user can define the maximum distance using
the <code>max_degree_distance</code> argument. Finally, the Workflow
will output a list of potential duplicated records (a <a
href="#data-storage"><strong><em>stop-check</em></strong></a> point, <a
href="#figure-2figure-2">Fig. 2</a>). For each record-pair, the user
must specify, which records should be deleted by writing <code>1</code>
(deleting the Neotoma) or <code>2</code> (deleting the other data
source) in the <code>delete</code> column of the created list (leaving
<code>0</code> will leave both records in).</p>
</div>
<div id="additional-data-preparation" class="section level6">
<h6>Additional data preparation</h6>
<p>Several more steps take place to create the fully merged dataset
compilation before proceeding to the chronology step (does not require
any action by user):</p>
<ol style="list-style-type: decimal">
<li><p>All taxon names are transformed into a more computer-friendly
format for easier manipulation. The
<code>RFossilpol::proc_clean_count_names()</code> function will first
transform special characters to text (e.g. <code>+</code> to
<code>_plus_</code>) and then use the <a
href="https://sfirke.github.io/janitor/">{janitor} package</a> to
transform into <a
href="https://en.wikipedia.org/wiki/Snake_case">“<em>snake_case</em>”
style</a>. In addition, the user can specify additional specific changes
in names (e.g. based on presence of special characters) by adjusting the
<code>user_name_patterns</code> argument (see example in the script).
During this cleaning of taxa names, the Workflow will save
<code>taxa_refrence_table</code> for back traceability to Neotoma
taxonomy. The <code>taxa_reference_table</code> is a CSV file saved in
the same folder as the harmonisation tables
(<code>Data/Input/Harmonisation_tables/</code>). More about <a
href="#v-harmonisation-05_harmonisation">harmonisation
process</a>.</p></li>
<li><p>Individual levels (sample depths) are sorted by their depth for
each record by <code>RFossilpol::proc_prepare_raw_count_levels()</code>.
This includes subroutines, for example, only keeping levels present in
all data tables, filtering out levels without pollen data, and taxa
which are not present in any level.</p></li>
<li><p>Spatial information for each record is assigned based on the
provided geographical shapefiles. Specifically:</p>
<ul>
<li><p><strong>Region information</strong> - the shapefile in
<code>Data/Input/Spatial/Regions_shapefile</code> will assign the
regional names for each record (see <a href="#data-input">Data input
Section</a>). The user can (and is recommended to) change the spatial
delineation of the data by altering or replacing the shapefile.</p></li>
<li><p><strong>Political delineation (countries)</strong> - obtained
from <a href="www.gadm.org">GADM database</a>, version 2.8, November
2015.</p></li>
<li><p><strong>Harmonisation region</strong> - the shapefile in
<code>Data/Input/Spatial/Harmonisation_regions_shapefile</code> will
assign the harmonisation region (to be able to link the corresponding
harmonisation table to use; See see <a href="#data-input">Data input
Section</a>). The default shapefile in the Workflow is a copy of the
Region information shapefile but should be adjusted by the user to
correspond to the area covered by the different harmonisation
tables.</p></li>
<li><p><strong>Calibration curves (normal and post-bomb)</strong> -
depending on the geographical position of the record, a different
calibration curve needs to be assigned, as different curves are used for
the northern and southern hemispheres, and for terrestrial and marine
environments. See more details about <a
href="#calibration-curves">calibration curves</a>.</p></li>
<li><p><strong>Additional</strong> - The user can add any additional
spatial delimitation (e.g. ecozones). This will require adding the
specific shapefile (or TIF file) in
<code>/Data/Input/Spatial/NAME_OF_FOLDER</code> and adjusting the R code
manually (<code>optional_info_to_assign</code>) so that the shapefile is
sourced, and its information assigned to each record (see the example in
the script).</p></li>
</ul></li>
<li><p>The Workflow will create a new table with age limitations for
each region presented in the data, which needs to be edited by the user
(a <a href="#data-storage"><strong><em>stop-check</em></strong></a>
point, <a href="#figure-2figure-2">Fig. 2</a>). For example,
<code>Regional_age_limits</code> table will have the following
values:</p>
<ul>
<li><code>young_age</code> = youngest age the record must have</li>
<li><code>old_age</code> = oldest age the record must have</li>
<li><code>end_of_interest_period</code> = levels beyond this age will be
omitted</li>
</ul></li>
</ol>
</div>
</div>
</div>
</div>
<div id="iv.-chronologies-04_chronologies" class="section level3">
<h3><strong>IV. Chronologies: <code>04_Chronologies</code></strong></h3>
<p>To estimate the age of individual levels based on their depth, an
age-depth model needs to be constructed based on the chronology data of
the record. An age-depth model will provide age estimates of each
individual level and the full age range of the record.</p>
<p>Age-depth modelling can be very computationally heavy and can take a
substantial amount of time. Therefore, the Workflow automatically
processes several files (rds format):</p>
<ul>
<li><em>Chronology-control-tables</em>:
<ul>
<li><code>/Data/Processed/Chronology/Chron_tables_prepared/chron_tables_prepared*.rds</code>
contains all the chronology control tables prepared to be
re-calibrated</li>
</ul></li>
<li><em>Age-depth-models</em>:
<ul>
<li><code>/Data/Processed/Chronology/Models_full/*</code> contains one
file for each age-depth model</li>
</ul></li>
<li><em>Predicted-ages</em>:
<ul>
<li><code>/Data/Processed/Chronology/Predicted_ages/predicted_ages*.rds</code>
contains the predicted ages using the age-depth models</li>
</ul></li>
</ul>
<p>It may be <strong>not</strong> preferential to re-run all age-depth
models every time the user wants to re-run the Workflow. Therefore, the
Workflow offers the option to save the successful age-depth models
(<em>Age-depth-models</em> &amp; <em>Predicted-ages</em>) and keep them
between individual runs. This can be done in several ways:</p>
<ul>
<li><p><code>recalib_AD_models</code> in the Config file can be set to
<code>FALSE</code>. This will result in completely omitting the scripts
aimed at calculating age-depth models, and therefore this step will be
skipped. Note that this will only work if the age-depth models have
already been successfully created at least once (<em>Predicted-ages</em>
exists).</p></li>
<li><p><code>calc_AD_models_denovo</code> in the Config file can be set
to <code>FALSE</code> (this is the default). When set to
<code>TRUE</code>, the Workflow will <strong>not</strong> use the
<em>Age-depth-models</em> files (successful age-depth models) and will
re-calibrate everything “<em>de novo</em>”.</p></li>
<li><p><code>predict_ages_denovo</code> in the Config file is set to
<code>FALSE</code> as the default. When set to <code>TRUE</code>, the
Workflow will use all the age-depth model files but re-assign the ages
to each level of all related records (even for records where ages were
successfully predicted). This is done, for instance, when the number of
levels in a record increased since the last Workflow run and these
levels still need to have ages assigned.</p></li>
</ul>
<p><strong>IMPORTANT NOTE</strong>: If you select both
<code>calc_AD_models_denovo</code> and <code>predict_ages_denovo</code>
as <code>FALSE</code> for your first run of age-depth modelling, you
might be asked by the Workflow to temporarily switch both to
<code>TRUE</code> in the console (you do not have to change anything in
the Config file).</p>
<div id="chronologies---scripts" class="section level4">
<h4>Chronologies - Scripts</h4>
<ul>
<li><code>Run_01_04.R</code> - run all scripts within this folder</li>
<li><a
href="#id_01_prepare_chron_control_tablesr"><code>01_Prepare_chron_control_tables.R</code></a>
- prepare the chronology tables for age-depth modelling</li>
<li><a
href="#id_02_run_age_depth_modelsr"><code>02_Run_age_depth_models.R</code></a>-
create age-depth models with BChron</li>
<li><a href="#id_03_predict_agesr"><code>03_Predict_ages.R</code></a> -
estimate the ages of individual levels</li>
<li><a
href="#id_04_save_ad_figuresr"><code>04_Save_AD_figures.R</code></a> -
save visual output of the age-depth models in pdf format</li>
<li><a
href="#id_05_merge_chron_outputr"><code>05_Merge_chron_output.R</code></a>
- link age-depth models to corresponding datasets</li>
</ul>
</div>
<div id="chronologies---description-of-individual-scripts"
class="section level4">
<h4>Chronologies - Description of individual scripts</h4>
<div id="prepare_chron_control_tables.r" class="section level5">
<h5><em>01_Prepare_chron_control_tables.R</em></h5>
<p>Age-depth models are constructed using <em>chronology control
points</em> (usually radiocarbon dates) with known depth, estimated age,
and associated age uncertainties. Each record can and should ideally
have several such points saved in the <em>chronology control table</em>.
Each chronology control point has the following properties:</p>
<ul>
<li>Depth</li>
<li>Estimated age</li>
<li>Error of the estimated age</li>
<li>Type of the chronology control point</li>
<li>The calibration curve used (which is needed to convert the raw
radiocarbon ages to calendar ages (Note: radiocarbon <em>ages</em> are
not true calendar ages!)).</li>
</ul>
<p>This script will take several steps to prepare all records for
age-depth modelling, specifically:</p>
<ol style="list-style-type: decimal">
<li>Create and attach the necessary calibration curves</li>
<li>Select the preferred chronology control point types</li>
<li>Prepare chronology tables (include fixing issues with percentage
carbon (if necessary))</li>
</ol>
<div id="calibration-curves" class="section level6">
<h6>Calibration curves</h6>
<p>Calibration curves are assigned to each control point based on
several criteria. If a control point has a type flagged to be calibrated
(see next section), a calibration curve is assigned based on the
geographical position of the record. Note: only chronology control
points of uncalibrated radiocarbon ages need recalibration. FOSSILPOL
has a shapefile based on figure 7 in Hogg et al (2020) to correctly
assigned IntCal20, SHCal20 and a mixed calibration curve to each record.
We follow this recommendation “<em>…the use of (i) IntCal20 for areas
north of the ITCZ in June-August (dashed lines in Figure 7) which
receive NH air masses all year round, (ii) SHCal20 for areas south of
ITCZ in December-February (dotted lines in Figure 7) which see SH air
masses all year round, and (iii) a mixed curve for areas between the two
seasonal ITCZ positions shown in Figure 7, which receive northern air
masses in December-February and southern air masses in
June-August.</em>” FOSSILPOL also comes with a mixed curve that was
constructed using the <a
href="https://github.com/ahb108/rcarbon"><code>{rcarbon}</code>
package</a> with a proportion of curve contribution 1:1. See <a
href="https://chrono.qub.ac.uk/blaauw/clam.html">more info</a> on mixed
calibration curve use.</p>
<p>All calibration curves have an age limitation, i.e. each curve can be
used only for certain ages. Radiocarbon calibration curves currently do
not cover ages older than 55 thousand years. For the younger ages,
during the last century, there are issues with the Earth’s atmospheric
radiocarbon and a different set of calibration curves needs to be used.
This is caused by the deployment of nuclear weapons in the ’50s and
’60s, which caused a spike in atmospheric radiocarbon and resulted in
radiocarbon measurements from material in the following decades to
return often highly negative ages. Therefore, if the control point has a
radiocarbon age younger than 200 yr BP, a <em>post-bomb calibration
curve</em> is used instead (see further below). As with the normal
calibration curves, the geographical location of the record is taken
into account to assign the corresponding post-bomb calibration
curve.</p>
<p>Modern radiocarbon dates are calibrated by using one of the post-bomb
calibration curves (<code>nh_zone_1</code>, <code>nh_zone_2</code>,
<code>nh_zone_3</code>, <code>sh_zone_1_2</code>,
<code>sh_zone_3</code>), following Hua et al., 2013 and <a
href="http://calib.org/CALIBomb/">link</a>. The Workflow will
automatically assign the corresponding curve based on the geographical
location of the record (see function
<code>IntCal::copyCalibrationCurve()</code>). If modern radiocarbon
dates are detected, the Workflow will then display the detected records
to the user and apply the conversion according to the post-bomb curves
from the <a
href="https://cran.r-project.org/web/packages/IntCal/index.html"><code>{IntCal}</code>
package</a>.</p>
</div>
<div id="types-of-chronology-control-points" class="section level6">
<h6>Types of chronology control points</h6>
<p>Each control point in the control table has several properties:
unique ID, depth, age, error, thickness, and chronology control point
type (e.g. radiocarbon, biostratigraphic, annual laminations, tephra).
Each type of chronology control point has different age uncertainties.
For instance, many older records relied on indirect dating techniques
based on biostratigraphic layers, similar geological levels from other
records (e.g. a volcanic event), and pollen-based levels (e.g. the
appearance of a key taxon), among others and can have large age
uncertainties into thousands of years. Neotoma has over 50 different
chronology controls points that fall within the categories of
geochronological (e.g. lead-210, radiocarbon, uranium-series), relative
time scale (e.g. MIS5e, Heinrich Stadial 1, Late Wisconsin event),
stratigraphic (e.g. biostratigraphic events such as the introduction of
anthropogenic taxa), cultural (e.g. European Settlement Horizon), other
absolute dating methods (e.g. annual laminations, collection date), and
other dating methods (e.g. extrapolated or guesses). Only the chronology
control points in uncalibrated radiocarbon ages require recalibration
with the calibration curves as most, if not all, other control points
will be in calendar ages and no recalibration should be implemented.</p>
<p>A user has the option to select which control point types should be
accepted “as-is” and which should be calibrated (a <a
href="#data-storage"><strong><em>stop-check</em></strong></a> point, <a
href="#figure-2figure-2">Fig. 2</a>). The Workflow will automatically
produce a list of all detected control points from all selected records,
which includes columns called <code>include</code> and
<code>calibrate</code>. In the first, the user should indicate if the
chronology control point should be included. In latter, the user should
indicate if the point should be recalibrated using the calibration
curves, here uncalibrated radiocarbon ages.</p>
</div>
<div id="chronology-table-preparation" class="section level6">
<h6>Chronology table preparation</h6>
<p>The chronology control tables will need to undergo a number of
user-defined adjustments:</p>
<ul>
<li><p>filtering out unwanted control point types selected by user
(defined by <a
href="#data-storage"><strong><em>stop-check</em></strong></a> point, see
above)</p></li>
<li><p>filtering out records that do not fulfil the minimal number of
control points (defined by <code>min_n_of_control_points</code> in the
Config file, default = 2, <a href="#figure-2figure-2">Fig. 2</a> -
config criteria <strong>8</strong>). The value
<code>min_n_of_control_points</code> will serve as a criterion for the
prepared chronology control tables, where records that do not fulfil
such requirements will be filtered out. Note that there is a trade-off
between accepting only the tables with a high number of control points
per time interval (more robust age-depth model) and the number of
records that will be able to fulfil strict criteria.</p></li>
<li><p>fixing instances of missing values. Defined in the Config file,
the values <code>default_thickness</code> (defined in the Config file,
default = <code>1</code>, <a href="#figure-2figure-2">Fig. 2</a> -
config criteria <strong>9</strong>) and <code>default_error</code>
(default = <code>100</code>, <a href="#figure-2figure-2">Fig. 2</a> -
config criteria <strong>10</strong>) will replace missing values
(<code>NA</code>) for thickness and error, specifically.</p></li>
<li><p>filtering out control points with an error that is considered too
big. Any control point with an error bigger than
<code>max_age_error</code> (defined in the Config file; default =
<code>3000</code> yr, <a href="#figure-2figure-2">Fig. 2</a> - config
criteria <strong>11</strong>) will be filtered out.</p></li>
<li><p>removing control points that are duplicated in terms of age
and/or depth.</p></li>
<li><p>in several cases, the chronology control point from the core-top
has a type specified as <code>guess</code>. A user can specify that the
type <code>guess</code> is only acceptable to a certain depth using the
<code>guess_depth</code> variable in the Config file (default is
<code>10</code> cm, <a href="#figure-2figure-2">Fig. 2</a> - config
criteria <strong>12</strong>)</p></li>
</ul>
<p>In addition, the number and distribution of such control points can
give a good indicator of the temporal uncertainty around levels’ ages
(Giesecke et al. 2014 [VHA], Flantua et al. 2016 [CP]). For example, a
record with few chronology control points within the focus time period
will have large uncertainties of predicted ages. Hence, the information
of the quality of chronologies, i.e. taking into account the types and
levels of chronology control points, can be a criterion used in the
selection of records.</p>
</div>
<div id="percentage-carbon" class="section level6">
<h6>Percentage carbon</h6>
<p>There are three ways by which post-bomb radiocarbon dates are
reported, namely by 1) modern radiocarbon dates (&lt;0 F14C yr); 2)
percent modern carbon (pMC, normalised to 100%); and 3) fraction
radiocarbon (F14C, normalised to 1; Reimer et al. 2004). Currently,
there is no direct way to know from Neotoma whether the dates are in pMC
or F14C. Even if the cases are likely to be few, such dates need to be
checked to avoid incorrect calculations.</p>
<p>The strongest rule of thumb is that normal radiocarbon dates and
errors should always be integer (no decimals) and are uploaded so by the
data stewards to Neotoma. The second rule of thumb is that pMC control
points are characterised by an age value from around 100 with a decimal
place. Thus, the Workflow will automatically export suspicious records
and the user must specify which control points should be
back-transformed (a <a
href="#data-storage"><strong><em>stop-check</em></strong></a> point, (<a
href="#figure-2figure-2">Fig. 2</a>).). For those selected by the user
to be back-transformed (<code>include</code> == <code>TRUE</code>), the
Workflow will first convert the pMC values to <em>normal</em> post-bomb
radiocarbon ages (negative 14C ages) by using the
<code>IntCal::pMC.age()</code> function, after which normal post-bomb
calibration curves are used to calibrate the values to final calendar
ages (see above).</p>
<p>Although F14C has been recommended by experts for the reporting of
post-bomb samples (Reimer et al. 2004 [Radiocarbon], in practice, the
bulk of the reporting is done as modern radiocarbon dates followed by
pMC to a much lesser extent. The Workflow currently does not deal with
F14C as no such cases have been detected to date. As this is not
consistent with the rest of the data, back-transformation could be done
when working with other data sources, and the <a
href="https://cran.r-project.org/web/packages/IntCal/index.html"><code>{IntCal}</code>
package</a> can be used to do so.</p>
</div>
</div>
<div id="run_age_depth_models.r" class="section level5">
<h5><em>02_Run_age_depth_models.R</em></h5>
<p>Individual age-depth models are estimated using the <a
href="http://andrewcparnell.github.io/Bchron/"><code>{Bchron}</code>
package</a>, which estimates the Bayesian probabilistic trajectory of
the age-depth model curve (non-parametric chronology model to age-depth
data according to the Compound Poisson-Gamma model). Therefore, it is
suitable for various combinations of control point types, outliers, and
age reversals.</p>
<p>If there are many ages at close or similar depths (e.g. annual
laminations), initialisation problems may occur and the Bchron could
fail to converge the age-depth model. In such a scenario, the thickness
of duplicated chronology control points is automatically increased by
0.01 (see <code>artificialThickness</code> argument in
<code>Bchron::Bchronology()</code> function).</p>
<div id="multi-core-computation" class="section level6">
<h6>Multi-core computation</h6>
<p>Because creating age-depth models for multiple records can be
computationally demanding, the Workflow uses multi-core
(<em>parallel</em>) computation. The Workflow automatically detects the
number of cores for the machine on which the code is running (this can
be adjusted by specifying the number in <code>number_of_cores</code> in
the Config file, <a href="#figure-2figure-2">Fig. 2</a> - config
criteria <strong>14</strong>). Several age-depth models are then created
at the same time. This is done by splitting the records into batches,
with each batch containing a certain number of records (see
<code>batch size</code> in Config file, <a href="#figure-2figure-2">Fig.
2</a> - config criteria <strong>13</strong>; by default it is based on
the <code>number_of_cores</code>). If <code>number_of_cores</code> is
selected (or detected) as <code>1</code>, the Workflow will not use the
batch approach (as obsolete) and estimate the age-depth models
one-by-one (see below).</p>
<p>Note that there is a possibility that the age-depth model estimation
will crash (freeze) for unforeseen reasons. Therefore, the Workflow is
structured in a way that if an estimation of the whole batch crashes,
the Workflow will skip that batch and continue with other batches. The
user can specify how long the machine should wait before skipping the
batch with the <code>time_per_record</code> argument in the
<code>RFossilpol::chron_recalibrate_ad_models()</code> function.</p>
<p>For each batch, the Workflow will try to estimate age-depth models
three times. The user can specify the number of attempts by changing the
<code>batch_attempts</code> in the
<code>RFossilpol::chron_recalibrate_ad_models()</code> function. If the
age-depth modelling is stopped in the process, the Workflow will
automatically use the previously successful batches, which are saved
automatically. This should help when the user needs to stop the
age-depth modelling and resume it at a later time.</p>
<p>Next, the Workflow continues to another subroutine where age-depth
models for records from <em>failed</em> batches are estimated one by
one. Similar to batch estimation, the age-depth model estimation is
tried three times for each <em>crashed</em> record until successful at
least once. On some rare occasions, a record could cause R to freeze
completely and prevents it from skipping to another record. In that
case, the dataset ID of the dataset that caused the crash will be
automatically written in the <em>Crash file</em> (found in
<code>/Data/Input/Chronology_setting/Bchron_crash/</code>) and omitted
from the future run of age-depth models. The user is recommended to do a
detailed check of the specific dataset, e.g. the chronology control
table for possible inconsistencies or other flaws that could be causing
<code>{BChron}</code> to fail to produce an age-depth model.</p>
</div>
<div id="iterations" class="section level6">
<h6>Iterations</h6>
<p>In the Config file, the number of iterations is set to
<code>50,000</code> by default, discarding the first <code>10,000</code>
iterations (<em>burn-in</em>) and keeping every iteration beyond the
burn-in with a step size of <code>40</code> (<em>thinning</em>) (<a
href="#figure-2figure-2">Fig. 2</a> - config criteria
<strong>15</strong>,<strong>16</strong>,<strong>17</strong>). The user
can change the number of iterations by altering the
<code>iteration_multiplier</code> in the Config file (<a
href="#figure-2figure-2">Fig. 2</a> - config criteria
<strong>18</strong>). This will result in changing the total
interactions but keeping the ratios of burn-ins and thinning. Thus
<code>1000</code> ((<code>50k</code> - <code>10k</code>) /
<code>40</code>) posterior values are drawn. The default number of
iterations should produce a robust estimation but they can be increased
by the user if preferred (by increasing the
<code>iteration_multiplier</code>). Note that increasing the
<code>iteration_multiplier</code> will automatically increase the time
that the program will wait for estimation of an age-depth model before
skipping it (see <code>time_per_record</code> above).</p>
<p>The user should keep in mind that creating age-depth models for
hundreds of records using a high number of iterations is computationally
demanding and can take a significant amount of time in the order of tens
of hours or several days.</p>
</div>
</div>
<div id="predict_ages.r" class="section level5">
<h5><em>03_Predict_ages.R</em></h5>
<p>With a successful age-depth model, the ages of individual levels are
estimated. The Workflow will estimate age and the error estimate for
each level, which will encapsulate 95% of all age posterior values
(<em>upper</em> and <em>lower</em> boundary). As <code>{BChron}</code>
uses a probabilistic model it is possible to obtain <em>possible</em>
ages of each level. To do so, a number of ages (default =
<code>1000</code>, see above) are drawn from the model posterior
representing all the <em>possible</em> ages, and a series of quantiles
of various values (<code>25</code>, <code>50</code>, <code>75</code>,
etc.) are then calculated. The 50th quantile (<em>median</em>) is used
as the final age of each level, and the 2.5th and 97.5th quantiles as
upper and lower boundaries respectively. This results in the age
estimate of each level including its error estimates. The whole matrix
<em>levels by posterior drawn (possible age)</em> is saved as an <em>age
uncertainty matrix</em> (<code>age_uncertainty</code> column). This
whole process is completely automated and does not require any input
from the user.</p>
</div>
<div id="save_ad_figures.r" class="section level5">
<h5><em>04_Save_AD_figures.R</em></h5>
<p>The visual representation of all age-depth models is saved as output
for visual confirmation of successful model estimation. The files (as
PDFs) are stored in the <code>/Outputs/Figures/Chronologies/</code>
folder and split into subfolders defined by region. The properties of
the figures (size of figures, font size, etc) can be altered in the
Config file (<code>image_width</code>, <code>image_height</code>,
<code>image_units</code>, <code>image_dpi</code>,
<code>text_size</code>, <code>line_size</code>). They can be used for
inspection of the age-depth curves to look for unrealistically large age
estimates or error bars, hiatuses, or extreme extrapolations toward the
present or the past.</p>
</div>
<div id="merge_chron_output.r" class="section level5">
<h5><em>05_Merge_chron_output.R</em></h5>
<p>The successfully predicted ages are linked with all the records from
the various sources (Sections I-III). The individual levels of each
record are then ordered by the predicted ages and the same order of
levels is then applied to the tables with the pollen counts.</p>
</div>
</div>
</div>
<div id="v.-harmonisation-05_harmonisation" class="section level3">
<h3><strong>V. Harmonisation:
<code>05_Harmonisation</code></strong></h3>
<p>The goal of taxonomic harmonisation is to standardise all site-level
names to the same pollen morphotypes (set of pollen and spore
morphotypes used for all pollen records) and thus reduce the effect of
taxonomic uncertainty and nomenclatural complexity (See relevant
literature in Appendix 1 of Flantua et al. 2023, GEB). For this purpose,
a <em>harmonisation table</em> can be created that groups the
morphotypes into the highest taxonomic level that is most likely to be
identified by most of the pollen analysts.</p>
<div id="harmonisation---scripts" class="section level4">
<h4>Harmonisation - Scripts</h4>
<ul>
<li><code>Run_01_05.R</code> - run all scripts within this folder</li>
<li><a href="#id_01_harmonisationr"><code>01_Harmonisation.R</code></a>
- prepare all harmonisation tables and harmonise the raw counts.</li>
</ul>
</div>
<div id="harmonisation---description-of-individual-scripts"
class="section level4">
<h4>Harmonisation - Description of individual scripts</h4>
<div id="harmonisation.r" class="section level5">
<h5><em>01_Harmonisation.R</em></h5>
<p>First, the Workflow will check the <em>harmonisation regions</em>
present in the data, defined by the shapefile (see <a
href="#data-input">Data input</a> and <a
href="#additional-data-preparation">Section III</a>), and confirm that
there is one harmonisation table per region (a <a
href="#data-storage"><strong><em>stop-check</em></strong></a> point, <a
href="#figure-2figure-2">Fig. 2</a>). If any table is missing (or the
Workflow is run for the first time), the Workflow will automatically
create a harmonisation table per harmonisation region, with all the raw
taxa names from all the records from within that region. Note that we
refer here to “taxon names” for simplicity but the identification is
often done at the level of family or genus, and are best referred to as
morphotypes.</p>
<p>Each harmonisation table is created so that each taxon can have two
columns:</p>
<ol style="list-style-type: lower-roman">
<li><p><code>taxon_name</code> which is the original name of the taxon
(morphotype) formatted into a more computer-friendly format
(<em>snake_case</em>)</p></li>
<li><p><code>level 1</code>, which should be used to merge various taxa
(morphotypes) into harmonised taxonomic units, specific for the
project.</p></li>
</ol>
<p>Note that in order to link the names to the original display in
Neotoma, user can use the <code>taxa_reference_table</code> (see <a
href="#additional-data-preparation">name cleaning process</a>).</p>
<p>The user can also define which taxa should be completely removed
during the harmonisation process (marked as <code>delete</code>), in
case of a taxonomic mistake or a palaeoecological proxy not of interest,
e.g. spores). The user can add additional columns
(e.g. <code>level_2</code>) and then specify which levels should be
included by altering the argument <code>harm_name</code> when using the
<code>RFossilpol::harmonise_all_regions()</code> function.</p>
<p>With these tables, each dataset is harmonised so that all taxa that
belong to the same harmonised taxa (morphotypes) are summed together in
each level - this process is applicable for both count and percentage
data. This process includes automatic detection of successful pollen
harmonisation by checking the total number of pollen grains before and
after harmonisation (it can be turned off by changing the argument
<code>pollen_grain_test</code> to <code>FALSE</code> when using the
<code>RFossilpol::harmonise_all_regions()</code> function).</p>
</div>
</div>
</div>
<div id="vi.-data-filtering-06_main_filtering" class="section level3">
<h3><strong>VI. Data filtering:
<code>06_Main_filtering</code></strong></h3>
<p>To obtain a comprehensive dataset compilation of multiple fossil
pollen records, to increase its overall quality, and to answer research
questions reliably, we recommend the user to further trim down the data
selection by filtering out individual levels and/or whole records. All
of these filtering criteria can be adjusted by the user in the Config
file:</p>
<ul>
<li><p><code>filter_by_pollen_sum</code> (<a
href="#figure-2figure-2">Fig. 2</a> - config criteria
<strong>20</strong>) - if <code>TRUE</code>, the Workflow will use the
quantity of counted pollen grains at each level as a factor in
determining the quality of the level.</p></li>
<li><p><code>filter_by_age_limit</code> (<a
href="#figure-2figure-2">Fig. 2</a> - config criteria
<strong>24</strong>) - if <code>TRUE</code>, the Workflow will filter
out records that do not span the user-defined time period (defined by
<code>young_age</code> and <code>old_age</code> in
<code>Regional_age_limits</code> table; see <a
href="#iii-initial-data-processing-03_merging_and_geographic_delineation">Section
III</a>).</p></li>
<li><p><code>filter_by_extrapolation</code> (<a
href="#figure-2figure-2">Fig. 2</a> - config criteria
<strong>25</strong>) - if <code>TRUE</code>, the Workflow will filter
out levels based on the number of years between their age and the last
chronology control point used for age-depth modelling
(<code>chron_control_limits</code>).</p></li>
<li><p><code>filter_by_interest_region</code> (<a
href="#figure-2figure-2">Fig. 2</a> - config criteria
<strong>27</strong>) - if <code>TRUE</code>, the Workflow will filter
out levels that are older than <code>end_of_interest_period</code>
(defined in <code>Regional_age_limits</code> table; see <a
href="#iii-initial-data-processing-03_merging_and_geographic_delineation">Section
III</a>) to subsequently reduce processing time during follow-up
analyses.</p></li>
<li><p><code>filter_by_number_of_levels</code> (<a
href="#figure-2figure-2">Fig. 2</a> - config criteria
<strong>28</strong>) - if <code>TRUE</code>, the Workflow will filter
out records based on the number of levels.</p></li>
</ul>
<p>In addition, two more options can be activated by the user:</p>
<ul>
<li><p><code>use_age_quantiles</code> (<a href="#figure-2figure-2">Fig.
2</a> - config criteria <strong>30</strong>) - if <code>TRUE</code>, the
Workflow will use the 95th age quantile (uncertainty of the level age)
throughout the data filtration process, i.e. the age of the level will
be assumed to be anywhere between the upper and lower boundary (see <a
href="#vi-data-filtering-06_main_filtering">Section IV</a>). This will
result in a more stable dataset compilation between the different
results of age-depth modelling (as a probabilistic result can output
slightly different results each time they are run). However, the final
dataset compilation may require some additional filtering before any
analyses, as the 95th age quantile can span very long time
periods.</p></li>
<li><p><code>use_bookend_level</code> (<a href="#figure-2figure-2">Fig.
2</a> - config criteria <strong>31</strong>) - if <code>TRUE</code>, the
Workflow will leave one additional level beyond the oldest age of the
user-defined time period. This will result in a <em>bookend</em> level,
which can help to anchor information beyond the period of
interest.</p></li>
</ul>
<div id="data-filtering---scripts" class="section level4">
<h4>Data filtering - Scripts</h4>
<ul>
<li><code>Run_01_06.R</code> - run all scripts within this folder</li>
<li><a
href="#id_01_level_filteringr"><code>01_Level_filtering.R</code></a> -
filter out levels and records based on the user-defined criteria
predefined in the Config file</li>
</ul>
</div>
<div id="data-filtering---description-of-individual-scripts"
class="section level4">
<h4>Data filtering - Description of individual scripts</h4>
<div id="level_filtering.r" class="section level5">
<h5><em>01_Level_filtering.R</em></h5>
<div id="pollen-count-sum" class="section level6">
<h6>Pollen count sum</h6>
<p>The number of counted pollen grains at each level is an index of data
quality. To obtain a reliable representation of the vegetation,
researchers often aim to count more than <code>300</code> pollen grains
(following Moore et al., 1991), but other recommendations may also have
been followed (&gt;<code>150</code>; e.g. Djamali &amp; Cilleros, 2020)
and will vary with region and by scientific question (Birks &amp; Birks,
1980). For example, to achieve a representative sample of the regional
pollen pool, counts in Arctic records may only reach c. <code>100</code>
grains per level, whereas counts in Mediterranean sites can be as high
as <code>1000</code> (Birks &amp; Birks, 1980, p. 165), but the main
determinant can also be the preference of the pollen analyst. Reasons
for low numbers (&lt;<code>100</code>) are often mainly due to time
constraints of the data contributor, but can also be natural
depositional phenomena causing poor pollen preservation, such as low
local pollen production in arctic or alpine environments. Given that
statistical inferential power is proportional to sample size, we
recommend defining a minimum number of total pollen grains in each
level. Subsequently, whole records can be selected on the proportion of
levels with a selected minimum number of pollen grains counted per
level.</p>
<p>The user can select two different quantities of total pollen grains
per level: a) minimum number (<code>min_n_grains</code>, <a
href="#figure-2figure-2">Fig. 2</a> - config criteria
<strong>21</strong>) and b) acceptable number
(<code>target_n_grains</code>, <a href="#figure-2figure-2">Fig. 2</a> -
config criteria <strong>22</strong>). All levels with total pollen
grains below the minimum number will be filtered out. In addition, the
whole record will only be accepted if <em>X</em>% (set by
<code>percentage_samples</code>; default = <code>50</code>, <a
href="#figure-2figure-2">Fig. 2</a> - config criteria
<strong>23</strong>) of all levels fulfils at least the acceptable
number of pollen grains. This filtration criterion will only be used if
<code>filter_by_pollen_sum</code> == <code>TRUE</code>.</p>
</div>
<div id="age-criteria" class="section level6">
<h6>Age criteria</h6>
<p>As projects differ in their temporal focus, only a subset of records
will be of interest to the particular project. Therefore, records that
do not span a certain age period (from <code>young_age</code> to
<code>old_age</code>; specified by the <code>Regional_age_limits</code>
table, see <a
href="#iii-initial-data-processing-03_merging_and_geographic_delineation">Section
III</a>) will be filtered out. This filtration criterion will only be
used if <code>filter_by_age_limit</code> == <code>TRUE</code> in the
Config file.</p>
</div>
<div id="level-age-extrapolation" class="section level6">
<h6>Level age extrapolation</h6>
<p>Extrapolation of age inferences to samples beyond the set of
chronology control points is another factor in quality control. Levels
older than the oldest chronology control point have no other chronology
control point to constrain the age inference, and therefore, have
increasingly large uncertainty as the amount of extrapolation increases.
To limit the use of levels based on large extrapolations, we recommend
selecting a maximum extrapolation age, i.e. levels older than the
selected age criterion (e.g. <code>5000</code> yr) from the last
chronology control point are filtered out.</p>
<p>In order to limit the use of levels based on large extrapolations,
the user can select the maximum extrapolation age
(<code>maximum_age_extrapolation</code>, <a
href="#figure-2figure-2">Fig. 2</a> - config criteria
<strong>26</strong>), i.e. levels older than the selected criterion from
the last chronology control point will be filtered out. This filtration
criterion will only be used if <code>filter_by_extrapolation</code> ==
<code>TRUE</code> in the Config file.</p>
</div>
<div id="interest-period" class="section level6">
<h6>Interest period</h6>
<p>The individual levels of a record outside of the <em>interest
period</em> (<code>end_of_interest_period</code> specified by the
<code>Regional_age_limits</code> table) will be filtered out as they do
not provide additional information. This filtration criterion will only
be used if <code>filter_by_interest_region</code> == <code>TRUE</code>
in the Config file.</p>
</div>
<div id="number-of-levels" class="section level6">
<h6>Number of levels</h6>
<p>The total number of levels in a record is an important quality
criterion for further use of such a record in a specific analysis.
Records might have been sampled at low resolution (e.g. depth intervals
&gt; 30 cm) leaving substantial unassessed gaps - and thus time periods
- between levels. In addition, records with few levels will likely
contribute poorly to studies focused on specific time periods (many
non-value levels) and can cause unnecessary outlier values. Therefore,
we recommend selecting a minimum number of levels within the time period
of interest and use this as an additional criterion to filter out
unwanted records. The user can select the minimum number of levels
(<code>min_n_levels</code>, <a href="#figure-2figure-2">Fig. 2</a> -
config criteria <strong>29</strong>) that records must have at the end
of the filtration subroutine. This filtration criterion will only be
used if <code>filter_by_number_of_levels</code> == <code>TRUE</code> in
the Config file.</p>
</div>
</div>
</div>
</div>
<div id="vii.-outputs-07_outputs" class="section level3">
<h3><strong>VII. Outputs: <code>07_Outputs</code></strong></h3>
<div id="outputs---scripts" class="section level4">
<h4>Outputs - Scripts</h4>
<ul>
<li><code>Run_01_07.R</code> - run all scripts within this folder</li>
<li><a
href="#id_01_pollen_diagramsr"><code>01_Pollen_diagrams.R</code></a> -
save pollen diagrams for all records</li>
<li><a href="#id_02_save_assemblyr"><code>02_Save_assembly.R</code></a>
- save data assembly with selected variables (columns)</li>
<li><a
href="#id_03_save_referencesr"><code>03_Save_references.R</code></a> -
save reference and metatable</li>
</ul>
</div>
<div id="outputs---description-of-individual-scripts"
class="section level4">
<h4>Outputs - Description of individual scripts</h4>
<div id="pollen_diagrams.r" class="section level5">
<h5><em>01_Pollen_diagrams.R</em></h5>
<p>Pollen diagrams for all records will be created using the <a
href="https://cran.r-project.org/web/packages/rioja/rioja.pdf"><code>{rioja}</code>
package</a>. The harmonised data will be automatically transformed into
relative proportion for plotting purposes. The pollen diagrams will be
saved in the <code>/Outputs/Figures/Pollen_diagrams/</code> folder and
split into sub-folders defined by <em>Region</em> (e.g., continent).</p>
<p>The <code>RFossilpol::plot_all_pollen_diagrams()</code> function will
automatically produce a PDF of the pollen diagram split into several A4
pages and ready to be printed out. The y-axis is, by default, the age of
the levels, but it can be altered to depth (by the <code>y_var</code>
argument set to <code>"age"</code> or <code>"depth"</code>). The maximum
number of taxa per page can be altered by the <code>max_taxa</code>
argument (default = <code>20</code>). In addition, the function will
automatically omit very rare taxa. Specifically, the
<code>min_n_occur</code> argument will determine the minimum number of
occurrences per record each taxon has to have to be plotted.</p>
</div>
<div id="save_assembly.r" class="section level5">
<h5><em>02_Save_assembly.R</em></h5>
<p>A ready-to-use, taxonomically harmonised and standardised compilation
of fossil pollen data (ready for the analytical stage) is produced and
saved in the <code>/Outputs/Data/</code> folder. The file is saved as a
<em>tibble</em> (by <a
href="https://tibble.tidyverse.org/"><code>{tibble}</code> package</a>
from <a href="https://www.tidyverse.org/">tidyverse</a>) in
<code>rds</code> format.</p>
<p>The user can select which columns (variables) should be present in
the final data compilation by selecting
<code>select_final_variables</code> == <code>TRUE</code> in the Config
file. Here, the Workflow will interactively (in the R console) ask the
user to specify if each variable should be included (Yes/No).</p>
</div>
<div id="save_references.r" class="section level5">
<h5><em>03_Save_references.R</em></h5>
<p>The workflow will save all metadata and citation information needed
to create the dataset compilation. All outputs can be found in the
<code>/Outputs/Meta_and_references/</code> folder. The
function<code>RFossilpol::proc_save_references()</code> will do this
automatically but it can be specified by the user, i.e. the user can
specify what information should be saved using the
<code>user_sel_variables</code> argument. By default this information
contains:</p>
<ul>
<li><p><code>"meta_table"</code> - a <em>metadata table</em>
(<code>data_assembly_meta.csv</code>) contains the following variables
by default (note that this list may be altered by changing the final
variables by <code>select_final_variables</code>):</p>
<ul>
<li>the list of all records in the final dataset compilation</li>
<li>geographical location</li>
<li>depositional environment</li>
<li>assigned continental region</li>
<li>assigned harmonisation region</li>
<li>final number of levels</li>
<li>number of chronology control points used for age-depth
modelling</li>
<li>assigned calibration curve(s)</li>
<li>age limits of each record of each data source (Neotoma or other data
source)</li>
<li>DOI (from Neotoma).</li>
</ul></li>
<li><p><code>"author_table"</code> - a <em>reference table</em>
(<code>authors_meta.csv</code>) containing information about the
datasets used, the main data contributor, and their contact
information.</p></li>
<li><p><code>"affiliation_table"</code> - if an affiliation is provided
with the data from sources other than Neotoma, the <em>affiliation
table</em> (<code>affiliation_table</code>) is also exported linking
affiliations and their authors.</p></li>
<li><p><code>"graphical_summary"</code> - a PDF
(<code>graphical_summary*.pdf</code>) with three panel figures using
<code>RFossilpol::plot_graphical_summary()</code> function (note that
additional arguments can be passed to this function, see
<code>?RFossilpol::plot_graphical_summary</code> for more
information):</p>
<ul>
<li><ol style="list-style-type: upper-alpha">
<li>map - map of geographical location with each record represented as a
point</li>
</ol></li>
<li><ol start="2" style="list-style-type: upper-alpha">
<li>count of data records- a lollipop plot with the number of records in
each geographical group when assigned</li>
</ol></li>
<li><ol start="3" style="list-style-type: upper-alpha">
<li>age length - age limits of records, with each record represented by
a single line</li>
</ol></li>
</ul></li>
<li><p><code>"reproducibility_bundle"</code> - a zip file
(<code>reproducibility_bundle.zip</code>) of the Config file, all <a
href="#data-storage"><strong><em>stop-check</em></strong></a> CSV
tables, and all shapefiles. The idea is to increase the reproducibility
of the user´s workflow where ideally this zip file can be shared when
publishing a publication with a project created using the FOSSILPOL
Workflow. For reviewing purposes, the zip file can be shared and
detailed feedback can be provided before publication.</p></li>
</ul>
</div>
</div>
</div>
</div>
<div id="references" class="section level2">
<h2>References</h2>
<ul>
<li><p>Birks, H.J.B., Birks, H.H. (1980) Quaternary palaeoecology. E.
Arnold, London. 289 pages.</p></li>
<li><p>Djamali, M., Cilleros, K. (2020) Statistically significant
minimum pollen count in Quaternary pollen analysis; the case of
pollen-rich lake sediments. Review of Palaeobotany and Palynology 275,
pp.104156. ff10.1016/j.revpalbo.2019.104156</p></li>
<li><p>Flantua, S. G. A., Blaauw, M., &amp; Hooghiemstra, H. (2016).
Geochronological database and classification system for age
uncertainties in Neotropical pollen records. Climate of the Past, 12(2),
387-414. <a href="https://doi.org/10.5194/cp-12-387-2016"
class="uri">https://doi.org/10.5194/cp-12-387-2016</a></p></li>
<li><p>Flantua, S.G.A., Mottl, O., Felde, V.A., Bhatta, K.P., Birks,
H.H., Grytnes, J-A., Seddon, A.W.R., Birks H.J.B. (in press) A guide to
the processing and standardisation of global palaeoecological data for
large-scale syntheses using fossil pollen. Global Ecology and
Biogeography.</p></li>
<li><p>Giesecke, T., Davis, B., Brewer, S. et al. (2014) Towards mapping
the late Quaternary vegetation change of Europe. Vegetation History and
Archaeobotany 23, 75-86. <a
href="https://doi.org/10.1007/s00334-012-0390-y"
class="uri">https://doi.org/10.1007/s00334-012-0390-y</a></p></li>
<li><p>Reimer, P. J., Brown, T.A., Reimer, R.W. (2004) Discussion:
Reporting and Calibration of Post-Bomb 14C Data Radiocarbon 46,
1299-1304. <a href="https://doi.org/10.1017/S0033822200033154"
class="uri">https://doi.org/10.1017/S0033822200033154</a>.</p></li>
</ul>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
