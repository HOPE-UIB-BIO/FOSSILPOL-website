[
  {
    "objectID": "step_by_step_guide.html",
    "href": "step_by_step_guide.html",
    "title": "A step-by-step guide to data processing with FOSSILPOL",
    "section": "",
    "text": "The FOSSILPOL Workflow is structured in a modular manner, where all steps are organised sequentially and guided by one main configuration file (Config file) where all criteria and setup configurations are pre-defined by the user.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe FOSSILPOL workflow is set up in a way that data from Neotoma Paleoecological Database (“Neotoma” hereafter) are the primary data input. However, other data sources can also be used in parallel by using our predefined format (Fig. 2). The user thus has the flexibility to source data from either Neotoma or from another data source as long as our predefined format file is used (see other data sourcing).\nThree additional data inputs are required for the initial set-up of the Workflow:\n\nConfiguration file (00_Config_file.R) - this contains all the user-selected settings which will be applied throughout the Workflow. These range from technical settings (e.g. location of the data storage) to specific requirements (e.g. filtering criteria) for records to be included. An overview of where the config critera are used through the Workflow is summaried in (Fig. 2).\nGeographical shapefiles - the workflow is internally set-up in a way that data are processed by geographical regions and shapefiles are used to assign relevant geographical information to the records to process. First, the Workflow is conceptualised for a global project, so the general structure of data processing is done per continent (i.e. region = “continent”), but the user can use any other delineation of interest. The Workflow comes with a default shapefile roughly delimiting continents, but it can be adjusted or replaced as per project needs. Second, the taxonomic harmonisation of records is structured by harmonisation regions provided by harmonisation region shapefile. By default, this shapefile is a copy of the continental shapefile, but as harmonisation tables are region-specific (see next data input item) this shapefile needs to be adjusted to represent the geographical delimitation of the harmonisation regions used. Finally, if the user is interested in other biogeographic, climatic, or ecological units of interest to be linked to each record (e.g. ecozones, biome type, climate zones), then additional shapefiles (or TIF files) can be added to the workflow (see details here).\nHarmonisation tables - in each project, one harmonisation table must be provided per harmonisation region (delimited by the corresponding harmonisation region shapefile, see above). A harmonisation table always comes with two columns: i) original taxa(original taxa) with taxonomic names originally present in Neotoma and/or other data source in the project, and ii) level_1 (harmonised taxa) with the standardised taxonomic names. The Workflow will detect if a harmonisation table has been provided by the user, or otherwise create a new table with all detected raw taxa names for each harmonisation region. The latter can consequently serve as a template for harmonisation in the level_1 column (see details here).\n\n\n\n\n\n\n\nThe Workflow will produce several files including temporary output files, stop-check tables, and final outputs (data assembly, figures, etc.):\n\nTemporary output files: the Workflow is set up in a way that temporary (in-progress) data files are saved at various stages of the Workflow. Each file will contain the date of creation for easier organisation. When run multiple times, the Workflow will automatically detect if there are any changes in a selected file and only overwrite it, if an updated file is produced (this is powered by {RUtilpol} package). This also means that the user does not have to re-run the whole Workflow but can re-run only specific parts. As the total size of the files can become substantial, the user can specify if all files should be stored within the project folder (default) or in another directory (specified by using the data_storage_path in the Config file). With such specification, and after running the 00_Config_file.R script, there will be an additional folder structure created (see Code block 3).\nstop-checks CSV tables: while running the Workflow, there will be several times that a user will be asked to check and, where necessary, adjust the produced CSV tables to subsequently continue with the Workflow (i.e. re-run script). This is done to oblige the user to check the produced intermediate results before continuing. For example, at a certain point, the Workflow will produce a list of all ecological groups detected within the dataset compilation obtained from Neotoma. A user then has to edit the mentioned CSV table and specify, which ecological groups should be kept (include = TRUE) and which should be filtered out (include = FALSE). Note that there are several stop-checks throughout the Workflow (see overview in Fig. 2).\nWorkflow output (Outputs/, see Section VII for more information):\n\na ready-to-use, taxonomically harmonised and temporally standardised compilation of fossil pollen data, ready for the analytical stage (rds format)\nplots of modelled age-depth curves for each record (pdf format)\npollen diagram of each record (pdf format)\nmetadata table relaying the main data contributor, contact information, and corresponding publications for citation purposes of the used datasets (PDF).\nreproducibility bundle, a zip file with contains all important sections for a reproducibility of the whole project.\n\noverview figures of the spatial and temporal distribution of the dataset compilation, namely a map and a graph of the record lengths, respectively (PDF).\n\n\n\n\n\n│\n└───Data\n│   │\n│   └───Input\n│   │   │\n│   │   └───Chronology_setting\n│   │   │   │\n│   │   │   └───Bchron_crash\n│   │   │   │\n│   │   │   └───Chron_control_point_types\n│   │   │   │\n│   │   │   └───Percentage_radiocarbon\n│   │   │\n│   │   └───Depositional_environment\n│   │   │   │\n│   │   │   └───Neotoma\n│   │   │   │\n│   │   │   └───Other\n│   │   │\n│   │   └───Eco_group\n│   │   │\n│   │   └───Harmonisation_tables\n│   │   │\n│   │   └───Neotoma_download\n│   │   │\n│   │   └───Potential_duplicates\n│   │   │\n│   │   └───Other\n│   │   │\n│   │   └───Regional_age_limits\n│   │   \n│   └───Personal_database_storage\n│   │\n│   └───Processed\n│       │\n│       └───Chronology\n│       │   │\n│       │   └───Chron_tables_prepared\n│       │   │\n│       │   └───Models_full\n│       │   │\n│       │   └───Predicted_ages\n│       │   │\n│       │   └───Temporary_output\n│       │\n│       └───Data_filtered\n│       │\n│       └───Data_harmonised\n│       │\n│       └───Data_merged\n│       │\n│       └───Data_with_chronologies\n│       │\n│       └───Neotoma_processed\n│       │   │\n│       │   └───Neotoma_chron_control\n│       │   │\n│       │   └───Neotoma_dep_env\n│       │   │\n│       │   └───Neotoma_meta\n│       │\n│       └───Other\n│ \n└───Outputs\n    │\n    └───Data\n    │\n    └───Figures\n    │   │\n    │   └───Chronology\n    │   │\n    │   └───Pollen_diagrams\n    │   \n    └───Tables\n        │\n        └───Meta_and_references\n\n\n\n\n\nHere we focus on the scripts within the R/01_Data_processing folder representing all steps needed for data processing (from obtaining data to the final dataset compilation), organised in the following Sections:\n\nData sourcing: /01_Neotoma_source/ - retrieve and process data from Neotoma\nData sourcing: /02_Other_source/ - process data from other data source (optional)\nInitial data processing: /03_Merging_and_geographic_delineation/ - merge data sources, filter out duplicates, and assign values based on geographical location\nChronologies: /04_Chronologies/ - prepare chronology control tables, calculate age-depth models, and predict ages for levels\nHarmonisation: /05_Harmonisation/ - prepare all harmonisation tables and harmonise pollen taxa (morphotypes)\nData filtering: /06_Main_filtering/ - filter out levels and records based on user-defined criteria\nOutputs: /07_Outputs/ - save the final output including dataset compilation, pollen diagrams, metadata information, graphical summary snd reproducibility bundle\n\n\n\n\n\n\n\nRun_01_01.R- run all scripts within this folder\n01_Download_neotoma.R- download the pollen data from the Neotoma database\n02_Extract_samples.R - create a table from Neotoma’s downloaded lists and download author information\n03_Filter_dep_env.R - get depositional environment data and filter out records based on the user preferences\n04_Extract_chron_control_tables.R - get chronologies, including the preferred table with chronology control points\n05_Extract_raw_pollen_data.R - extract the raw pollen counts from Neotoma and filter by user-selected ecological groups.\n\n\n\nAll pollen records are downloaded from Neotoma based on the geographical criteria (spatial extent, Fig. 2 - config criteria 1) and the selected data type, in this case: \"pollen\". Note that a more complex spatial extent, like a polygon, can be used with the loc argument in RFossilpol::proc_neo_get_all_neotoma_datasets()(see usage of loc in neotoma2 example here).\n\n\n\nEach record is processed using a unique dataset ID (dataset_id) with metadata information extracted. Metadata includes information about the name of the record, geographical information, and the authors and publication DOI connected to the dataset. The authors and their link to the dataset are saved into a separate Author-Dataset database created specifically for each project. It allows easy extraction of authors and DOI for the final dataset compilation produced by the Workflow.\n\n\n\nDepositional information from each record gives information about the environments where a record was extracted. Based on the research question, there could be a preference for certain environments (e.g. terrestrial vs. marine). Currently in Neotoma, the data about depositional environments are organised in a hierarchical structure (e.g. “Pond” is nested in “Natural Lake”, which is nested in “Lacustrine”), in which the maximum number of nested layers is five. At the lowest hierarchical level, there are currently over 50 different categories of depositional environments (for fossil pollen records). Based on the selected records, the Workflow will produce a list of all depositional environments (and their hierarchical position) present in the user´s data selection. The user is then requested to define the environments of choice (this is a stop-check point, Fig. 2). Note that excluding depositional environments with the higher hierarchical position will not automatically exclude all depositional environments nested in it.\n\n\n\nChronology data for each record is available in a table that contains information about chronology control points used to construct an age-depth model. Some records can have multiple chronology tables as some records have been used for several projects or recalibrated (updated) by data stewards. These tables are numbered according to the order in which they were created and uploaded. Each chronology comes with the age-unit of the age-depth model output (e.g. “Radiocarbon years BP”, “Calibrated radiocarbon years BP”) and the temporal range of the record (youngest and oldest age). The chronologies in “Radiocarbon years BP” are often older chronologies as it is now common practice to recalibrate radiocarbon-dated material and produce chronologies expressed in “Calibrated radiocarbon years BP”. Note: The chronologies in “Calibrated radiocarbon years BP” still come with chronology table(s) containing the uncalibrated radiocarbon ages and need to be calibrated by the user if a new age-depth model is desired. The Workflow automatically selects one table per record based on the order defined by chron_order in the Config file (Fig. 2 - config criteria 2). Note: if more tables have the same age-unit type (e.g. Calibrated radiocarbon years BP), the Workflow will opt for the more recent table. The user can specify their preference for certain age-unit types in the Config file. In addition, only records which have at least a certain number of control points (defined by min_n_of_control_points in the Config file, Fig. 2 - config criteria 3) will be subsequently used.\n\n\n\nEach level of each record comes with additional information: a) unique sample ID (sample_id), b) information about depth (and estimated age later), and c) pollen counts for each taxon present in that level. The information about levels is split into two different tables (first with depth and ages, and second with pollen counts) linked together by sample ID (sample_id).\nThe Workflow will only keep records with a minimal number of levels as defined in the Config file (min_n_levels, Fig. 2 - config criteria 4). The minimum number of levels is by default selected as three but the user can change this setting.\nIn the case of data sourced from Neotoma, each pollen taxon has information about the ecological group (e.g. palms, mangroves, etc). Based on the selected records, the Workflow will produce a full list of all ecological groups after which the user is requested to define which ecological groups to include (a stop-check point, Fig. 2, see explanation of abbreviation in Table 1)\n\n\n\n\n\n\n\n\n\nABBREVIATION\nECOLOGICAL GROUP\n\n\n\n\nACRI\nAcritarchs\n\n\nANAC\nAnachronic\n\n\nALGA\nAlgae (e.g. Botryococcus)\n\n\nAQB\nAquatics (e.g. Sphagnum)\n\n\nAQVP\nAquatic vascular plants (e.g. Isoetes)\n\n\nBIOM\nBiometric measurements\n\n\nEMBR\nEmbryophyta\n\n\nFUNG\nFungi\n\n\nLABO\nLab analyses\n\n\nMAN\nMangroves\n\n\nPALM\nPalms\n\n\nPLNT\nPlant\n\n\nSEED\nUnidentified, but definitely pollen - Spermatophyte rank or clade\n\n\nSUCC\nSucculents\n\n\nTRSH\nTrees and shrubs\n\n\nUNID\nUnknown and Indeterminable\n\n\nUPBR\nUpland bryophytes\n\n\nUPHE\nUpland herbs\n\n\nVACR\nTerrestrial vascular cryptogams\n\n\nVASC\nVascular plants\n\n\n\n\n\n\n\n\n\n\n\n\nRun_01_02.R - run all scripts within this folder\n01_Import_other_data.R - source other data sources and filter the records in a similar way as Neotoma.\n\n\n\nOur FOSSILPOL Workflow allows the use of other data source in combination with the Neotoma data. Including other data sources is fully optional and can be skipped as indicated by use_other_datasource = TRUE/FALSE in the Config file.\nAny data can be used as long as it contains the following required information: a) metadata, b) depositional environment, c) chronology, d) level (age-depth), and e) pollen counts. In order to prepare data for usage, the user needs to download file template specially prepared for this. Each pollen record needs to be stored as a separate file with unique name. We recommend e.g., private_data_(insert_site_name).xlsx. The site name in the filename is crucial as it will be compared against all other pollen records in Neotoma to test for potential duplicates in a later stage of the Workflow. All files must be stored in /Data/Input/Other/ (or specified by dir_files argument, see below).\n\n\n\nThe sourcing of other data sources follows a simple order of actions:\n\nData files need to be prepared by the user following the template one record per file.\nData are extracted and formatted to be compatible with Neotoma data using the RFossilpol::import_datasets_from_folder() function, with the following arguments:\n\ndir_files - user can specify which folder contains the prepared data (default = Data/Input/Other/\nsuffix - argument to keep track of the source of the data. Default is set to \"other\", which means that datasets can be easily identified as their name will be (dataset id)_other\nsource_of_data - will flag the source of each dataset in the compilation in meta-data overview (see section VII). Default is set to \"personal_data\"\ndata_publicity - will flag the data publicity of each dataset in the compilation in meta-data overview (see section VII - Outputs). Default is set to \"restricted\"\npollen_percentage - pollen counts measured as proportions (e.g., from scanning of pollen diagrams) can be flagged here. Default set to FALSE\n\nNames of data contributors are extracted and added to the Author-Dataset database used in Author-dataset attribution (see section VII - Outputs).\nData are treated in a similar way as data from Neotoma, in terms of filtering by geographical location, number of levels (Fig. 2 - config criteria 5,6), and depositional environments (stop-check point, Fig. 2).\n\n\n\n\n\n\n\n\n\nRun_01_03.R - run all scripts within this folder\n01_Merge_datasets.R - merge data from all sources, filter out duplicates, and assign values based on geographical location\n\n\n\nAfter initial data processing, records from Neotoma and Other sources are merged together.\n\n\nThere is a possibility that some datasets from the other data sources are already in Neotoma. To avoid duplication within the final dataset compilation, the Workflow will compare datasets from both sources and identify potential duplicates. This step is optional but recommended to follow. To do so, the user needs to specify that detect_duplicates == TRUE in the Config file (this is set as default, Fig. 2 - config criteria 7). The Workflow will start a simple subroutine using the function RFossilpol::proc_filter_out_duplicates(). Because comparing all records within each data source to each other is relatively computationally demanding, the function will split the data into several groups using their geographical location (ca. 100 records per group). The user can define the number of groups using the n_subgroups argument. Next, each record from one source is compared to all records from the other source as long as they are within 1 degree radius (the assumption here is that duplicated records will be in a similar location). The user can define the maximum distance using the max_degree_distance argument. Finally, the Workflow will output a list of potential duplicated records (a stop-check point, Fig. 2). For each record-pair, the user must specify, which records should be deleted by writing 1 (deleting the Neotoma) or 2 (deleting the other data source) in the delete column of the created list (leaving 0 will leave both records in).\n\n\n\nSeveral more steps take place to create the fully merged dataset compilation before proceeding to the chronology step (does not require any action by user):\n\nAll taxon names are transformed into a more computer-friendly format for easier manipulation. The RFossilpol::proc_clean_count_names() function will first transform special characters to text (e.g. + to _plus_) and then use the {janitor} to transform into “snake_case” style. In addition, the user can specify additional specific changes in names (e.g. based on presence of special characters) by adjusting the user_name_patterns argument (see example in the script). During this cleaning of taxa names, the Workflow will save taxa_refrence_table for back traceability to Neotoma taxonomy. The taxa_reference_table is a CSV file saved in the same folder as the harmonisation tables (Data/Input/Harmonisation_tables/). More about harmonisation process.\nIndividual levels (sample depths) are sorted by their depth for each record by RFossilpol::proc_prepare_raw_count_levels(). This includes subroutines, for example, only keeping levels present in all data tables, filtering out levels without pollen data, and taxa which are not present in any level.\nSpatial information for each record is assigned based on the provided geographical shapefiles. Specifically:\n\nRegion information - the shapefile in Data/Input/Spatial/Regions_shapefile will assign the regional names for each record (see Data input Section). The user can (and is recommended to) change the spatial delineation of the data by altering or replacing the shapefile.\nPolitical delineation (countries) - obtained from GADM database, version 2.8, November 2015.\nHarmonisation region - the shapefile in Data/Input/Spatial/Harmonisation_regions_shapefile will assign the harmonisation region (to be able to link the corresponding harmonisation table to use; See see Data input Section). The default shapefile in the Workflow is a copy of the Region information shapefile but should be adjusted by the user to correspond to the area covered by the different harmonisation tables.\nCalibration curves (normal and post-bomb) - depending on the geographical position of the record, a different calibration curve needs to be assigned, as different curves are used for the northern and southern hemispheres, and for terrestrial and marine environments. See more details about calibration curves.\nAdditional - The user can add any additional spatial delimitation (e.g. ecozones). This will require adding the specific shapefile (or TIF file) in /Data/Input/Spatial/NAME_OF_FOLDER and adjusting the R code manually (optional_info_to_assign) so that the shapefile is sourced, and its information assigned to each record (see the example in the script).\n\nThe Workflow will create a new table with age limitations for each region presented in the data, which needs to be edited by the user (a stop-check point, Fig. 2). For example, Regional_age_limits table will have the following values:\n\nyoung_age = youngest age the record must have\nold_age = oldest age the record must have\nend_of_interest_period = levels beyond this age will be omitted\n\n\n\n\n\n\n\n\n\n\n\nRun_01_04.R - run all scripts within this folder\n01_Prepare_chron_control_tables.R - prepare the chronology tables for age-depth modelling\n02_Run_age_depth_models.R- create age-depth models with BChron\n03_Predict_ages.R - estimate the ages of individual levels\n04_Save_AD_figures.R - save visual output of the age-depth models in pdf format\n05_Merge_chron_output.R - link age-depth models to corresponding datasets\n\n\n\nTo estimate the age of individual levels based on their depth, an age-depth model needs to be constructed based on the chronology data of the record. An age-depth model will provide age estimates of each individual level and the full age range of the record.\nAge-depth modelling can be very computationally heavy and can take a substantial amount of time. Therefore, the Workflow automatically processes several files (rds format):\n\nChronology-control-tables:\n\n/Data/Processed/Chronology/Chron_tables_prepared/chron_tables_prepared*.rds contains all the chronology control tables prepared to be re-calibrated\n\nAge-depth-models:\n\n/Data/Processed/Chronology/Models_full/* contains one file for each age-depth model\n\nPredicted-ages:\n\n/Data/Processed/Chronology/Predicted_ages/predicted_ages*.rds contains the predicted ages using the age-depth models\n\n\nIt may be not preferential to re-run all age-depth models every time the user wants to re-run the Workflow. Therefore, the Workflow offers the option to save the successful age-depth models (Age-depth-models & Predicted-ages) and keep them between individual runs. This can be done in several ways:\n\nrecalib_AD_models in the Config file can be set to FALSE. This will result in completely omitting the scripts aimed at calculating age-depth models, and therefore this step will be skipped. Note that this will only work if the age-depth models have already been successfully created at least once (Predicted-ages exists).\ncalc_AD_models_denovo in the Config file can be set to FALSE (this is the default). When set to TRUE, the Workflow will not use the Age-depth-models files (successful age-depth models) and will re-calibrate everything “de novo”.\npredict_ages_denovo in the Config file is set to FALSE as the default. When set to TRUE, the Workflow will use all the age-depth model files but re-assign the ages to each level of all related records (even for records where ages were successfully predicted). This is done, for instance, when the number of levels in a record increased since the last Workflow run and these levels still need to have ages assigned.\n\nIMPORTANT: If you select both calc_AD_models_denovo and predict_ages_denovo as FALSE for your first run of age-depth modelling, you might be asked by the Workflow to temporarily switch both to TRUE in the console (you do not have to change anything in the Config file).\n\n\n\nAge-depth models are constructed using chronology control points (usually radiocarbon dates) with known depth, estimated age, and associated age uncertainties. Each record can and should ideally have several such points saved in the chronology control table. Each chronology control point has the following properties:\n\nDepth\nEstimated age\nError of the estimated age\nType of the chronology control point\nThe calibration curve used (which is needed to convert the raw radiocarbon ages to calendar ages (Note: radiocarbon ages are not true calendar ages!)).\n\nThis script will take several steps to prepare all records for age-depth modelling, specifically:\n\nCreate and attach the necessary calibration curves\nSelect the preferred chronology control point types\nPrepare chronology tables (include fixing issues with percentage carbon (if necessary))\n\n\n\nCalibration curves are assigned to each control point based on several criteria. If a control point has a type flagged to be calibrated (see next section), a calibration curve is assigned based on the geographical position of the record. Note: only chronology control points of uncalibrated radiocarbon ages need recalibration. FOSSILPOL has a shapefile based on figure 7 in Hogg et al (2020) to correctly assigned IntCal20, SHCal20 and a mixed calibration curve to each record. We follow this recommendation “…the use of (i) IntCal20 for areas north of the ITCZ in June-August (dashed lines in Figure 7) which receive NH air masses all year round, (ii) SHCal20 for areas south of ITCZ in December-February (dotted lines in Figure 7) which see SH air masses all year round, and (iii) a mixed curve for areas between the two seasonal ITCZ positions shown in Figure 7, which receive northern air masses in December-February and southern air masses in June-August.” FOSSILPOL also comes with a mixed curve that was constructed using the {rcarbon} package with a proportion of curve contribution 1:1. See more info on mixed calibration curve use.\nAll calibration curves have an age limitation, i.e. each curve can be used only for certain ages. Radiocarbon calibration curves currently do not cover ages older than 55 thousand years. For the younger ages, during the last century, there are issues with the Earth’s atmospheric radiocarbon and a different set of calibration curves needs to be used. This is caused by the deployment of nuclear weapons in the ’50s and ’60s, which caused a spike in atmospheric radiocarbon and resulted in radiocarbon measurements from material in the following decades to return often highly negative ages. Therefore, if the control point has a radiocarbon age younger than 200 yr BP, a post-bomb calibration curve is used instead (see further below). As with the normal calibration curves, the geographical location of the record is taken into account to assign the corresponding post-bomb calibration curve.\nModern radiocarbon dates are calibrated by using one of the post-bomb calibration curves (nh_zone_1, nh_zone_2, nh_zone_3, sh_zone_1_2, sh_zone_3), following Hua et al., 2013 and link. The Workflow will automatically assign the corresponding curve based on the geographical location of the record (see function IntCal::copyCalibrationCurve()). If modern radiocarbon dates are detected, the Workflow will then display the detected records to the user and apply the conversion according to the post-bomb curves from the {IntCal} package.\n\n\n\nEach control point in the control table has several properties: unique ID, depth, age, error, thickness, and chronology control point type (e.g. radiocarbon, biostratigraphic, annual laminations, tephra). Each type of chronology control point has different age uncertainties. For instance, many older records relied on indirect dating techniques based on biostratigraphic layers, similar geological levels from other records (e.g. a volcanic event), and pollen-based levels (e.g. the appearance of a key taxon), among others and can have large age uncertainties into thousands of years. Neotoma has over 50 different chronology controls points that fall within the categories of geochronological (e.g. lead-210, radiocarbon, uranium-series), relative time scale (e.g. MIS5e, Heinrich Stadial 1, Late Wisconsin event), stratigraphic (e.g. biostratigraphic events such as the introduction of anthropogenic taxa), cultural (e.g. European Settlement Horizon), other absolute dating methods (e.g. annual laminations, collection date), and other dating methods (e.g. extrapolated or guesses). Only the chronology control points in uncalibrated radiocarbon ages require recalibration with the calibration curves as most, if not all, other control points will be in calendar ages and no recalibration should be implemented.\nA user has the option to select which control point types should be accepted “as-is” and which should be calibrated (a stop-check point, Fig. 2). The Workflow will automatically produce a list of all detected control points from all selected records, which includes columns called include and calibrate. In the first, the user should indicate if the chronology control point should be included. In latter, the user should indicate if the point should be recalibrated using the calibration curves, here uncalibrated radiocarbon ages.\n\n\n\nThe chronology control tables will need to undergo a number of user-defined adjustments:\n\nfiltering out unwanted control point types selected by user (defined by stop-check point, see above)\nfiltering out records that do not fulfil the minimal number of control points (defined by min_n_of_control_points in the Config file, default = 2, Fig. 2 - config criteria 8). The value min_n_of_control_points will serve as a criterion for the prepared chronology control tables, where records that do not fulfil such requirements will be filtered out. Note that there is a trade-off between accepting only the tables with a high number of control points per time interval (more robust age-depth model) and the number of records that will be able to fulfil strict criteria.\nfixing instances of missing values. Defined in the Config file, the values default_thickness (defined in the Config file, default = 1, Fig. 2 - config criteria 9) and default_error (default = 100, Fig. 2 - config criteria 10) will replace missing values (NA) for thickness and error, specifically.\nfiltering out control points with an error that is considered too big. Any control point with an error bigger than max_age_error (defined in the Config file; default = 3000 yr, Fig. 2 - config criteria 11) will be filtered out.\nremoving control points that are duplicated in terms of age and/or depth.\nin several cases, the chronology control point from the core-top has a type specified as guess. A user can specify that the type guess is only acceptable to a certain depth using the guess_depth variable in the Config file (default is 10 cm, Fig. 2 - config criteria 12)\n\nIn addition, the number and distribution of such control points can give a good indicator of the temporal uncertainty around levels’ ages (Giesecke et al. 2014, Flantua et al. 2016). For example, a record with few chronology control points within the focus time period will have large uncertainties of predicted ages. Hence, the information of the quality of chronologies, i.e. taking into account the types and levels of chronology control points, can be a criterion used in the selection of records.\n\n\n\nThere are three ways by which post-bomb radiocarbon dates are reported, namely by 1) modern radiocarbon dates (&lt;0 F14C yr); 2) percent modern carbon (pMC, normalised to 100%); and 3) fraction radiocarbon (F14C, normalised to 1; Reimer et al. 2004). Currently, there is no direct way to know from Neotoma whether the dates are in pMC or F14C. Even if the cases are likely to be few, such dates need to be checked to avoid incorrect calculations.\nThe strongest rule of thumb is that normal radiocarbon dates and errors should always be integer (no decimals) and are uploaded so by the data stewards to Neotoma. The second rule of thumb is that pMC control points are characterised by an age value from around 100 with a decimal place. Thus, the Workflow will automatically export suspicious records and the user must specify which control points should be back-transformed (a stop-check point, (Fig. 2).). For those selected by the user to be back-transformed (include == TRUE), the Workflow will first convert the pMC values to normal post-bomb radiocarbon ages (negative 14C ages) by using the IntCal::pMC.age() function, after which normal post-bomb calibration curves are used to calibrate the values to final calendar ages (see above).\nAlthough F14C has been recommended by experts for the reporting of post-bomb samples (Reimer et al. 2004, in practice, the bulk of the reporting is done as modern radiocarbon dates followed by pMC to a much lesser extent. The Workflow currently does not deal with F14C as no such cases have been detected to date. As this is not consistent with the rest of the data, back-transformation could be done when working with other data sources, and the {IntCal} package can be used to do so.\n\n\n\n\nIndividual age-depth models are estimated using the {Bchron} package, which estimates the Bayesian probabilistic trajectory of the age-depth model curve (non-parametric chronology model to age-depth data according to the Compound Poisson-Gamma model). Therefore, it is suitable for various combinations of control point types, outliers, and age reversals.\nIf there are many ages at close or similar depths (e.g. annual laminations), initialisation problems may occur and the Bchron could fail to converge the age-depth model. In such a scenario, the thickness of duplicated chronology control points is automatically increased by 0.01 (see artificialThickness argument in Bchron::Bchronology() function).\n\n\nBecause creating age-depth models for multiple records can be computationally demanding, the Workflow uses multi-core (parallel) computation. The Workflow automatically detects the number of cores for the machine on which the code is running (this can be adjusted by specifying the number in number_of_cores in the Config file, Fig. 2 - config criteria 14). Several age-depth models are then created at the same time. This is done by splitting the records into batches, with each batch containing a certain number of records (see batch size in Config file, Fig. 2 - config criteria 13; by default it is based on the number_of_cores). If number_of_cores is selected (or detected) as 1, the Workflow will not use the batch approach (as obsolete) and estimate the age-depth models one-by-one (see below).\nNote that there is a possibility that the age-depth model estimation will crash (freeze) for unforeseen reasons. Therefore, the Workflow is structured in a way that if an estimation of the whole batch crashes, the Workflow will skip that batch and continue with other batches. The user can specify how long the machine should wait before skipping the batch with the time_per_record argument in the RFossilpol::chron_recalibrate_ad_models() function.\nFor each batch, the Workflow will try to estimate age-depth models three times. The user can specify the number of attempts by changing the batch_attempts in the RFossilpol::chron_recalibrate_ad_models() function. If the age-depth modelling is stopped in the process, the Workflow will automatically use the previously successful batches, which are saved automatically. This should help when the user needs to stop the age-depth modelling and resume it at a later time.\nNext, the Workflow continues to another subroutine where age-depth models for records from failed batches are estimated one by one. Similar to batch estimation, the age-depth model estimation is tried three times for each crashed record until successful at least once. On some rare occasions, a record could cause R to freeze completely and prevents it from skipping to another record. In that case, the dataset ID of the dataset that caused the crash will be automatically written in the Crash file (found in /Data/Input/Chronology_setting/Bchron_crash/) and omitted from the future run of age-depth models. The user is recommended to do a detailed check of the specific dataset, e.g. the chronology control table for possible inconsistencies or other flaws that could be causing {Bchron} to fail to produce an age-depth model.\n\n\n\nIn the Config file, the number of iterations is set to 50,000 by default, discarding the first 10,000 iterations (burn-in) and keeping every iteration beyond the burn-in with a step size of 40 (thinning) (Fig. 2 - config criteria 15,16,17). The user can change the number of iterations by altering the iteration_multiplier in the Config file (Fig. 2 - config criteria 18). This will result in changing the total interactions but keeping the ratios of burn-ins and thinning. Thus 1000 ((50k - 10k) / 40) posterior values are drawn. The default number of iterations should produce a robust estimation but they can be increased by the user if preferred (by increasing the iteration_multiplier). Note that increasing the iteration_multiplier will automatically increase the time that the program will wait for estimation of an age-depth model before skipping it (see time_per_record above).\nThe user should keep in mind that creating age-depth models for hundreds of records using a high number of iterations is computationally demanding and can take a significant amount of time in the order of tens of hours or several days.\n\n\n\n\nWith a successful age-depth model, the ages of individual levels are estimated. The Workflow will estimate age and the error estimate for each level, which will encapsulate 95% of all age posterior values (upper and lower boundary). As {Bchron} uses a probabilistic model it is possible to obtain possible ages of each level. To do so, a number of ages (default = 1000, see above) are drawn from the model posterior representing all the possible ages, and a series of quantiles of various values (25, 50, 75, etc.) are then calculated. The 50th quantile (median) is used as the final age of each level, and the 2.5th and 97.5th quantiles as upper and lower boundaries respectively. This results in the age estimate of each level including its error estimates. The whole matrix levels by posterior drawn (possible age) is saved as an age uncertainty matrix (age_uncertainty column). This whole process is completely automated and does not require any input from the user.\n\n\n\nThe visual representation of all age-depth models is saved as output for visual confirmation of successful model estimation. The files (as PDFs) are stored in the /Outputs/Figures/Chronologies/ folder and split into subfolders defined by region. The properties of the figures (size of figures, font size, etc) can be altered in the Config file (image_width, image_height, image_units, image_dpi, text_size, line_size). They can be used for inspection of the age-depth curves to look for unrealistically large age estimates or error bars, hiatuses, or extreme extrapolations toward the present or the past.\n\n\n\nThe successfully predicted ages are linked with all the records from the various sources (Sections I-III). The individual levels of each record are then ordered by the predicted ages and the same order of levels is then applied to the tables with the pollen counts.\n\n\n\n\n\n\n\n\nRun_01_05.R - run all scripts within this folder\n01_Harmonisation.R - prepare all harmonisation tables and harmonise the raw counts.\n\n\n\nThe goal of taxonomic harmonisation is to standardise all site-level names to the same pollen morphotypes (set of pollen and spore morphotypes used for all pollen records) and thus reduce the effect of taxonomic uncertainty and nomenclatural complexity (See relevant literature in Appendix 1 of Flantua et al. 2023). For this purpose, a harmonisation table can be created that groups the morphotypes into the highest taxonomic level that is most likely to be identified by most of the pollen analysts.\n\n\n\nFirst, the Workflow will check the harmonisation regions present in the data, defined by the shapefile (see Data input and Section III), and confirm that there is one harmonisation table per region (a stop-check point, Fig. 2). If any table is missing (or the Workflow is run for the first time), the Workflow will automatically create a harmonisation table per harmonisation region, with all the raw taxa names from all the records from within that region. Note that we refer here to “taxon names” for simplicity but the identification is often done at the level of family or genus, and are best referred to as morphotypes.\nEach harmonisation table is created so that each taxon can have two columns:\n\ntaxon_name which is the original name of the taxon (morphotype) formatted into a more computer-friendly format (snake_case)\nlevel 1, which should be used to merge various taxa (morphotypes) into harmonised taxonomic units, specific for the project.\n\nNote that in order to link the names to the original display in Neotoma, user can use the taxa_reference_table (see name cleaning process).\nThe user can also define which taxa should be completely removed during the harmonisation process (marked as delete), in case of a taxonomic mistake or a palaeoecological proxy not of interest, e.g. spores). The user can add additional columns (e.g. level_2) and then specify which levels should be included by altering the argument harm_name when using the RFossilpol::harmonise_all_regions() function.\nWith these tables, each dataset is harmonised so that all taxa that belong to the same harmonised taxa (morphotypes) are summed together in each level - this process is applicable for both count and percentage data. This process includes automatic detection of successful pollen harmonisation by checking the total number of pollen grains before and after harmonisation (it can be turned off by changing the argument pollen_grain_test to FALSE when using the RFossilpol::harmonise_all_regions() function).\n\n\n\n\n\n\n\n\nRun_01_06.R - run all scripts within this folder\n01_Level_filtering.R - filter out levels and records based on the user-defined criteria predefined in the Config file\n\n\n\nTo obtain a comprehensive dataset compilation of multiple fossil pollen records, to increase its overall quality, and to answer research questions reliably, we recommend the user to further trim down the data selection by filtering out individual levels and/or whole records. All of these filtering criteria can be adjusted by the user in the Config file:\n\nfilter_by_pollen_sum (Fig. 2 - config criteria 20) - if TRUE, the Workflow will use the quantity of counted pollen grains at each level as a factor in determining the quality of the level.\nfilter_by_age_limit (Fig. 2 - config criteria 24) - if TRUE, the Workflow will filter out records that do not span the user-defined time period (defined by young_age and old_age in Regional_age_limits table; see Section III).\nfilter_by_extrapolation (Fig. 2 - config criteria 25) - if TRUE, the Workflow will filter out levels based on the number of years between their age and the last chronology control point used for age-depth modelling (chron_control_limits).\nfilter_by_interest_region (Fig. 2 - config criteria 27) - if TRUE, the Workflow will filter out levels that are older than end_of_interest_period (defined in Regional_age_limits table; see Section III) to subsequently reduce processing time during follow-up analyses.\nfilter_by_number_of_levels (Fig. 2 - config criteria 28) - if TRUE, the Workflow will filter out records based on the number of levels.\n\nIn addition, two more options can be activated by the user:\n\nuse_age_quantiles (Fig. 2 - config criteria 30) - if TRUE, the Workflow will use the 95th age quantile (uncertainty of the level age) throughout the data filtration process, i.e. the age of the level will be assumed to be anywhere between the upper and lower boundary (see Section IV). This will result in a more stable dataset compilation between the different results of age-depth modelling (as a probabilistic result can output slightly different results each time they are run). However, the final dataset compilation may require some additional filtering before any analyses, as the 95th age quantile can span very long time periods.\nuse_bookend_level (Fig. 2 - config criteria 31) - if TRUE, the Workflow will leave one additional level beyond the oldest age of the user-defined time period. This will result in a bookend level, which can help to anchor information beyond the period of interest.\n\n\n\n\n\n\nThe number of counted pollen grains at each level is an index of data quality. To obtain a reliable representation of the vegetation, researchers often aim to count more than 300 pollen grains (following Moore et al., 1991), but other recommendations may also have been followed (&gt;150; e.g. Djamali & Cilleros, 2020) and will vary with region and by scientific question (Birks & Birks, 1980). For example, to achieve a representative sample of the regional pollen pool, counts in Arctic records may only reach c. 100 grains per level, whereas counts in Mediterranean sites can be as high as 1000 (Birks & Birks, 1980, p. 165), but the main determinant can also be the preference of the pollen analyst. Reasons for low numbers (&lt;100) are often mainly due to time constraints of the data contributor, but can also be natural depositional phenomena causing poor pollen preservation, such as low local pollen production in arctic or alpine environments. Given that statistical inferential power is proportional to sample size, we recommend defining a minimum number of total pollen grains in each level. Subsequently, whole records can be selected on the proportion of levels with a selected minimum number of pollen grains counted per level.\nThe user can select two different quantities of total pollen grains per level: a) minimum number (min_n_grains, Fig. 2 - config criteria 21) and b) acceptable number (target_n_grains, Fig. 2 - config criteria 22). All levels with total pollen grains below the minimum number will be filtered out. In addition, the whole record will only be accepted if X% (set by percentage_samples; default = 50, Fig. 2 - config criteria 23) of all levels fulfils at least the acceptable number of pollen grains. This filtration criterion will only be used if filter_by_pollen_sum == TRUE.\n\n\n\nAs projects differ in their temporal focus, only a subset of records will be of interest to the particular project. Therefore, records that do not span a certain age period (from young_age to old_age; specified by the Regional_age_limits table, see Section III) will be filtered out. This filtration criterion will only be used if filter_by_age_limit == TRUE in the Config file.\n\n\n\nExtrapolation of age inferences to samples beyond the set of chronology control points is another factor in quality control. Levels older than the oldest chronology control point have no other chronology control point to constrain the age inference, and therefore, have increasingly large uncertainty as the amount of extrapolation increases. To limit the use of levels based on large extrapolations, we recommend selecting a maximum extrapolation age, i.e. levels older than the selected age criterion (e.g. 5000 yr) from the last chronology control point are filtered out.\nIn order to limit the use of levels based on large extrapolations, the user can select the maximum extrapolation age (maximum_age_extrapolation, Fig. 2 - config criteria 26), i.e. levels older than the selected criterion from the last chronology control point will be filtered out. This filtration criterion will only be used if filter_by_extrapolation == TRUE in the Config file.\n\n\n\nThe individual levels of a record outside of the interest period (end_of_interest_period specified by the Regional_age_limits table) will be filtered out as they do not provide additional information. This filtration criterion will only be used if filter_by_interest_region == TRUE in the Config file.\n\n\n\nThe total number of levels in a record is an important quality criterion for further use of such a record in a specific analysis. Records might have been sampled at low resolution (e.g. depth intervals &gt; 30 cm) leaving substantial unassessed gaps - and thus time periods - between levels. In addition, records with few levels will likely contribute poorly to studies focused on specific time periods (many non-value levels) and can cause unnecessary outlier values. Therefore, we recommend selecting a minimum number of levels within the time period of interest and use this as an additional criterion to filter out unwanted records. The user can select the minimum number of levels (min_n_levels, Fig. 2 - config criteria 29) that records must have at the end of the filtration subroutine. This filtration criterion will only be used if filter_by_number_of_levels == TRUE in the Config file.\n\n\n\n\n\n\n\n\n\nRun_01_07.R - run all scripts within this folder\n01_Pollen_diagrams.R - save pollen diagrams for all records\n02_Save_assembly.R - save data assembly with selected variables (columns)\n03_Save_references.R - save reference and metatable\n\n\n\nPollen diagrams for all records will be created using the {rioja} package. The harmonised data will be automatically transformed into relative proportion for plotting purposes. The pollen diagrams will be saved in the /Outputs/Figures/Pollen_diagrams/ folder and split into sub-folders defined by Region (e.g., continent).\nThe RFossilpol::plot_all_pollen_diagrams() function will automatically produce a PDF of the pollen diagram split into several A4 pages and ready to be printed out. The y-axis is, by default, the age of the levels, but it can be altered to depth (by the y_var argument set to \"age\" or \"depth\"). The maximum number of taxa per page can be altered by the max_taxa argument (default = 20). In addition, the function will automatically omit very rare taxa. Specifically, the min_n_occur argument will determine the minimum number of occurrences per record each taxon has to have to be plotted.\n\n\n\nA ready-to-use, taxonomically harmonised and standardised compilation of fossil pollen data (ready for the analytical stage) is produced and saved in the /Outputs/Data/ folder. The file is saved as a tibble (by {tibble} package from tidyverse) in rds format.\nThe user can select which columns (variables) should be present in the final data compilation by selecting select_final_variables == TRUE in the Config file. Here, the Workflow will interactively (in the R console) ask the user to specify if each variable should be included (Yes/No).\n\n\n\nThe workflow will save all metadata and citation information needed to create the dataset compilation. All outputs can be found in the /Outputs/Meta_and_references/ folder. The functionRFossilpol::proc_save_references() will do this automatically but it can be specified by the user, i.e. the user can specify what information should be saved using the user_sel_variables argument. By default this information contains:\n\n\"meta_table\" - a metadata table (data_assembly_meta.csv) contains the following variables by default (note that this list may be altered by changing the final variables by select_final_variables):\n\nthe list of all records in the final dataset compilation\ngeographical location\ndepositional environment\nassigned continental region\nassigned harmonisation region\nfinal number of levels\nnumber of chronology control points used for age-depth modelling\nassigned calibration curve(s)\nage limits of each record of each data source (Neotoma or other data source)\nDOI (from Neotoma).\n\n\"author_table\" - a reference table (authors_meta.csv) containing information about the datasets used, the main data contributor, and their contact information.\n\"affiliation_table\" - if an affiliation is provided with the data from sources other than Neotoma, the affiliation table (affiliation_table) is also exported linking affiliations and their authors.\n\"graphical_summary\" - a PDF (graphical_summary*.pdf) with three panel figures using RFossilpol::plot_graphical_summary() function (note that additional arguments can be passed to this function, see ?RFossilpol::plot_graphical_summary for more information):\n\nmap - map of geographical location with each record represented as a point\ncount of data records- a lollipop plot with the number of records in each geographical group when assigned\nage length - age limits of records, with each record represented by a single line\n\n\"reproducibility_bundle\" - a zip file (reproducibility_bundle.zip) of the Config file, all stop-check CSV tables, and all shapefiles. The idea is to increase the reproducibility of the user´s workflow where ideally this zip file can be shared when publishing a publication with a project created using the FOSSILPOL Workflow. For reviewing purposes, the zip file can be shared and detailed feedback can be provided before publication."
  },
  {
    "objectID": "step_by_step_guide.html#summary",
    "href": "step_by_step_guide.html#summary",
    "title": "A step-by-step guide to data processing with FOSSILPOL",
    "section": "",
    "text": "The FOSSILPOL Workflow is structured in a modular manner, where all steps are organised sequentially and guided by one main configuration file (Config file) where all criteria and setup configurations are pre-defined by the user."
  },
  {
    "objectID": "step_by_step_guide.html#data-input",
    "href": "step_by_step_guide.html#data-input",
    "title": "A step-by-step guide to data processing with FOSSILPOL",
    "section": "",
    "text": "The FOSSILPOL workflow is set up in a way that data from Neotoma Paleoecological Database (“Neotoma” hereafter) are the primary data input. However, other data sources can also be used in parallel by using our predefined format (Fig. 2). The user thus has the flexibility to source data from either Neotoma or from another data source as long as our predefined format file is used (see other data sourcing).\nThree additional data inputs are required for the initial set-up of the Workflow:\n\nConfiguration file (00_Config_file.R) - this contains all the user-selected settings which will be applied throughout the Workflow. These range from technical settings (e.g. location of the data storage) to specific requirements (e.g. filtering criteria) for records to be included. An overview of where the config critera are used through the Workflow is summaried in (Fig. 2).\nGeographical shapefiles - the workflow is internally set-up in a way that data are processed by geographical regions and shapefiles are used to assign relevant geographical information to the records to process. First, the Workflow is conceptualised for a global project, so the general structure of data processing is done per continent (i.e. region = “continent”), but the user can use any other delineation of interest. The Workflow comes with a default shapefile roughly delimiting continents, but it can be adjusted or replaced as per project needs. Second, the taxonomic harmonisation of records is structured by harmonisation regions provided by harmonisation region shapefile. By default, this shapefile is a copy of the continental shapefile, but as harmonisation tables are region-specific (see next data input item) this shapefile needs to be adjusted to represent the geographical delimitation of the harmonisation regions used. Finally, if the user is interested in other biogeographic, climatic, or ecological units of interest to be linked to each record (e.g. ecozones, biome type, climate zones), then additional shapefiles (or TIF files) can be added to the workflow (see details here).\nHarmonisation tables - in each project, one harmonisation table must be provided per harmonisation region (delimited by the corresponding harmonisation region shapefile, see above). A harmonisation table always comes with two columns: i) original taxa(original taxa) with taxonomic names originally present in Neotoma and/or other data source in the project, and ii) level_1 (harmonised taxa) with the standardised taxonomic names. The Workflow will detect if a harmonisation table has been provided by the user, or otherwise create a new table with all detected raw taxa names for each harmonisation region. The latter can consequently serve as a template for harmonisation in the level_1 column (see details here)."
  },
  {
    "objectID": "step_by_step_guide.html#data-storage",
    "href": "step_by_step_guide.html#data-storage",
    "title": "A step-by-step guide to data processing with FOSSILPOL",
    "section": "",
    "text": "The Workflow will produce several files including temporary output files, stop-check tables, and final outputs (data assembly, figures, etc.):\n\nTemporary output files: the Workflow is set up in a way that temporary (in-progress) data files are saved at various stages of the Workflow. Each file will contain the date of creation for easier organisation. When run multiple times, the Workflow will automatically detect if there are any changes in a selected file and only overwrite it, if an updated file is produced (this is powered by {RUtilpol} package). This also means that the user does not have to re-run the whole Workflow but can re-run only specific parts. As the total size of the files can become substantial, the user can specify if all files should be stored within the project folder (default) or in another directory (specified by using the data_storage_path in the Config file). With such specification, and after running the 00_Config_file.R script, there will be an additional folder structure created (see Code block 3).\nstop-checks CSV tables: while running the Workflow, there will be several times that a user will be asked to check and, where necessary, adjust the produced CSV tables to subsequently continue with the Workflow (i.e. re-run script). This is done to oblige the user to check the produced intermediate results before continuing. For example, at a certain point, the Workflow will produce a list of all ecological groups detected within the dataset compilation obtained from Neotoma. A user then has to edit the mentioned CSV table and specify, which ecological groups should be kept (include = TRUE) and which should be filtered out (include = FALSE). Note that there are several stop-checks throughout the Workflow (see overview in Fig. 2).\nWorkflow output (Outputs/, see Section VII for more information):\n\na ready-to-use, taxonomically harmonised and temporally standardised compilation of fossil pollen data, ready for the analytical stage (rds format)\nplots of modelled age-depth curves for each record (pdf format)\npollen diagram of each record (pdf format)\nmetadata table relaying the main data contributor, contact information, and corresponding publications for citation purposes of the used datasets (PDF).\nreproducibility bundle, a zip file with contains all important sections for a reproducibility of the whole project.\n\noverview figures of the spatial and temporal distribution of the dataset compilation, namely a map and a graph of the record lengths, respectively (PDF).\n\n\n\n\n\n│\n└───Data\n│   │\n│   └───Input\n│   │   │\n│   │   └───Chronology_setting\n│   │   │   │\n│   │   │   └───Bchron_crash\n│   │   │   │\n│   │   │   └───Chron_control_point_types\n│   │   │   │\n│   │   │   └───Percentage_radiocarbon\n│   │   │\n│   │   └───Depositional_environment\n│   │   │   │\n│   │   │   └───Neotoma\n│   │   │   │\n│   │   │   └───Other\n│   │   │\n│   │   └───Eco_group\n│   │   │\n│   │   └───Harmonisation_tables\n│   │   │\n│   │   └───Neotoma_download\n│   │   │\n│   │   └───Potential_duplicates\n│   │   │\n│   │   └───Other\n│   │   │\n│   │   └───Regional_age_limits\n│   │   \n│   └───Personal_database_storage\n│   │\n│   └───Processed\n│       │\n│       └───Chronology\n│       │   │\n│       │   └───Chron_tables_prepared\n│       │   │\n│       │   └───Models_full\n│       │   │\n│       │   └───Predicted_ages\n│       │   │\n│       │   └───Temporary_output\n│       │\n│       └───Data_filtered\n│       │\n│       └───Data_harmonised\n│       │\n│       └───Data_merged\n│       │\n│       └───Data_with_chronologies\n│       │\n│       └───Neotoma_processed\n│       │   │\n│       │   └───Neotoma_chron_control\n│       │   │\n│       │   └───Neotoma_dep_env\n│       │   │\n│       │   └───Neotoma_meta\n│       │\n│       └───Other\n│ \n└───Outputs\n    │\n    └───Data\n    │\n    └───Figures\n    │   │\n    │   └───Chronology\n    │   │\n    │   └───Pollen_diagrams\n    │   \n    └───Tables\n        │\n        └───Meta_and_references"
  },
  {
    "objectID": "step_by_step_guide.html#data-processing",
    "href": "step_by_step_guide.html#data-processing",
    "title": "A step-by-step guide to data processing with FOSSILPOL",
    "section": "",
    "text": "Here we focus on the scripts within the R/01_Data_processing folder representing all steps needed for data processing (from obtaining data to the final dataset compilation), organised in the following Sections:\n\nData sourcing: /01_Neotoma_source/ - retrieve and process data from Neotoma\nData sourcing: /02_Other_source/ - process data from other data source (optional)\nInitial data processing: /03_Merging_and_geographic_delineation/ - merge data sources, filter out duplicates, and assign values based on geographical location\nChronologies: /04_Chronologies/ - prepare chronology control tables, calculate age-depth models, and predict ages for levels\nHarmonisation: /05_Harmonisation/ - prepare all harmonisation tables and harmonise pollen taxa (morphotypes)\nData filtering: /06_Main_filtering/ - filter out levels and records based on user-defined criteria\nOutputs: /07_Outputs/ - save the final output including dataset compilation, pollen diagrams, metadata information, graphical summary snd reproducibility bundle\n\n\n\n\n\n\n\nRun_01_01.R- run all scripts within this folder\n01_Download_neotoma.R- download the pollen data from the Neotoma database\n02_Extract_samples.R - create a table from Neotoma’s downloaded lists and download author information\n03_Filter_dep_env.R - get depositional environment data and filter out records based on the user preferences\n04_Extract_chron_control_tables.R - get chronologies, including the preferred table with chronology control points\n05_Extract_raw_pollen_data.R - extract the raw pollen counts from Neotoma and filter by user-selected ecological groups.\n\n\n\nAll pollen records are downloaded from Neotoma based on the geographical criteria (spatial extent, Fig. 2 - config criteria 1) and the selected data type, in this case: \"pollen\". Note that a more complex spatial extent, like a polygon, can be used with the loc argument in RFossilpol::proc_neo_get_all_neotoma_datasets()(see usage of loc in neotoma2 example here).\n\n\n\nEach record is processed using a unique dataset ID (dataset_id) with metadata information extracted. Metadata includes information about the name of the record, geographical information, and the authors and publication DOI connected to the dataset. The authors and their link to the dataset are saved into a separate Author-Dataset database created specifically for each project. It allows easy extraction of authors and DOI for the final dataset compilation produced by the Workflow.\n\n\n\nDepositional information from each record gives information about the environments where a record was extracted. Based on the research question, there could be a preference for certain environments (e.g. terrestrial vs. marine). Currently in Neotoma, the data about depositional environments are organised in a hierarchical structure (e.g. “Pond” is nested in “Natural Lake”, which is nested in “Lacustrine”), in which the maximum number of nested layers is five. At the lowest hierarchical level, there are currently over 50 different categories of depositional environments (for fossil pollen records). Based on the selected records, the Workflow will produce a list of all depositional environments (and their hierarchical position) present in the user´s data selection. The user is then requested to define the environments of choice (this is a stop-check point, Fig. 2). Note that excluding depositional environments with the higher hierarchical position will not automatically exclude all depositional environments nested in it.\n\n\n\nChronology data for each record is available in a table that contains information about chronology control points used to construct an age-depth model. Some records can have multiple chronology tables as some records have been used for several projects or recalibrated (updated) by data stewards. These tables are numbered according to the order in which they were created and uploaded. Each chronology comes with the age-unit of the age-depth model output (e.g. “Radiocarbon years BP”, “Calibrated radiocarbon years BP”) and the temporal range of the record (youngest and oldest age). The chronologies in “Radiocarbon years BP” are often older chronologies as it is now common practice to recalibrate radiocarbon-dated material and produce chronologies expressed in “Calibrated radiocarbon years BP”. Note: The chronologies in “Calibrated radiocarbon years BP” still come with chronology table(s) containing the uncalibrated radiocarbon ages and need to be calibrated by the user if a new age-depth model is desired. The Workflow automatically selects one table per record based on the order defined by chron_order in the Config file (Fig. 2 - config criteria 2). Note: if more tables have the same age-unit type (e.g. Calibrated radiocarbon years BP), the Workflow will opt for the more recent table. The user can specify their preference for certain age-unit types in the Config file. In addition, only records which have at least a certain number of control points (defined by min_n_of_control_points in the Config file, Fig. 2 - config criteria 3) will be subsequently used.\n\n\n\nEach level of each record comes with additional information: a) unique sample ID (sample_id), b) information about depth (and estimated age later), and c) pollen counts for each taxon present in that level. The information about levels is split into two different tables (first with depth and ages, and second with pollen counts) linked together by sample ID (sample_id).\nThe Workflow will only keep records with a minimal number of levels as defined in the Config file (min_n_levels, Fig. 2 - config criteria 4). The minimum number of levels is by default selected as three but the user can change this setting.\nIn the case of data sourced from Neotoma, each pollen taxon has information about the ecological group (e.g. palms, mangroves, etc). Based on the selected records, the Workflow will produce a full list of all ecological groups after which the user is requested to define which ecological groups to include (a stop-check point, Fig. 2, see explanation of abbreviation in Table 1)\n\n\n\n\n\n\n\n\n\nABBREVIATION\nECOLOGICAL GROUP\n\n\n\n\nACRI\nAcritarchs\n\n\nANAC\nAnachronic\n\n\nALGA\nAlgae (e.g. Botryococcus)\n\n\nAQB\nAquatics (e.g. Sphagnum)\n\n\nAQVP\nAquatic vascular plants (e.g. Isoetes)\n\n\nBIOM\nBiometric measurements\n\n\nEMBR\nEmbryophyta\n\n\nFUNG\nFungi\n\n\nLABO\nLab analyses\n\n\nMAN\nMangroves\n\n\nPALM\nPalms\n\n\nPLNT\nPlant\n\n\nSEED\nUnidentified, but definitely pollen - Spermatophyte rank or clade\n\n\nSUCC\nSucculents\n\n\nTRSH\nTrees and shrubs\n\n\nUNID\nUnknown and Indeterminable\n\n\nUPBR\nUpland bryophytes\n\n\nUPHE\nUpland herbs\n\n\nVACR\nTerrestrial vascular cryptogams\n\n\nVASC\nVascular plants\n\n\n\n\n\n\n\n\n\n\n\n\nRun_01_02.R - run all scripts within this folder\n01_Import_other_data.R - source other data sources and filter the records in a similar way as Neotoma.\n\n\n\nOur FOSSILPOL Workflow allows the use of other data source in combination with the Neotoma data. Including other data sources is fully optional and can be skipped as indicated by use_other_datasource = TRUE/FALSE in the Config file.\nAny data can be used as long as it contains the following required information: a) metadata, b) depositional environment, c) chronology, d) level (age-depth), and e) pollen counts. In order to prepare data for usage, the user needs to download file template specially prepared for this. Each pollen record needs to be stored as a separate file with unique name. We recommend e.g., private_data_(insert_site_name).xlsx. The site name in the filename is crucial as it will be compared against all other pollen records in Neotoma to test for potential duplicates in a later stage of the Workflow. All files must be stored in /Data/Input/Other/ (or specified by dir_files argument, see below).\n\n\n\nThe sourcing of other data sources follows a simple order of actions:\n\nData files need to be prepared by the user following the template one record per file.\nData are extracted and formatted to be compatible with Neotoma data using the RFossilpol::import_datasets_from_folder() function, with the following arguments:\n\ndir_files - user can specify which folder contains the prepared data (default = Data/Input/Other/\nsuffix - argument to keep track of the source of the data. Default is set to \"other\", which means that datasets can be easily identified as their name will be (dataset id)_other\nsource_of_data - will flag the source of each dataset in the compilation in meta-data overview (see section VII). Default is set to \"personal_data\"\ndata_publicity - will flag the data publicity of each dataset in the compilation in meta-data overview (see section VII - Outputs). Default is set to \"restricted\"\npollen_percentage - pollen counts measured as proportions (e.g., from scanning of pollen diagrams) can be flagged here. Default set to FALSE\n\nNames of data contributors are extracted and added to the Author-Dataset database used in Author-dataset attribution (see section VII - Outputs).\nData are treated in a similar way as data from Neotoma, in terms of filtering by geographical location, number of levels (Fig. 2 - config criteria 5,6), and depositional environments (stop-check point, Fig. 2).\n\n\n\n\n\n\n\n\n\nRun_01_03.R - run all scripts within this folder\n01_Merge_datasets.R - merge data from all sources, filter out duplicates, and assign values based on geographical location\n\n\n\nAfter initial data processing, records from Neotoma and Other sources are merged together.\n\n\nThere is a possibility that some datasets from the other data sources are already in Neotoma. To avoid duplication within the final dataset compilation, the Workflow will compare datasets from both sources and identify potential duplicates. This step is optional but recommended to follow. To do so, the user needs to specify that detect_duplicates == TRUE in the Config file (this is set as default, Fig. 2 - config criteria 7). The Workflow will start a simple subroutine using the function RFossilpol::proc_filter_out_duplicates(). Because comparing all records within each data source to each other is relatively computationally demanding, the function will split the data into several groups using their geographical location (ca. 100 records per group). The user can define the number of groups using the n_subgroups argument. Next, each record from one source is compared to all records from the other source as long as they are within 1 degree radius (the assumption here is that duplicated records will be in a similar location). The user can define the maximum distance using the max_degree_distance argument. Finally, the Workflow will output a list of potential duplicated records (a stop-check point, Fig. 2). For each record-pair, the user must specify, which records should be deleted by writing 1 (deleting the Neotoma) or 2 (deleting the other data source) in the delete column of the created list (leaving 0 will leave both records in).\n\n\n\nSeveral more steps take place to create the fully merged dataset compilation before proceeding to the chronology step (does not require any action by user):\n\nAll taxon names are transformed into a more computer-friendly format for easier manipulation. The RFossilpol::proc_clean_count_names() function will first transform special characters to text (e.g. + to _plus_) and then use the {janitor} to transform into “snake_case” style. In addition, the user can specify additional specific changes in names (e.g. based on presence of special characters) by adjusting the user_name_patterns argument (see example in the script). During this cleaning of taxa names, the Workflow will save taxa_refrence_table for back traceability to Neotoma taxonomy. The taxa_reference_table is a CSV file saved in the same folder as the harmonisation tables (Data/Input/Harmonisation_tables/). More about harmonisation process.\nIndividual levels (sample depths) are sorted by their depth for each record by RFossilpol::proc_prepare_raw_count_levels(). This includes subroutines, for example, only keeping levels present in all data tables, filtering out levels without pollen data, and taxa which are not present in any level.\nSpatial information for each record is assigned based on the provided geographical shapefiles. Specifically:\n\nRegion information - the shapefile in Data/Input/Spatial/Regions_shapefile will assign the regional names for each record (see Data input Section). The user can (and is recommended to) change the spatial delineation of the data by altering or replacing the shapefile.\nPolitical delineation (countries) - obtained from GADM database, version 2.8, November 2015.\nHarmonisation region - the shapefile in Data/Input/Spatial/Harmonisation_regions_shapefile will assign the harmonisation region (to be able to link the corresponding harmonisation table to use; See see Data input Section). The default shapefile in the Workflow is a copy of the Region information shapefile but should be adjusted by the user to correspond to the area covered by the different harmonisation tables.\nCalibration curves (normal and post-bomb) - depending on the geographical position of the record, a different calibration curve needs to be assigned, as different curves are used for the northern and southern hemispheres, and for terrestrial and marine environments. See more details about calibration curves.\nAdditional - The user can add any additional spatial delimitation (e.g. ecozones). This will require adding the specific shapefile (or TIF file) in /Data/Input/Spatial/NAME_OF_FOLDER and adjusting the R code manually (optional_info_to_assign) so that the shapefile is sourced, and its information assigned to each record (see the example in the script).\n\nThe Workflow will create a new table with age limitations for each region presented in the data, which needs to be edited by the user (a stop-check point, Fig. 2). For example, Regional_age_limits table will have the following values:\n\nyoung_age = youngest age the record must have\nold_age = oldest age the record must have\nend_of_interest_period = levels beyond this age will be omitted\n\n\n\n\n\n\n\n\n\n\n\nRun_01_04.R - run all scripts within this folder\n01_Prepare_chron_control_tables.R - prepare the chronology tables for age-depth modelling\n02_Run_age_depth_models.R- create age-depth models with BChron\n03_Predict_ages.R - estimate the ages of individual levels\n04_Save_AD_figures.R - save visual output of the age-depth models in pdf format\n05_Merge_chron_output.R - link age-depth models to corresponding datasets\n\n\n\nTo estimate the age of individual levels based on their depth, an age-depth model needs to be constructed based on the chronology data of the record. An age-depth model will provide age estimates of each individual level and the full age range of the record.\nAge-depth modelling can be very computationally heavy and can take a substantial amount of time. Therefore, the Workflow automatically processes several files (rds format):\n\nChronology-control-tables:\n\n/Data/Processed/Chronology/Chron_tables_prepared/chron_tables_prepared*.rds contains all the chronology control tables prepared to be re-calibrated\n\nAge-depth-models:\n\n/Data/Processed/Chronology/Models_full/* contains one file for each age-depth model\n\nPredicted-ages:\n\n/Data/Processed/Chronology/Predicted_ages/predicted_ages*.rds contains the predicted ages using the age-depth models\n\n\nIt may be not preferential to re-run all age-depth models every time the user wants to re-run the Workflow. Therefore, the Workflow offers the option to save the successful age-depth models (Age-depth-models & Predicted-ages) and keep them between individual runs. This can be done in several ways:\n\nrecalib_AD_models in the Config file can be set to FALSE. This will result in completely omitting the scripts aimed at calculating age-depth models, and therefore this step will be skipped. Note that this will only work if the age-depth models have already been successfully created at least once (Predicted-ages exists).\ncalc_AD_models_denovo in the Config file can be set to FALSE (this is the default). When set to TRUE, the Workflow will not use the Age-depth-models files (successful age-depth models) and will re-calibrate everything “de novo”.\npredict_ages_denovo in the Config file is set to FALSE as the default. When set to TRUE, the Workflow will use all the age-depth model files but re-assign the ages to each level of all related records (even for records where ages were successfully predicted). This is done, for instance, when the number of levels in a record increased since the last Workflow run and these levels still need to have ages assigned.\n\nIMPORTANT: If you select both calc_AD_models_denovo and predict_ages_denovo as FALSE for your first run of age-depth modelling, you might be asked by the Workflow to temporarily switch both to TRUE in the console (you do not have to change anything in the Config file).\n\n\n\nAge-depth models are constructed using chronology control points (usually radiocarbon dates) with known depth, estimated age, and associated age uncertainties. Each record can and should ideally have several such points saved in the chronology control table. Each chronology control point has the following properties:\n\nDepth\nEstimated age\nError of the estimated age\nType of the chronology control point\nThe calibration curve used (which is needed to convert the raw radiocarbon ages to calendar ages (Note: radiocarbon ages are not true calendar ages!)).\n\nThis script will take several steps to prepare all records for age-depth modelling, specifically:\n\nCreate and attach the necessary calibration curves\nSelect the preferred chronology control point types\nPrepare chronology tables (include fixing issues with percentage carbon (if necessary))\n\n\n\nCalibration curves are assigned to each control point based on several criteria. If a control point has a type flagged to be calibrated (see next section), a calibration curve is assigned based on the geographical position of the record. Note: only chronology control points of uncalibrated radiocarbon ages need recalibration. FOSSILPOL has a shapefile based on figure 7 in Hogg et al (2020) to correctly assigned IntCal20, SHCal20 and a mixed calibration curve to each record. We follow this recommendation “…the use of (i) IntCal20 for areas north of the ITCZ in June-August (dashed lines in Figure 7) which receive NH air masses all year round, (ii) SHCal20 for areas south of ITCZ in December-February (dotted lines in Figure 7) which see SH air masses all year round, and (iii) a mixed curve for areas between the two seasonal ITCZ positions shown in Figure 7, which receive northern air masses in December-February and southern air masses in June-August.” FOSSILPOL also comes with a mixed curve that was constructed using the {rcarbon} package with a proportion of curve contribution 1:1. See more info on mixed calibration curve use.\nAll calibration curves have an age limitation, i.e. each curve can be used only for certain ages. Radiocarbon calibration curves currently do not cover ages older than 55 thousand years. For the younger ages, during the last century, there are issues with the Earth’s atmospheric radiocarbon and a different set of calibration curves needs to be used. This is caused by the deployment of nuclear weapons in the ’50s and ’60s, which caused a spike in atmospheric radiocarbon and resulted in radiocarbon measurements from material in the following decades to return often highly negative ages. Therefore, if the control point has a radiocarbon age younger than 200 yr BP, a post-bomb calibration curve is used instead (see further below). As with the normal calibration curves, the geographical location of the record is taken into account to assign the corresponding post-bomb calibration curve.\nModern radiocarbon dates are calibrated by using one of the post-bomb calibration curves (nh_zone_1, nh_zone_2, nh_zone_3, sh_zone_1_2, sh_zone_3), following Hua et al., 2013 and link. The Workflow will automatically assign the corresponding curve based on the geographical location of the record (see function IntCal::copyCalibrationCurve()). If modern radiocarbon dates are detected, the Workflow will then display the detected records to the user and apply the conversion according to the post-bomb curves from the {IntCal} package.\n\n\n\nEach control point in the control table has several properties: unique ID, depth, age, error, thickness, and chronology control point type (e.g. radiocarbon, biostratigraphic, annual laminations, tephra). Each type of chronology control point has different age uncertainties. For instance, many older records relied on indirect dating techniques based on biostratigraphic layers, similar geological levels from other records (e.g. a volcanic event), and pollen-based levels (e.g. the appearance of a key taxon), among others and can have large age uncertainties into thousands of years. Neotoma has over 50 different chronology controls points that fall within the categories of geochronological (e.g. lead-210, radiocarbon, uranium-series), relative time scale (e.g. MIS5e, Heinrich Stadial 1, Late Wisconsin event), stratigraphic (e.g. biostratigraphic events such as the introduction of anthropogenic taxa), cultural (e.g. European Settlement Horizon), other absolute dating methods (e.g. annual laminations, collection date), and other dating methods (e.g. extrapolated or guesses). Only the chronology control points in uncalibrated radiocarbon ages require recalibration with the calibration curves as most, if not all, other control points will be in calendar ages and no recalibration should be implemented.\nA user has the option to select which control point types should be accepted “as-is” and which should be calibrated (a stop-check point, Fig. 2). The Workflow will automatically produce a list of all detected control points from all selected records, which includes columns called include and calibrate. In the first, the user should indicate if the chronology control point should be included. In latter, the user should indicate if the point should be recalibrated using the calibration curves, here uncalibrated radiocarbon ages.\n\n\n\nThe chronology control tables will need to undergo a number of user-defined adjustments:\n\nfiltering out unwanted control point types selected by user (defined by stop-check point, see above)\nfiltering out records that do not fulfil the minimal number of control points (defined by min_n_of_control_points in the Config file, default = 2, Fig. 2 - config criteria 8). The value min_n_of_control_points will serve as a criterion for the prepared chronology control tables, where records that do not fulfil such requirements will be filtered out. Note that there is a trade-off between accepting only the tables with a high number of control points per time interval (more robust age-depth model) and the number of records that will be able to fulfil strict criteria.\nfixing instances of missing values. Defined in the Config file, the values default_thickness (defined in the Config file, default = 1, Fig. 2 - config criteria 9) and default_error (default = 100, Fig. 2 - config criteria 10) will replace missing values (NA) for thickness and error, specifically.\nfiltering out control points with an error that is considered too big. Any control point with an error bigger than max_age_error (defined in the Config file; default = 3000 yr, Fig. 2 - config criteria 11) will be filtered out.\nremoving control points that are duplicated in terms of age and/or depth.\nin several cases, the chronology control point from the core-top has a type specified as guess. A user can specify that the type guess is only acceptable to a certain depth using the guess_depth variable in the Config file (default is 10 cm, Fig. 2 - config criteria 12)\n\nIn addition, the number and distribution of such control points can give a good indicator of the temporal uncertainty around levels’ ages (Giesecke et al. 2014, Flantua et al. 2016). For example, a record with few chronology control points within the focus time period will have large uncertainties of predicted ages. Hence, the information of the quality of chronologies, i.e. taking into account the types and levels of chronology control points, can be a criterion used in the selection of records.\n\n\n\nThere are three ways by which post-bomb radiocarbon dates are reported, namely by 1) modern radiocarbon dates (&lt;0 F14C yr); 2) percent modern carbon (pMC, normalised to 100%); and 3) fraction radiocarbon (F14C, normalised to 1; Reimer et al. 2004). Currently, there is no direct way to know from Neotoma whether the dates are in pMC or F14C. Even if the cases are likely to be few, such dates need to be checked to avoid incorrect calculations.\nThe strongest rule of thumb is that normal radiocarbon dates and errors should always be integer (no decimals) and are uploaded so by the data stewards to Neotoma. The second rule of thumb is that pMC control points are characterised by an age value from around 100 with a decimal place. Thus, the Workflow will automatically export suspicious records and the user must specify which control points should be back-transformed (a stop-check point, (Fig. 2).). For those selected by the user to be back-transformed (include == TRUE), the Workflow will first convert the pMC values to normal post-bomb radiocarbon ages (negative 14C ages) by using the IntCal::pMC.age() function, after which normal post-bomb calibration curves are used to calibrate the values to final calendar ages (see above).\nAlthough F14C has been recommended by experts for the reporting of post-bomb samples (Reimer et al. 2004, in practice, the bulk of the reporting is done as modern radiocarbon dates followed by pMC to a much lesser extent. The Workflow currently does not deal with F14C as no such cases have been detected to date. As this is not consistent with the rest of the data, back-transformation could be done when working with other data sources, and the {IntCal} package can be used to do so.\n\n\n\n\nIndividual age-depth models are estimated using the {Bchron} package, which estimates the Bayesian probabilistic trajectory of the age-depth model curve (non-parametric chronology model to age-depth data according to the Compound Poisson-Gamma model). Therefore, it is suitable for various combinations of control point types, outliers, and age reversals.\nIf there are many ages at close or similar depths (e.g. annual laminations), initialisation problems may occur and the Bchron could fail to converge the age-depth model. In such a scenario, the thickness of duplicated chronology control points is automatically increased by 0.01 (see artificialThickness argument in Bchron::Bchronology() function).\n\n\nBecause creating age-depth models for multiple records can be computationally demanding, the Workflow uses multi-core (parallel) computation. The Workflow automatically detects the number of cores for the machine on which the code is running (this can be adjusted by specifying the number in number_of_cores in the Config file, Fig. 2 - config criteria 14). Several age-depth models are then created at the same time. This is done by splitting the records into batches, with each batch containing a certain number of records (see batch size in Config file, Fig. 2 - config criteria 13; by default it is based on the number_of_cores). If number_of_cores is selected (or detected) as 1, the Workflow will not use the batch approach (as obsolete) and estimate the age-depth models one-by-one (see below).\nNote that there is a possibility that the age-depth model estimation will crash (freeze) for unforeseen reasons. Therefore, the Workflow is structured in a way that if an estimation of the whole batch crashes, the Workflow will skip that batch and continue with other batches. The user can specify how long the machine should wait before skipping the batch with the time_per_record argument in the RFossilpol::chron_recalibrate_ad_models() function.\nFor each batch, the Workflow will try to estimate age-depth models three times. The user can specify the number of attempts by changing the batch_attempts in the RFossilpol::chron_recalibrate_ad_models() function. If the age-depth modelling is stopped in the process, the Workflow will automatically use the previously successful batches, which are saved automatically. This should help when the user needs to stop the age-depth modelling and resume it at a later time.\nNext, the Workflow continues to another subroutine where age-depth models for records from failed batches are estimated one by one. Similar to batch estimation, the age-depth model estimation is tried three times for each crashed record until successful at least once. On some rare occasions, a record could cause R to freeze completely and prevents it from skipping to another record. In that case, the dataset ID of the dataset that caused the crash will be automatically written in the Crash file (found in /Data/Input/Chronology_setting/Bchron_crash/) and omitted from the future run of age-depth models. The user is recommended to do a detailed check of the specific dataset, e.g. the chronology control table for possible inconsistencies or other flaws that could be causing {Bchron} to fail to produce an age-depth model.\n\n\n\nIn the Config file, the number of iterations is set to 50,000 by default, discarding the first 10,000 iterations (burn-in) and keeping every iteration beyond the burn-in with a step size of 40 (thinning) (Fig. 2 - config criteria 15,16,17). The user can change the number of iterations by altering the iteration_multiplier in the Config file (Fig. 2 - config criteria 18). This will result in changing the total interactions but keeping the ratios of burn-ins and thinning. Thus 1000 ((50k - 10k) / 40) posterior values are drawn. The default number of iterations should produce a robust estimation but they can be increased by the user if preferred (by increasing the iteration_multiplier). Note that increasing the iteration_multiplier will automatically increase the time that the program will wait for estimation of an age-depth model before skipping it (see time_per_record above).\nThe user should keep in mind that creating age-depth models for hundreds of records using a high number of iterations is computationally demanding and can take a significant amount of time in the order of tens of hours or several days.\n\n\n\n\nWith a successful age-depth model, the ages of individual levels are estimated. The Workflow will estimate age and the error estimate for each level, which will encapsulate 95% of all age posterior values (upper and lower boundary). As {Bchron} uses a probabilistic model it is possible to obtain possible ages of each level. To do so, a number of ages (default = 1000, see above) are drawn from the model posterior representing all the possible ages, and a series of quantiles of various values (25, 50, 75, etc.) are then calculated. The 50th quantile (median) is used as the final age of each level, and the 2.5th and 97.5th quantiles as upper and lower boundaries respectively. This results in the age estimate of each level including its error estimates. The whole matrix levels by posterior drawn (possible age) is saved as an age uncertainty matrix (age_uncertainty column). This whole process is completely automated and does not require any input from the user.\n\n\n\nThe visual representation of all age-depth models is saved as output for visual confirmation of successful model estimation. The files (as PDFs) are stored in the /Outputs/Figures/Chronologies/ folder and split into subfolders defined by region. The properties of the figures (size of figures, font size, etc) can be altered in the Config file (image_width, image_height, image_units, image_dpi, text_size, line_size). They can be used for inspection of the age-depth curves to look for unrealistically large age estimates or error bars, hiatuses, or extreme extrapolations toward the present or the past.\n\n\n\nThe successfully predicted ages are linked with all the records from the various sources (Sections I-III). The individual levels of each record are then ordered by the predicted ages and the same order of levels is then applied to the tables with the pollen counts.\n\n\n\n\n\n\n\n\nRun_01_05.R - run all scripts within this folder\n01_Harmonisation.R - prepare all harmonisation tables and harmonise the raw counts.\n\n\n\nThe goal of taxonomic harmonisation is to standardise all site-level names to the same pollen morphotypes (set of pollen and spore morphotypes used for all pollen records) and thus reduce the effect of taxonomic uncertainty and nomenclatural complexity (See relevant literature in Appendix 1 of Flantua et al. 2023). For this purpose, a harmonisation table can be created that groups the morphotypes into the highest taxonomic level that is most likely to be identified by most of the pollen analysts.\n\n\n\nFirst, the Workflow will check the harmonisation regions present in the data, defined by the shapefile (see Data input and Section III), and confirm that there is one harmonisation table per region (a stop-check point, Fig. 2). If any table is missing (or the Workflow is run for the first time), the Workflow will automatically create a harmonisation table per harmonisation region, with all the raw taxa names from all the records from within that region. Note that we refer here to “taxon names” for simplicity but the identification is often done at the level of family or genus, and are best referred to as morphotypes.\nEach harmonisation table is created so that each taxon can have two columns:\n\ntaxon_name which is the original name of the taxon (morphotype) formatted into a more computer-friendly format (snake_case)\nlevel 1, which should be used to merge various taxa (morphotypes) into harmonised taxonomic units, specific for the project.\n\nNote that in order to link the names to the original display in Neotoma, user can use the taxa_reference_table (see name cleaning process).\nThe user can also define which taxa should be completely removed during the harmonisation process (marked as delete), in case of a taxonomic mistake or a palaeoecological proxy not of interest, e.g. spores). The user can add additional columns (e.g. level_2) and then specify which levels should be included by altering the argument harm_name when using the RFossilpol::harmonise_all_regions() function.\nWith these tables, each dataset is harmonised so that all taxa that belong to the same harmonised taxa (morphotypes) are summed together in each level - this process is applicable for both count and percentage data. This process includes automatic detection of successful pollen harmonisation by checking the total number of pollen grains before and after harmonisation (it can be turned off by changing the argument pollen_grain_test to FALSE when using the RFossilpol::harmonise_all_regions() function).\n\n\n\n\n\n\n\n\nRun_01_06.R - run all scripts within this folder\n01_Level_filtering.R - filter out levels and records based on the user-defined criteria predefined in the Config file\n\n\n\nTo obtain a comprehensive dataset compilation of multiple fossil pollen records, to increase its overall quality, and to answer research questions reliably, we recommend the user to further trim down the data selection by filtering out individual levels and/or whole records. All of these filtering criteria can be adjusted by the user in the Config file:\n\nfilter_by_pollen_sum (Fig. 2 - config criteria 20) - if TRUE, the Workflow will use the quantity of counted pollen grains at each level as a factor in determining the quality of the level.\nfilter_by_age_limit (Fig. 2 - config criteria 24) - if TRUE, the Workflow will filter out records that do not span the user-defined time period (defined by young_age and old_age in Regional_age_limits table; see Section III).\nfilter_by_extrapolation (Fig. 2 - config criteria 25) - if TRUE, the Workflow will filter out levels based on the number of years between their age and the last chronology control point used for age-depth modelling (chron_control_limits).\nfilter_by_interest_region (Fig. 2 - config criteria 27) - if TRUE, the Workflow will filter out levels that are older than end_of_interest_period (defined in Regional_age_limits table; see Section III) to subsequently reduce processing time during follow-up analyses.\nfilter_by_number_of_levels (Fig. 2 - config criteria 28) - if TRUE, the Workflow will filter out records based on the number of levels.\n\nIn addition, two more options can be activated by the user:\n\nuse_age_quantiles (Fig. 2 - config criteria 30) - if TRUE, the Workflow will use the 95th age quantile (uncertainty of the level age) throughout the data filtration process, i.e. the age of the level will be assumed to be anywhere between the upper and lower boundary (see Section IV). This will result in a more stable dataset compilation between the different results of age-depth modelling (as a probabilistic result can output slightly different results each time they are run). However, the final dataset compilation may require some additional filtering before any analyses, as the 95th age quantile can span very long time periods.\nuse_bookend_level (Fig. 2 - config criteria 31) - if TRUE, the Workflow will leave one additional level beyond the oldest age of the user-defined time period. This will result in a bookend level, which can help to anchor information beyond the period of interest.\n\n\n\n\n\n\nThe number of counted pollen grains at each level is an index of data quality. To obtain a reliable representation of the vegetation, researchers often aim to count more than 300 pollen grains (following Moore et al., 1991), but other recommendations may also have been followed (&gt;150; e.g. Djamali & Cilleros, 2020) and will vary with region and by scientific question (Birks & Birks, 1980). For example, to achieve a representative sample of the regional pollen pool, counts in Arctic records may only reach c. 100 grains per level, whereas counts in Mediterranean sites can be as high as 1000 (Birks & Birks, 1980, p. 165), but the main determinant can also be the preference of the pollen analyst. Reasons for low numbers (&lt;100) are often mainly due to time constraints of the data contributor, but can also be natural depositional phenomena causing poor pollen preservation, such as low local pollen production in arctic or alpine environments. Given that statistical inferential power is proportional to sample size, we recommend defining a minimum number of total pollen grains in each level. Subsequently, whole records can be selected on the proportion of levels with a selected minimum number of pollen grains counted per level.\nThe user can select two different quantities of total pollen grains per level: a) minimum number (min_n_grains, Fig. 2 - config criteria 21) and b) acceptable number (target_n_grains, Fig. 2 - config criteria 22). All levels with total pollen grains below the minimum number will be filtered out. In addition, the whole record will only be accepted if X% (set by percentage_samples; default = 50, Fig. 2 - config criteria 23) of all levels fulfils at least the acceptable number of pollen grains. This filtration criterion will only be used if filter_by_pollen_sum == TRUE.\n\n\n\nAs projects differ in their temporal focus, only a subset of records will be of interest to the particular project. Therefore, records that do not span a certain age period (from young_age to old_age; specified by the Regional_age_limits table, see Section III) will be filtered out. This filtration criterion will only be used if filter_by_age_limit == TRUE in the Config file.\n\n\n\nExtrapolation of age inferences to samples beyond the set of chronology control points is another factor in quality control. Levels older than the oldest chronology control point have no other chronology control point to constrain the age inference, and therefore, have increasingly large uncertainty as the amount of extrapolation increases. To limit the use of levels based on large extrapolations, we recommend selecting a maximum extrapolation age, i.e. levels older than the selected age criterion (e.g. 5000 yr) from the last chronology control point are filtered out.\nIn order to limit the use of levels based on large extrapolations, the user can select the maximum extrapolation age (maximum_age_extrapolation, Fig. 2 - config criteria 26), i.e. levels older than the selected criterion from the last chronology control point will be filtered out. This filtration criterion will only be used if filter_by_extrapolation == TRUE in the Config file.\n\n\n\nThe individual levels of a record outside of the interest period (end_of_interest_period specified by the Regional_age_limits table) will be filtered out as they do not provide additional information. This filtration criterion will only be used if filter_by_interest_region == TRUE in the Config file.\n\n\n\nThe total number of levels in a record is an important quality criterion for further use of such a record in a specific analysis. Records might have been sampled at low resolution (e.g. depth intervals &gt; 30 cm) leaving substantial unassessed gaps - and thus time periods - between levels. In addition, records with few levels will likely contribute poorly to studies focused on specific time periods (many non-value levels) and can cause unnecessary outlier values. Therefore, we recommend selecting a minimum number of levels within the time period of interest and use this as an additional criterion to filter out unwanted records. The user can select the minimum number of levels (min_n_levels, Fig. 2 - config criteria 29) that records must have at the end of the filtration subroutine. This filtration criterion will only be used if filter_by_number_of_levels == TRUE in the Config file.\n\n\n\n\n\n\n\n\n\nRun_01_07.R - run all scripts within this folder\n01_Pollen_diagrams.R - save pollen diagrams for all records\n02_Save_assembly.R - save data assembly with selected variables (columns)\n03_Save_references.R - save reference and metatable\n\n\n\nPollen diagrams for all records will be created using the {rioja} package. The harmonised data will be automatically transformed into relative proportion for plotting purposes. The pollen diagrams will be saved in the /Outputs/Figures/Pollen_diagrams/ folder and split into sub-folders defined by Region (e.g., continent).\nThe RFossilpol::plot_all_pollen_diagrams() function will automatically produce a PDF of the pollen diagram split into several A4 pages and ready to be printed out. The y-axis is, by default, the age of the levels, but it can be altered to depth (by the y_var argument set to \"age\" or \"depth\"). The maximum number of taxa per page can be altered by the max_taxa argument (default = 20). In addition, the function will automatically omit very rare taxa. Specifically, the min_n_occur argument will determine the minimum number of occurrences per record each taxon has to have to be plotted.\n\n\n\nA ready-to-use, taxonomically harmonised and standardised compilation of fossil pollen data (ready for the analytical stage) is produced and saved in the /Outputs/Data/ folder. The file is saved as a tibble (by {tibble} package from tidyverse) in rds format.\nThe user can select which columns (variables) should be present in the final data compilation by selecting select_final_variables == TRUE in the Config file. Here, the Workflow will interactively (in the R console) ask the user to specify if each variable should be included (Yes/No).\n\n\n\nThe workflow will save all metadata and citation information needed to create the dataset compilation. All outputs can be found in the /Outputs/Meta_and_references/ folder. The functionRFossilpol::proc_save_references() will do this automatically but it can be specified by the user, i.e. the user can specify what information should be saved using the user_sel_variables argument. By default this information contains:\n\n\"meta_table\" - a metadata table (data_assembly_meta.csv) contains the following variables by default (note that this list may be altered by changing the final variables by select_final_variables):\n\nthe list of all records in the final dataset compilation\ngeographical location\ndepositional environment\nassigned continental region\nassigned harmonisation region\nfinal number of levels\nnumber of chronology control points used for age-depth modelling\nassigned calibration curve(s)\nage limits of each record of each data source (Neotoma or other data source)\nDOI (from Neotoma).\n\n\"author_table\" - a reference table (authors_meta.csv) containing information about the datasets used, the main data contributor, and their contact information.\n\"affiliation_table\" - if an affiliation is provided with the data from sources other than Neotoma, the affiliation table (affiliation_table) is also exported linking affiliations and their authors.\n\"graphical_summary\" - a PDF (graphical_summary*.pdf) with three panel figures using RFossilpol::plot_graphical_summary() function (note that additional arguments can be passed to this function, see ?RFossilpol::plot_graphical_summary for more information):\n\nmap - map of geographical location with each record represented as a point\ncount of data records- a lollipop plot with the number of records in each geographical group when assigned\nage length - age limits of records, with each record represented by a single line\n\n\"reproducibility_bundle\" - a zip file (reproducibility_bundle.zip) of the Config file, all stop-check CSV tables, and all shapefiles. The idea is to increase the reproducibility of the user´s workflow where ideally this zip file can be shared when publishing a publication with a project created using the FOSSILPOL Workflow. For reviewing purposes, the zip file can be shared and detailed feedback can be provided before publication."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "FOSSILPOL project",
    "section": "",
    "text": "This website presents all information needed about the FOSSILPOL workflow that aims to process and standardise global palaeoecological pollen data.\n\n\nThe FOSSILPOL workflow has been developed during the ERC project called the Humans on Planet Earth (HOPE) team at the University of Bergen:\nSuzette G.A. Flantua*, Ondrej Mottl*, Vivian Felde*, Kuber P. Bhatta*, Hilary Birks, John-Arvid Grytnes, Alistair Seddon, H. John B. Birks\n* shared first authors\nOndřej Mottl is the main maintainer of the FOSSILPOL project\n\n\n\n\n\n  \n\n\n\n\n\nThe project consists of several sections:\n\nA guide to the processing and standardisation of global palaeoecological data for large-scale syntheses using fossil pollen - An open-access publication in Global Ecology and Biogeography (Flantua et al. 2023)\nFOSSILPOL workflow - A tool to process multiple fossil pollen records to create a comprehensive, standardised dataset compilation, ready for multi-record and multi-proxy analyses (coded in the R environment)\n{RFossilpol} package - An R-package developed specifically to provide tools and functions for the FOSSILPOL workflow\nWebsite - A main hub of all information about the FOSSILPOL project, including a step-by-step guide for users of FOSSILPOL with a description of the most important functions and user criteria throughout the workflow\nScandinavia example - A case study example of the FOSSILPOL workflow being applied to fossil pollen records from northern Europe\nIssue tracker - Specific GitHub repository to manage all Issues detected in the FOSSILPOL workflow\nFuture updates - Overview of the future updates we plan to do on the FOSSILPOL workflow and R-package\n\n\n\n\n\nGeneral information - Information about how to obtain, set up, and run the FOSSILPOL project\nA step-by-step guide - Detailed information about individual steps of the data processing\nGet in touch! - Information about the community, future updates, issue reporting, and potential collaboration\nResources - Additional materials related to the FOSSILPOL project.\n\n\n\n\n\nFOSSILPOL workflow: \n{RFossilpol} R-package:"
  },
  {
    "objectID": "index.html#summary",
    "href": "index.html#summary",
    "title": "FOSSILPOL project",
    "section": "",
    "text": "This website presents all information needed about the FOSSILPOL workflow that aims to process and standardise global palaeoecological pollen data.\n\n\nThe FOSSILPOL workflow has been developed during the ERC project called the Humans on Planet Earth (HOPE) team at the University of Bergen:\nSuzette G.A. Flantua*, Ondrej Mottl*, Vivian Felde*, Kuber P. Bhatta*, Hilary Birks, John-Arvid Grytnes, Alistair Seddon, H. John B. Birks\n* shared first authors\nOndřej Mottl is the main maintainer of the FOSSILPOL project"
  },
  {
    "objectID": "index.html#project-sections",
    "href": "index.html#project-sections",
    "title": "FOSSILPOL project",
    "section": "",
    "text": "The project consists of several sections:\n\nA guide to the processing and standardisation of global palaeoecological data for large-scale syntheses using fossil pollen - An open-access publication in Global Ecology and Biogeography (Flantua et al. 2023)\nFOSSILPOL workflow - A tool to process multiple fossil pollen records to create a comprehensive, standardised dataset compilation, ready for multi-record and multi-proxy analyses (coded in the R environment)\n{RFossilpol} package - An R-package developed specifically to provide tools and functions for the FOSSILPOL workflow\nWebsite - A main hub of all information about the FOSSILPOL project, including a step-by-step guide for users of FOSSILPOL with a description of the most important functions and user criteria throughout the workflow\nScandinavia example - A case study example of the FOSSILPOL workflow being applied to fossil pollen records from northern Europe\nIssue tracker - Specific GitHub repository to manage all Issues detected in the FOSSILPOL workflow\nFuture updates - Overview of the future updates we plan to do on the FOSSILPOL workflow and R-package"
  },
  {
    "objectID": "index.html#website-pages",
    "href": "index.html#website-pages",
    "title": "FOSSILPOL project",
    "section": "",
    "text": "General information - Information about how to obtain, set up, and run the FOSSILPOL project\nA step-by-step guide - Detailed information about individual steps of the data processing\nGet in touch! - Information about the community, future updates, issue reporting, and potential collaboration\nResources - Additional materials related to the FOSSILPOL project."
  },
  {
    "objectID": "index.html#current-versions",
    "href": "index.html#current-versions",
    "title": "FOSSILPOL project",
    "section": "",
    "text": "FOSSILPOL workflow: \n{RFossilpol} R-package:"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "FOSSILPOL project",
    "section": "",
    "text": "Here we present a guide on how to obtain and set up the FOSSILPOL workflow which is an R-based modular workflow to process multiple fossil pollen records to create a comprehensive, standardised dataset compilation, ready for multi-record and multi-proxy analyses at various spatial and temporal scales (Fig. 1). The general guide is described in the publication titled A guide to the processing and standardisation of global palaeoecological data for large-scale syntheses using fossil pollen, by Flantua, S.G.A., Mottl, O., Felde, V.A., Bhatta, K.P., Birks, H.H., Grytnes, J-A., Seddon, A.W.R., Birks H.J.B. (2023) in Global Ecology and Biogeography.\n\n\n\n\n \n\n\n\nThe FOSSILPOL workflow is coded as an RStudio project (in the R programming language), which should be customised by the user as per their specific research project.\nThe FOSSILPOL workflow has been developed for processing fossil pollen data in the ERC project called the Humans on Planet Earth (HOPE) team at the University of Bergen.\nThe FOSSILPOL logo and Figure 1 have been created by amazing MilanTvM\n\n\n\n\n\n\nThe FOSSILPOL workflow (referred to as “the Workflow” from here on) is accessible in two ways:\n\nIf a user has a GitHub account, the easiest way is to create your own GitHub repo using this GitHub template. More details about how to use GitHub templates are on GitHub Docs.\nA user can download the latest Release of the Workflow as a zip file from the FOSSILPOL Workflow Release page.\n\nThe R project consists of codes with individual scripts and functions. All scripts are stored in the R/ folder. After obtaining the workflow, the FOSSILPOL R project will have the following structure:\n\n\n\nproject\n│\n│   README.md\n│   Rprofile\n│   gitignore  \n│   Workflow_template.Rproj\n│\n└───Data\n│   │\n│   └───Input\n│       │\n│       └───Spatial\n│           │\n│           └───Biomes_shapefile   \n│           │   │\n│           │   └───WWF\n│           │\n│           └───Calibration_curves_shapefile   \n│           │\n│           └───Countries_shapefile\n│           │  \n│           └───Harmonisation_regions_shapefile\n│           │\n│           └───Postbomb_shapefile\n│           │\n│           └───Regions_shapefile\n│\n└───R\n│   │\n│   │   ___Init_project___.R\n│   │   00_Config_file.R\n│   │\n│   └───01_Data_processing\n│   │   │   │\n│   │   │   │   Master_run_01.R\n│   │   │\n│   │   └───01_Neotoma_source\n│   │   │   │\n│   │   │   │   Run_01_01.R\n│   │   │   │   01_Download_neotoma.R\n│   │   │   │   02_Extract_samples.R\n│   │   │   │   03_Filter_dep_env.R\n│   │   │   │   04_Extract_chron_control_tables.R\n│   │   │   │   05_Extract_raw_pollen_data.R\n│   │   │   \n│   │   └───02_Other_source\n│   │   │   │\n│   │   │   │   Run_01_02.R\n│   │   │   │   01_Import_other_data.R\n│   │   │\n│   │   └───03_Merging_and_geography\n│   │   │   │\n│   │   │   │   Run_01_03.R\n│   │   │   │   01_Merge_datasets.R\n│   │   │\n│   │   └───04_Chronologies\n│   │   │   │\n│   │   │   │   Run_01_04.R\n│   │   │   │   01_Prepare_chron_control_tables.R\n│   │   │   │   02_Run_age_depth_models.R\n│   │   │   │   03_Predict_ages.R\n│   │   │   │   04_Save_AD_figures.R\n│   │   │   │   05_Merge_chron_output.R\n│   │   │\n│   │   └───05_Harmonisation\n│   │   │   │\n│   │   │   │   Run_01_05.R\n│   │   │   │   01_Harmonisation.R\n│   │   │\n│   │   └───06_Main_filtering\n│   │   │   │\n│   │   │   │   Run_01_06.R\n│   │   │   │   01_Level_filtering.R\n│   │   │\n│   │   └───07_Outputs\n│   │       │\n│   │       │   Run_01_07.R\n│   │       │   01_Pollen_diagrams.R\n│   │       │   02_Save_assembly.R\n│   │       │   03_Save_references.R\n│   │\n│   └───02_Main_analyses\n│   │   │\n│   │   │   Master_run_02.R\n│   │\n│   └───03_Supplementary_analyses\n│   │   │\n│   │   │   Master_run_03.R\n│   │\n│   └───Functions\n│       │\n│       │ example_function.R\n│\n└───renv\n    │\n    │   gitignore\n    │   activate.R\n    │   library_list.lock\n    │   settings.dcf\n\n\n\n\nOnce a user obtains their version of the Workflow, there are several steps to be done before using it:\n\nUpdate R and R-studio IDE. There are many guides on how to do so (e.g. here)\nExecute all individual steps with the ___Init_project___.R script. This will result in the preparation of all R-packages using the {renv} package, which is an R dependency management of your projects. Mainly it will install two main R-packages {RFossilpol} and {RUtilpol} and all their dependencies. {RFossilpol} has been developed specifically for the workflow and the latest release is automatically installed in the project set-up stage. This is important as the package version should align with the Workflow version. Therefore, we do not recommend updating the package after installation. Note that installing all packages can take a substantial amount of time.\nSet up your preferences by editing the 00_Config_file.R script (referred to as “Config file” from here on). The Config file is a script where all settings (configurations) and criteria used throughout the project are predefined by the user before running the Workflow. In addition, it prepares the current session by loading the required packages and saving all settings throughout the project. Points in the Config file that require the user’s attention are flagged by “[USER]”, meaning that these are criteria that need to be checked by the user. More information on the Config file is given in the website section A step-by-step guide. The crucial points are:\n\ndata_storage_path within section “2. Current date and working directory” - as the Workflow produces several large files, a user can specify the directory where such files will be stored. Note that the default place is within the project.\nsection “5. Define variables” - these are the variables that are important for data selection and filtering to obtain the final data compilation.\n\nRun R/01_Data_processing/Master_run_01.R to run the whole data processing part of the project (be ready for “stop-checks”). Alternatively, the user can run each script individually. After all stop-checks are resolved, the Workflow can be run as a whole to produce the final standardised dataset compilation, ready for multi-record and multi-proxy analyses.\nIf desired, the user can run additional project-specific scripts (not provided) in the 02_Main_analyses and 03_Supplementary_analyses folders to analyse the data compilation.\n\n\n\n\nThis Workflow is constructed using a script cascade. This means that the Master_run_01.R, located within R/01_Data_processing/ folder, executes all scripts within sub-folders of the R/01_Data_processing/ folder, which in turn, executes all their sub-folders (e.g., R/01_Data_processing/01_Neotoma_source/Run_01_01.R executes R/01_Data_processing/01_Neotoma_source/01_Download_neotoma.R, R/01_Data_processing/01_Neotoma_source /02_Extract_samples.R, …). See Code block 2.\nTherefore, a user can run the data processing section of the project by executing R/01_Data_processing/Master_run_01.R script or run individual sections by executing individual scripts within sections. An alternative example is that the user can run the Workflow subsection as a whole by running R/01_Data_processing/01_Neotoma_source/Run_01_01.R, and not R/01_Data_processing/01_Neotoma_source/01_Download_neotoma.R, and then R/01_Data_processing/01_Neotoma_source /02_Extract_samples.R, etc.\n\n\n\nR\n│\n└───01_Data_processing\n        │\n        │   Master_run_01.R\n        │\n        └───01_Neotoma_source\n            │\n            │   Run_01_01.R\n                │\n                │   01_Download_neotoma.R\n                │   02_Extract_samples.R\n                │   03_Filter_dep_env.R\n                │   04_Extract_chron_control_tables.R\n                │   05_Extract_raw_pollen_data.R"
  },
  {
    "objectID": "about.html#general-information",
    "href": "about.html#general-information",
    "title": "FOSSILPOL project",
    "section": "",
    "text": "Here we present a guide on how to obtain and set up the FOSSILPOL workflow which is an R-based modular workflow to process multiple fossil pollen records to create a comprehensive, standardised dataset compilation, ready for multi-record and multi-proxy analyses at various spatial and temporal scales (Fig. 1). The general guide is described in the publication titled A guide to the processing and standardisation of global palaeoecological data for large-scale syntheses using fossil pollen, by Flantua, S.G.A., Mottl, O., Felde, V.A., Bhatta, K.P., Birks, H.H., Grytnes, J-A., Seddon, A.W.R., Birks H.J.B. (2023) in Global Ecology and Biogeography."
  },
  {
    "objectID": "about.html#how-to-obtain-the-workflow",
    "href": "about.html#how-to-obtain-the-workflow",
    "title": "FOSSILPOL project",
    "section": "",
    "text": "The FOSSILPOL workflow (referred to as “the Workflow” from here on) is accessible in two ways:\n\nIf a user has a GitHub account, the easiest way is to create your own GitHub repo using this GitHub template. More details about how to use GitHub templates are on GitHub Docs.\nA user can download the latest Release of the Workflow as a zip file from the FOSSILPOL Workflow Release page.\n\nThe R project consists of codes with individual scripts and functions. All scripts are stored in the R/ folder. After obtaining the workflow, the FOSSILPOL R project will have the following structure:\n\n\n\nproject\n│\n│   README.md\n│   Rprofile\n│   gitignore  \n│   Workflow_template.Rproj\n│\n└───Data\n│   │\n│   └───Input\n│       │\n│       └───Spatial\n│           │\n│           └───Biomes_shapefile   \n│           │   │\n│           │   └───WWF\n│           │\n│           └───Calibration_curves_shapefile   \n│           │\n│           └───Countries_shapefile\n│           │  \n│           └───Harmonisation_regions_shapefile\n│           │\n│           └───Postbomb_shapefile\n│           │\n│           └───Regions_shapefile\n│\n└───R\n│   │\n│   │   ___Init_project___.R\n│   │   00_Config_file.R\n│   │\n│   └───01_Data_processing\n│   │   │   │\n│   │   │   │   Master_run_01.R\n│   │   │\n│   │   └───01_Neotoma_source\n│   │   │   │\n│   │   │   │   Run_01_01.R\n│   │   │   │   01_Download_neotoma.R\n│   │   │   │   02_Extract_samples.R\n│   │   │   │   03_Filter_dep_env.R\n│   │   │   │   04_Extract_chron_control_tables.R\n│   │   │   │   05_Extract_raw_pollen_data.R\n│   │   │   \n│   │   └───02_Other_source\n│   │   │   │\n│   │   │   │   Run_01_02.R\n│   │   │   │   01_Import_other_data.R\n│   │   │\n│   │   └───03_Merging_and_geography\n│   │   │   │\n│   │   │   │   Run_01_03.R\n│   │   │   │   01_Merge_datasets.R\n│   │   │\n│   │   └───04_Chronologies\n│   │   │   │\n│   │   │   │   Run_01_04.R\n│   │   │   │   01_Prepare_chron_control_tables.R\n│   │   │   │   02_Run_age_depth_models.R\n│   │   │   │   03_Predict_ages.R\n│   │   │   │   04_Save_AD_figures.R\n│   │   │   │   05_Merge_chron_output.R\n│   │   │\n│   │   └───05_Harmonisation\n│   │   │   │\n│   │   │   │   Run_01_05.R\n│   │   │   │   01_Harmonisation.R\n│   │   │\n│   │   └───06_Main_filtering\n│   │   │   │\n│   │   │   │   Run_01_06.R\n│   │   │   │   01_Level_filtering.R\n│   │   │\n│   │   └───07_Outputs\n│   │       │\n│   │       │   Run_01_07.R\n│   │       │   01_Pollen_diagrams.R\n│   │       │   02_Save_assembly.R\n│   │       │   03_Save_references.R\n│   │\n│   └───02_Main_analyses\n│   │   │\n│   │   │   Master_run_02.R\n│   │\n│   └───03_Supplementary_analyses\n│   │   │\n│   │   │   Master_run_03.R\n│   │\n│   └───Functions\n│       │\n│       │ example_function.R\n│\n└───renv\n    │\n    │   gitignore\n    │   activate.R\n    │   library_list.lock\n    │   settings.dcf\n\n\n\n\nOnce a user obtains their version of the Workflow, there are several steps to be done before using it:\n\nUpdate R and R-studio IDE. There are many guides on how to do so (e.g. here)\nExecute all individual steps with the ___Init_project___.R script. This will result in the preparation of all R-packages using the {renv} package, which is an R dependency management of your projects. Mainly it will install two main R-packages {RFossilpol} and {RUtilpol} and all their dependencies. {RFossilpol} has been developed specifically for the workflow and the latest release is automatically installed in the project set-up stage. This is important as the package version should align with the Workflow version. Therefore, we do not recommend updating the package after installation. Note that installing all packages can take a substantial amount of time.\nSet up your preferences by editing the 00_Config_file.R script (referred to as “Config file” from here on). The Config file is a script where all settings (configurations) and criteria used throughout the project are predefined by the user before running the Workflow. In addition, it prepares the current session by loading the required packages and saving all settings throughout the project. Points in the Config file that require the user’s attention are flagged by “[USER]”, meaning that these are criteria that need to be checked by the user. More information on the Config file is given in the website section A step-by-step guide. The crucial points are:\n\ndata_storage_path within section “2. Current date and working directory” - as the Workflow produces several large files, a user can specify the directory where such files will be stored. Note that the default place is within the project.\nsection “5. Define variables” - these are the variables that are important for data selection and filtering to obtain the final data compilation.\n\nRun R/01_Data_processing/Master_run_01.R to run the whole data processing part of the project (be ready for “stop-checks”). Alternatively, the user can run each script individually. After all stop-checks are resolved, the Workflow can be run as a whole to produce the final standardised dataset compilation, ready for multi-record and multi-proxy analyses.\nIf desired, the user can run additional project-specific scripts (not provided) in the 02_Main_analyses and 03_Supplementary_analyses folders to analyse the data compilation.\n\n\n\n\nThis Workflow is constructed using a script cascade. This means that the Master_run_01.R, located within R/01_Data_processing/ folder, executes all scripts within sub-folders of the R/01_Data_processing/ folder, which in turn, executes all their sub-folders (e.g., R/01_Data_processing/01_Neotoma_source/Run_01_01.R executes R/01_Data_processing/01_Neotoma_source/01_Download_neotoma.R, R/01_Data_processing/01_Neotoma_source /02_Extract_samples.R, …). See Code block 2.\nTherefore, a user can run the data processing section of the project by executing R/01_Data_processing/Master_run_01.R script or run individual sections by executing individual scripts within sections. An alternative example is that the user can run the Workflow subsection as a whole by running R/01_Data_processing/01_Neotoma_source/Run_01_01.R, and not R/01_Data_processing/01_Neotoma_source/01_Download_neotoma.R, and then R/01_Data_processing/01_Neotoma_source /02_Extract_samples.R, etc.\n\n\n\nR\n│\n└───01_Data_processing\n        │\n        │   Master_run_01.R\n        │\n        └───01_Neotoma_source\n            │\n            │   Run_01_01.R\n                │\n                │   01_Download_neotoma.R\n                │   02_Extract_samples.R\n                │   03_Filter_dep_env.R\n                │   04_Extract_chron_control_tables.R\n                │   05_Extract_raw_pollen_data.R"
  },
  {
    "objectID": "get_in_touch.html",
    "href": "get_in_touch.html",
    "title": "Get in touch",
    "section": "",
    "text": "We would love to build a community around FOSSILPOL and there are various ways to get in touch and grow together.\n\n\nWe have set up GitHub Discussions as a main hub for communication. Do you have a question? Have you thought about a cool new feature? Let’s write us a message. We hope that the Discussions page will serve as a line of communication to the developers as well as between various users.\n\n\n\nNo software is without problems and if you find a nasty bug while using this workflow, please use the Issues page to report it.\nConsider the following steps before and when opening a new Issue:\n\nHave you or someone else asked about it at the GitHub Discussions? The “Q&A” section is perfect for that!\nHave you checked that similar issues may have been already reported? The issue tracker has a filter function to search for keywords in open Issues. You can narrow down the search using labels🏷️ as filters. See Labels for more information. As a general rule, we don’t assign issues to anyone.\n\nTo open a new Issue:\n\nClick on the green New issue button in the upper right corner, and select Bug report.\nDescribe your problem in as much detail as possible. The issue should state what the problem is, what the expected behaviour should be, and, maybe, suggest a solution. Note that you can also attach files (e.g. sample data, R code, etc.) or images to the issue.\nSelect a suitable label🏷️ from the drop-down menu called Labels.\nClick on the green Submit new issue button and wait for a reply.\n\n\n\n\nWe also use the Issues page for serious feature requests. If some discussion GitHub Discussions portal leads to a flashed-out feature, you would like to implement, you can submit it as a feature request:\n\nGo to the Issues page\nClick on the green New issue button in the upper right corner, and select Feature request.\nDescribe your feature as detailed as possible. What is the expected behaviour? What packages we should use? Note that you can also attach files or images to the Issue.\nSelect a suitable label🏷️ from the drop-down menu called Labels.\nClick on the green Submit new issue button and wait for a reply.\n\n\n\n\nThe FOSSILPOL project is envisioned as software that will undergo updates for further improvement.\nWe are aware of the functions and features we would like to implement in the future.\nSee the planned future updates in the project future updates The three stages of request are:\n\n“next version” - a feature that will be implemented in the next FOSSILPOL release\n“future” - a feature that will be probably implemented in one of the future FOSSILPOL releases\n“in consideration” - the feature might be implemented but it is not a priority\n\nIf there is a feature you would like to implement, please first check the Issue Tracker and look if someone already suggested it and up-vote it, if it is already there. Before each version release, we will implement the most up-voted feature.\nWe aim to regularly update the list.\n\n\n\nThe FOSSILPOL project is envisioned as software that will undergo updates for further improvement.\nWe appreciate the help :sparkling_heart: and thank you just for considering contributing to FOSSILPOL.\nTo make sure that we maintain the highest quality of code, we do have to adhere to some strict guidelines though. Please read through this document to help you get up and running.\nIf you would like to report a bug, suggest enhancements, or request a new feature, jump to the Issues section.\n\n\nWe use the Git version control system to manage the developments in the repository hosted on GitHub. If you are new to Git or GitHub, please read through the GitHub Bootcamp to get up to speed.\nIf you are already familiar with Git and GitHub, please read Submitting Pull Requests.\n\n\n\nWhile we do have our style in coding and haven’t followed any standards available on the web, we do maintain some uniformity.\nIf we missed mentioning a particular case, you should always follow the below procedure:\n\nSee how it is done in the codebase.\nSee what Advanced R by Hadley Wickham convention says and choose something that is close to the codebase.\nIf all else fails, ask on GitHub Discussions\n\n\n\n\nAll changes to FOSSILPOL must be in the form of a pull request (also known as a PR). If you are unfamiliar with pull requests, please read this.\nHere is the recommended process:\n\nFork the repo so that you can make your changes without affecting the original project until you are ready to merge them. Check out the Guide to forking\nCheck out the branch (named the next version; if there is one).\nCommit your updates once you are happy with them. See contributing guide for commit messages.\nWhen you are finished with the changes, create a PR\n\nClick the “Ready for review” so that we can review your PR. This template helps reviewers understand your changes as well as the purpose of your pull request.\nDon’t forget to link PR to the Issue if you are solving one.\nEnable the checkbox to allow maintainer edits so that the branch can be updated for a merge. Once you submit your PR, a HOPE team member will review your proposal. We may ask questions or request additional information.\nWe may ask for changes to be made before a PR can be merged, either using suggested changes or pull request comments. You can apply suggested changes directly through the user interface (UI). You can make any other changes in your fork, and then commit them to your branch. As you update your PR and apply changes, mark each conversation as resolved\nIf you run into any merge issues, check out this git tutorial to help you resolve merge conflicts and other issues.\n\n\nBefore submitting a pull request, please make sure you follow all the guidelines below while working on your changes:\n\nEach pull request should try to accomplish one general task.\nAll work should be done on a branch with a descriptive name relating to the general task (eg. fix_bug_x or add_feature_y). Each commit should accomplish one small sub-task and should be explainable in a sentence or two.\nEach commit should have a descriptive commit message.\nYou should make sure your code passes all tests before committing."
  },
  {
    "objectID": "get_in_touch.html#contact-us",
    "href": "get_in_touch.html#contact-us",
    "title": "Get in touch",
    "section": "",
    "text": "We have set up GitHub Discussions as a main hub for communication. Do you have a question? Have you thought about a cool new feature? Let’s write us a message. We hope that the Discussions page will serve as a line of communication to the developers as well as between various users."
  },
  {
    "objectID": "get_in_touch.html#it-does-not-work",
    "href": "get_in_touch.html#it-does-not-work",
    "title": "Get in touch",
    "section": "",
    "text": "No software is without problems and if you find a nasty bug while using this workflow, please use the Issues page to report it.\nConsider the following steps before and when opening a new Issue:\n\nHave you or someone else asked about it at the GitHub Discussions? The “Q&A” section is perfect for that!\nHave you checked that similar issues may have been already reported? The issue tracker has a filter function to search for keywords in open Issues. You can narrow down the search using labels🏷️ as filters. See Labels for more information. As a general rule, we don’t assign issues to anyone.\n\nTo open a new Issue:\n\nClick on the green New issue button in the upper right corner, and select Bug report.\nDescribe your problem in as much detail as possible. The issue should state what the problem is, what the expected behaviour should be, and, maybe, suggest a solution. Note that you can also attach files (e.g. sample data, R code, etc.) or images to the issue.\nSelect a suitable label🏷️ from the drop-down menu called Labels.\nClick on the green Submit new issue button and wait for a reply."
  },
  {
    "objectID": "get_in_touch.html#can-we-add-this",
    "href": "get_in_touch.html#can-we-add-this",
    "title": "Get in touch",
    "section": "",
    "text": "We also use the Issues page for serious feature requests. If some discussion GitHub Discussions portal leads to a flashed-out feature, you would like to implement, you can submit it as a feature request:\n\nGo to the Issues page\nClick on the green New issue button in the upper right corner, and select Feature request.\nDescribe your feature as detailed as possible. What is the expected behaviour? What packages we should use? Note that you can also attach files or images to the Issue.\nSelect a suitable label🏷️ from the drop-down menu called Labels.\nClick on the green Submit new issue button and wait for a reply."
  },
  {
    "objectID": "get_in_touch.html#project-future-updates",
    "href": "get_in_touch.html#project-future-updates",
    "title": "Get in touch",
    "section": "",
    "text": "The FOSSILPOL project is envisioned as software that will undergo updates for further improvement.\nWe are aware of the functions and features we would like to implement in the future.\nSee the planned future updates in the project future updates The three stages of request are:\n\n“next version” - a feature that will be implemented in the next FOSSILPOL release\n“future” - a feature that will be probably implemented in one of the future FOSSILPOL releases\n“in consideration” - the feature might be implemented but it is not a priority\n\nIf there is a feature you would like to implement, please first check the Issue Tracker and look if someone already suggested it and up-vote it, if it is already there. Before each version release, we will implement the most up-voted feature.\nWe aim to regularly update the list."
  },
  {
    "objectID": "get_in_touch.html#contribute",
    "href": "get_in_touch.html#contribute",
    "title": "Get in touch",
    "section": "",
    "text": "The FOSSILPOL project is envisioned as software that will undergo updates for further improvement.\nWe appreciate the help :sparkling_heart: and thank you just for considering contributing to FOSSILPOL.\nTo make sure that we maintain the highest quality of code, we do have to adhere to some strict guidelines though. Please read through this document to help you get up and running.\nIf you would like to report a bug, suggest enhancements, or request a new feature, jump to the Issues section.\n\n\nWe use the Git version control system to manage the developments in the repository hosted on GitHub. If you are new to Git or GitHub, please read through the GitHub Bootcamp to get up to speed.\nIf you are already familiar with Git and GitHub, please read Submitting Pull Requests.\n\n\n\nWhile we do have our style in coding and haven’t followed any standards available on the web, we do maintain some uniformity.\nIf we missed mentioning a particular case, you should always follow the below procedure:\n\nSee how it is done in the codebase.\nSee what Advanced R by Hadley Wickham convention says and choose something that is close to the codebase.\nIf all else fails, ask on GitHub Discussions\n\n\n\n\nAll changes to FOSSILPOL must be in the form of a pull request (also known as a PR). If you are unfamiliar with pull requests, please read this.\nHere is the recommended process:\n\nFork the repo so that you can make your changes without affecting the original project until you are ready to merge them. Check out the Guide to forking\nCheck out the branch (named the next version; if there is one).\nCommit your updates once you are happy with them. See contributing guide for commit messages.\nWhen you are finished with the changes, create a PR\n\nClick the “Ready for review” so that we can review your PR. This template helps reviewers understand your changes as well as the purpose of your pull request.\nDon’t forget to link PR to the Issue if you are solving one.\nEnable the checkbox to allow maintainer edits so that the branch can be updated for a merge. Once you submit your PR, a HOPE team member will review your proposal. We may ask questions or request additional information.\nWe may ask for changes to be made before a PR can be merged, either using suggested changes or pull request comments. You can apply suggested changes directly through the user interface (UI). You can make any other changes in your fork, and then commit them to your branch. As you update your PR and apply changes, mark each conversation as resolved\nIf you run into any merge issues, check out this git tutorial to help you resolve merge conflicts and other issues.\n\n\nBefore submitting a pull request, please make sure you follow all the guidelines below while working on your changes:\n\nEach pull request should try to accomplish one general task.\nAll work should be done on a branch with a descriptive name relating to the general task (eg. fix_bug_x or add_feature_y). Each commit should accomplish one small sub-task and should be explainable in a sentence or two.\nEach commit should have a descriptive commit message.\nYou should make sure your code passes all tests before committing."
  },
  {
    "objectID": "other_materials.html",
    "href": "other_materials.html",
    "title": "Resources & Materials",
    "section": "",
    "text": "We will be placing additional material (e.g., presentations, posters) about the FOSSILPOL project here.\n\n\n\n“A guide to the processing and standardisation of global palaeoecological data for large-scale syntheses using fossil pollen”: .\n\n\n\n\n\nIBS 2024 presentation\n\n\n\n\n\nPoster presented at INQUA 2023 in Rome"
  },
  {
    "objectID": "other_materials.html#publications",
    "href": "other_materials.html#publications",
    "title": "Resources & Materials",
    "section": "",
    "text": "“A guide to the processing and standardisation of global palaeoecological data for large-scale syntheses using fossil pollen”: ."
  },
  {
    "objectID": "other_materials.html#presentations",
    "href": "other_materials.html#presentations",
    "title": "Resources & Materials",
    "section": "",
    "text": "IBS 2024 presentation"
  },
  {
    "objectID": "other_materials.html#posters",
    "href": "other_materials.html#posters",
    "title": "Resources & Materials",
    "section": "",
    "text": "Poster presented at INQUA 2023 in Rome"
  }
]